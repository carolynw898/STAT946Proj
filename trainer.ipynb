{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/carolynw898/STAT946Proj/blob/main/DiffuSym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from models import SymbolicDiffusion, PointNetConfig, tNet\n",
    "import numpy as np\n",
    "from utils import processDataFiles, CharDataset\n",
    "import glob\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "max_seq_len = 32         \n",
    "n_embd = 512             \n",
    "timesteps = 1000         \n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 10\n",
    "blockSize = 32\n",
    "numVars = 1\n",
    "numYs = 1\n",
    "numPoints = 250\n",
    "const_range = [-2.1, 2.1]\n",
    "trainRange = [-3.0, 3.0]\n",
    "decimals = 8\n",
    "addVars = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 967 examples, 52 unique.\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "# path = \"/kaggle/input/1-var-dataset/1_var_train.json\"\n",
    "path = \"0_1_0_13062021_174033.json\"\n",
    "files = glob.glob(path)\n",
    "text = processDataFiles(files)\n",
    "chars = sorted(list(set(text))+['_','T','<','>',':']) # extract unique characters from the text before converting the text to a list, # T is for the test data\n",
    "text = text.split('\\n') # convert the raw text to a set of examples\n",
    "trainText = text[:-1] if len(text[-1]) == 0 else text\n",
    "random.shuffle(trainText) # shuffle the dataset, it's important specailly for the combined number of variables experiment\n",
    "train_dataset = CharDataset(text, blockSize, chars, numVars=numVars, \n",
    "                numYs=numYs, numPoints=numPoints, addVars=addVars,\n",
    "                const_range=const_range, xRange=trainRange, decimals=decimals, augment=False)\n",
    "\n",
    "vocab_size = train_dataset.vocab_size    \n",
    "\n",
    "loader = DataLoader(train_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pconfig = PointNetConfig(\n",
    "    embeddingSize=n_embd,\n",
    "    numberofPoints=250,\n",
    "    numberofVars=1,\n",
    "    numberofYs=1,\n",
    ")\n",
    "model = SymbolicDiffusion(\n",
    "    pconfig=pconfig,\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=max_seq_len,\n",
    "    padding_idx=train_dataset.paddingID, # one of the dataset properties\n",
    "    max_num_vars=9,\n",
    "    n_layer=6,\n",
    "    n_head=8,\n",
    "    n_embd=n_embd,\n",
    "    timesteps=timesteps,\n",
    "    beta_start=0.0001,\n",
    "    beta_end=0.02,\n",
    ").to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for _,  tokens, points, variables in loader:\n",
    "        points = points.to(device)        \n",
    "        tokens = tokens.to(device)        \n",
    "        variables = variables.to(device)  \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        t = torch.randint(0, model.timesteps, (tokens.shape[0],), device=device)\n",
    "        y_pred, noise_pred, noise = model(points, tokens, variables, t)\n",
    "        loss = model.loss_fn(noise_pred, noise, y_pred, tokens, t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss\n",
    "\n",
    "def validate_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for _,  tokens, points, variables in loader:\n",
    "            points = points.to(device)\n",
    "            tokens = tokens.to(device)\n",
    "            variables = variables.to(device)\n",
    "\n",
    "            y_pred, noise_pred, noise = model(points, tokens, variables)\n",
    "            t = torch.randint(0, model.timesteps, (tokens.shape[0],), device=device)\n",
    "            loss = model.loss_fn(noise_pred, noise, y_pred, tokens, t)\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (32) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbolic_diffusion_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m y_pred, noise_pred, noise \u001b[38;5;241m=\u001b[39m model(points, tokens, variables)\n\u001b[1;32m     35\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, model\u001b[38;5;241m.\u001b[39mtimesteps, (tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 36\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/STAT946/Final Project/STAT946Proj/models.py:312\u001b[0m, in \u001b[0;36mSymbolicDiffusion.loss_fn\u001b[0;34m(self, noise_pred, noise, pred_logits, tokens, t)\u001b[0m\n\u001b[1;32m    308\u001b[0m ce_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m (t\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimesteps)\n\u001b[1;32m    309\u001b[0m ce_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(\n\u001b[1;32m    310\u001b[0m     pred_logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size), tokens\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    311\u001b[0m )\u001b[38;5;241m.\u001b[39mview(pred_logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 312\u001b[0m weighted_ce_loss \u001b[38;5;241m=\u001b[39m (\u001b[43mce_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mce_loss\u001b[49m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    314\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m mse_loss \u001b[38;5;241m+\u001b[39m weighted_ce_loss\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (32) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # track epochs\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # train\n",
    "    train_loss = train_epoch(model, loader, optimizer, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"symbolic_diffusion_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyML17AeCwdJDr6NLGVUATGc",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
