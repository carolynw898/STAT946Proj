{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 11178716,
          "sourceType": "datasetVersion",
          "datasetId": 6977314
        },
        {
          "sourceId": 11333846,
          "sourceType": "datasetVersion",
          "datasetId": 7089758
        },
        {
          "sourceId": 11372292,
          "sourceType": "datasetVersion",
          "datasetId": 7119350
        },
        {
          "sourceId": 306062,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 261112,
          "modelId": 282263
        },
        {
          "sourceId": 319741,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 269794,
          "modelId": 290784
        },
        {
          "sourceId": 319828,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 269860,
          "modelId": 290853
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "STAT946_Proj",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carolynw898/STAT946Proj/blob/main/STAT946_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "ByW7IGo3gXo1"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "65_QApDegXo4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "from scipy.optimize import minimize\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def generateDataStrEq(\n",
        "    eq, n_points=2, n_vars=3, decimals=4, supportPoints=None, min_x=0, max_x=3\n",
        "):\n",
        "    X = []\n",
        "    Y = []\n",
        "    # TODO: Need to make this faster\n",
        "    for p in range(n_points):\n",
        "        if supportPoints is None:\n",
        "            if type(min_x) == list:\n",
        "                x = []\n",
        "                for _ in range(n_vars):\n",
        "                    idx = np.random.randint(len(min_x))\n",
        "                    x += list(\n",
        "                        np.round(np.random.uniform(min_x[idx], max_x[idx], 1), decimals)\n",
        "                    )\n",
        "            else:\n",
        "                x = list(np.round(np.random.uniform(min_x, max_x, n_vars), decimals))\n",
        "            assert (\n",
        "                len(x) != 0\n",
        "            ), \"For some reason, we didn't generate the points correctly!\"\n",
        "        else:\n",
        "            x = supportPoints[p]\n",
        "\n",
        "        tmpEq = eq + \"\"\n",
        "        for nVID in range(n_vars):\n",
        "            tmpEq = tmpEq.replace(\"x{}\".format(nVID + 1), str(x[nVID]))\n",
        "        y = float(np.round(eval(tmpEq), decimals))\n",
        "        X.append(x)\n",
        "        Y.append(y)\n",
        "    return X, Y\n",
        "\n",
        "\n",
        "# def processDataFiles(files):\n",
        "#     text = \"\"\n",
        "#     for f in tqdm(files):\n",
        "#         with open(f, 'r') as h:\n",
        "#             lines = h.read() # don't worry we won't run out of file handles\n",
        "#             if lines[-1]==-1:\n",
        "#                 lines = lines[:-1]\n",
        "#             #text += lines #json.loads(line)\n",
        "#             text = ''.join([lines,text])\n",
        "#     return text\n",
        "\n",
        "\n",
        "def processDataFiles(files):\n",
        "    text = \"\"\n",
        "    for f in files:\n",
        "        with open(f, \"r\") as h:\n",
        "            lines = h.read()  # don't worry we won't run out of file handles\n",
        "            if lines[-1] == -1:\n",
        "                lines = lines[:-1]\n",
        "            # text += lines #json.loads(line)\n",
        "            text = \"\".join([lines, text])\n",
        "    return text\n",
        "\n",
        "\n",
        "def tokenize_equation(eq):\n",
        "    token_spec = [\n",
        "        (r\"\\*\\*\"),  # exponentiation\n",
        "        (r\"exp\"),  # exp function\n",
        "        (r\"[+\\-*/=()]\"),  # operators and parentheses\n",
        "        (r\"sin\"),  # sin function\n",
        "        (r\"cos\"),  # cos function\n",
        "        (r\"log\"),  # log function\n",
        "        (r\"x\\d+\"),  # variables like x1, x23, etc.\n",
        "        (r\"C\"),  # constants placeholder\n",
        "        (r\"-?\\d+\\.\\d+\"),  # decimal numbers\n",
        "        (r\"-?\\d+\"),  # integers\n",
        "        (r\"_\"),  # padding token\n",
        "    ]\n",
        "    token_regex = \"|\".join(f\"({pattern})\" for pattern in token_spec)\n",
        "    matches = re.finditer(token_regex, eq)\n",
        "    return [match.group(0) for match in matches]\n",
        "\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data,\n",
        "        block_size,\n",
        "        tokens,\n",
        "        numVars,\n",
        "        numYs,\n",
        "        numPoints,\n",
        "        target=\"Skeleton\",\n",
        "        addVars=False,\n",
        "        const_range=[-0.4, 0.4],\n",
        "        xRange=[-3.0, 3.0],\n",
        "        decimals=4,\n",
        "        augment=False,\n",
        "    ):\n",
        "\n",
        "        data_size, vocab_size = len(data), len(tokens)\n",
        "        print(\"data has %d examples, %d unique.\" % (data_size, vocab_size))\n",
        "\n",
        "        self.stoi = {tok: i for i, tok in enumerate(tokens)}\n",
        "        self.itos = {i: tok for i, tok in enumerate(tokens)}\n",
        "\n",
        "        self.numVars = numVars\n",
        "        self.numYs = numYs\n",
        "        self.numPoints = numPoints\n",
        "\n",
        "        # padding token\n",
        "        self.paddingToken = \"_\"\n",
        "        self.paddingID = self.stoi[\"_\"]  # or another ID not already used\n",
        "        self.stoi[self.paddingToken] = self.paddingID\n",
        "        self.itos[self.paddingID] = self.paddingToken\n",
        "\n",
        "        self.threshold = [-1000, 1000]\n",
        "\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data  # it should be a list of examples\n",
        "        self.target = target\n",
        "        self.addVars = addVars\n",
        "\n",
        "        self.const_range = const_range\n",
        "        self.xRange = xRange\n",
        "        self.decimals = decimals\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab an example from the data\n",
        "        chunk = self.data[idx]  # sequence of tokens including x, y, eq, etc.\n",
        "\n",
        "        try:\n",
        "            chunk = json.loads(chunk)  # convert the sequence tokens to a dictionary\n",
        "        except Exception as e:\n",
        "            print(\"Couldn't convert to json: {} \\n error is: {}\".format(chunk, e))\n",
        "            # try the previous example\n",
        "            idx = idx - 1\n",
        "            idx = idx if idx >= 0 else 0\n",
        "            chunk = self.data[idx]\n",
        "            chunk = json.loads(chunk)  # convert the sequence tokens to a dictionary\n",
        "\n",
        "        # find the number of variables in the equation\n",
        "        printInfoCondition = random.random() < 0.0000001\n",
        "        eq = chunk[self.target]\n",
        "        if printInfoCondition:\n",
        "            print(f\"\\nEquation: {eq}\")\n",
        "        vars = re.finditer(\"x[\\d]+\", eq)\n",
        "        numVars = 0\n",
        "        for v in vars:\n",
        "            v = v.group(0).strip(\"x\")\n",
        "            v = eval(v)\n",
        "            v = int(v)\n",
        "            if v > numVars:\n",
        "                numVars = v\n",
        "\n",
        "        if self.target == \"Skeleton\" and self.augment:\n",
        "            threshold = 5000\n",
        "            # randomly generate the constants\n",
        "            cleanEqn = \"\"\n",
        "            for chr in eq:\n",
        "                if chr == \"C\":\n",
        "                    # genereate a new random number\n",
        "                    chr = \"{}\".format(\n",
        "                        np.random.uniform(self.const_range[0], self.const_range[1])\n",
        "                    )\n",
        "                cleanEqn += chr\n",
        "\n",
        "            # update the points\n",
        "            nPoints = np.random.randint(\n",
        "                *self.numPoints\n",
        "            )  # if supportPoints is None else len(supportPoints)\n",
        "            try:\n",
        "                if printInfoCondition:\n",
        "                    print(\"Org:\", chunk[\"X\"], chunk[\"Y\"])\n",
        "\n",
        "                X, y = generateDataStrEq(\n",
        "                    cleanEqn,\n",
        "                    n_points=nPoints,\n",
        "                    n_vars=self.numVars,\n",
        "                    decimals=self.decimals,\n",
        "                    min_x=self.xRange[0],\n",
        "                    max_x=self.xRange[1],\n",
        "                )\n",
        "\n",
        "                # replace out of threshold with maximum numbers\n",
        "                y = [e if abs(e) < threshold else np.sign(e) * threshold for e in y]\n",
        "\n",
        "                # check if there is nan/inf/very large numbers in the y\n",
        "                conditions = (\n",
        "                    (np.isnan(y).any() or np.isinf(y).any())\n",
        "                    or len(y) == 0\n",
        "                    or (abs(min(y)) > threshold or abs(max(y)) > threshold)\n",
        "                )\n",
        "                if not conditions:\n",
        "                    chunk[\"X\"], chunk[\"Y\"] = X, y\n",
        "\n",
        "                if printInfoCondition:\n",
        "                    print(\"Evd:\", chunk[\"X\"], chunk[\"Y\"])\n",
        "            except Exception as e:\n",
        "                # for different reason this might happend including but not limited to division by zero\n",
        "                print(\n",
        "                    \"\".join(\n",
        "                        [\n",
        "                            f\"We just used the original equation and support points because of {e}. \",\n",
        "                            f\"The equation is {eq}, and we update the equation to {cleanEqn}\",\n",
        "                        ]\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        # encode every character in the equation to an integer\n",
        "        # < is SOS, > is EOS\n",
        "        if self.addVars:\n",
        "            dix = [self.stoi[s] for s in \"<\" + str(numVars) + \":\" + eq + \">\"]\n",
        "        else:\n",
        "            eq_tokens = tokenize_equation(eq)\n",
        "            if self.addVars:\n",
        "                token_seq = [\"<\", str(numVars), \":\", *eq_tokens, \">\"]\n",
        "            else:\n",
        "                token_seq = [\"<\", *eq_tokens, \">\"]\n",
        "            dix = [self.stoi[tok] for tok in token_seq]\n",
        "\n",
        "        inputs = dix[:-1]\n",
        "        outputs = dix[1:]\n",
        "\n",
        "        # add the padding to the equations\n",
        "        paddingSize = max(self.block_size - len(inputs), 0)\n",
        "        paddingList = [self.paddingID] * paddingSize\n",
        "        inputs += paddingList\n",
        "        outputs += paddingList\n",
        "\n",
        "        # make sure it is not more than what should be\n",
        "        inputs = inputs[: self.block_size]\n",
        "        outputs = outputs[: self.block_size]\n",
        "\n",
        "        points = torch.zeros(self.numVars + self.numYs, self.numPoints - 1)\n",
        "        for idx, xy in enumerate(zip(chunk[\"X\"], chunk[\"Y\"])):\n",
        "\n",
        "            if not isinstance(xy[0], list) or not isinstance(\n",
        "                xy[1], (list, float, np.float64)\n",
        "            ):\n",
        "                print(f\"Unexpected types: {type(xy[0])}, {type(xy[1])}\")\n",
        "                continue  # Skip if types are incorrect\n",
        "\n",
        "            # don't let to exceed the maximum number of points\n",
        "            if idx >= self.numPoints - 1:\n",
        "                break\n",
        "\n",
        "            x = xy[0]\n",
        "            x = x + [0] * (max(self.numVars - len(x), 0))  # padding\n",
        "\n",
        "            y = [xy[1]] if type(xy[1]) == float or type(xy[1]) == np.float64 else xy[1]\n",
        "\n",
        "            y = y + [0] * (max(self.numYs - len(y), 0))  # padding\n",
        "            p = x + y  # because it is only one point\n",
        "            p = torch.tensor(p)\n",
        "            # replace nan and inf\n",
        "            p = torch.nan_to_num(\n",
        "                p,\n",
        "                nan=self.threshold[1],\n",
        "                posinf=self.threshold[1],\n",
        "                neginf=self.threshold[0],\n",
        "            )\n",
        "\n",
        "            points[:, idx] = p\n",
        "\n",
        "        points = torch.nan_to_num(\n",
        "            points,\n",
        "            nan=self.threshold[1],\n",
        "            posinf=self.threshold[1],\n",
        "            neginf=self.threshold[0],\n",
        "        )\n",
        "\n",
        "        inputs = torch.tensor(inputs, dtype=torch.long)\n",
        "        outputs = torch.tensor(outputs, dtype=torch.long)\n",
        "        numVars = torch.tensor(numVars, dtype=torch.long)\n",
        "        return inputs, outputs, points, numVars\n",
        "\n",
        "\n",
        "# Relative Mean Square Error\n",
        "def relativeErr(y, yHat, info=False, eps=1e-5):\n",
        "    yHat = np.reshape(yHat, [1, -1])[0]\n",
        "    y = np.reshape(y, [1, -1])[0]\n",
        "    if len(y) > 0 and len(y) == len(yHat):\n",
        "        err = ((yHat - y)) ** 2 / np.linalg.norm(y + eps)\n",
        "        if info:\n",
        "            for _ in range(5):\n",
        "                i = np.random.randint(len(y))\n",
        "                # print(\"yPR,yTrue:{},{}, Err:{}\".format(yHat[i], y[i], err[i]))\n",
        "    else:\n",
        "        err = 100\n",
        "\n",
        "    return np.mean(err)\n",
        "\n",
        "\n",
        "def lossFunc(constants, eq, X, Y, eps=1e-5):\n",
        "    err = 0\n",
        "    eq = eq.replace(\"C\", \"{}\").format(*constants)\n",
        "\n",
        "    for x, y in zip(X, Y):\n",
        "        eqTemp = eq + \"\"\n",
        "        if type(x) == np.float32:\n",
        "            x = [x]\n",
        "        for i, e in enumerate(x):\n",
        "            # make sure e is not a tensor\n",
        "            if type(e) == torch.Tensor:\n",
        "                e = e.item()\n",
        "            eqTemp = eqTemp.replace(\"x{}\".format(i + 1), str(e))\n",
        "        try:\n",
        "            yHat = eval(eqTemp)\n",
        "        except:\n",
        "            # print(\"Exception has been occured! EQ: {}, OR: {}\".format(eqTemp, eq))\n",
        "            yHat = 100\n",
        "        try:\n",
        "            # handle overflow\n",
        "            err += relativeErr(y, yHat)  # (y-yHat)**2\n",
        "        except:\n",
        "            # print(\n",
        "            #    \"Exception has been occured! EQ: {}, OR: {}, y:{}-yHat:{}\".format(\n",
        "            #        eqTemp, eq, y, yHat\n",
        "            #    )\n",
        "            # )\n",
        "            err += 10\n",
        "\n",
        "    err /= len(Y)\n",
        "    return err\n",
        "\n",
        "\n",
        "def get_skeleton(tokens, train_dataset: CharDataset):\n",
        "    skeleton = \"\".join([train_dataset.itos[int(idx)] for idx in tokens])\n",
        "    skeleton = skeleton.strip(train_dataset.paddingToken).split(\">\")\n",
        "    skeleton = skeleton[0] if len(skeleton[0]) >= 1 else skeleton[1]\n",
        "    skeleton = skeleton.strip(\"<\").strip(\">\")\n",
        "    skeleton = skeleton.replace(\"Ce\", \"C*e\")\n",
        "\n",
        "    return skeleton\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "vyP3lF1ngXo4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "\n",
        "# from SymbolicGPT: https://github.com/mojivalipour/symbolicgpt/blob/master/models.py\n",
        "class PointNetConfig:\n",
        "    \"\"\"base PointNet config\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embeddingSize,\n",
        "        numberofPoints,\n",
        "        numberofVars,\n",
        "        numberofYs,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.embeddingSize = embeddingSize\n",
        "        self.numberofPoints = numberofPoints  # number of points\n",
        "        self.numberofVars = numberofVars  # input dimension (Xs)\n",
        "        self.numberofYs = numberofYs  # output dimension (Ys)\n",
        "\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "\n",
        "class tNet(nn.Module):\n",
        "    \"\"\"\n",
        "    The PointNet structure in the orginal PointNet paper:\n",
        "    PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation by Qi et. al. 2017\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(tNet, self).__init__()\n",
        "\n",
        "        self.activation_func = F.relu\n",
        "        self.num_units = config.embeddingSize\n",
        "\n",
        "        self.conv1 = nn.Conv1d(\n",
        "            config.numberofVars + config.numberofYs, self.num_units, 1\n",
        "        )\n",
        "        self.conv2 = nn.Conv1d(self.num_units, 2 * self.num_units, 1)\n",
        "        self.conv3 = nn.Conv1d(2 * self.num_units, 4 * self.num_units, 1)\n",
        "        self.fc1 = nn.Linear(4 * self.num_units, 2 * self.num_units)\n",
        "        self.fc2 = nn.Linear(2 * self.num_units, self.num_units)\n",
        "\n",
        "        # self.relu = nn.ReLU()\n",
        "\n",
        "        self.input_batch_norm = nn.BatchNorm1d(config.numberofVars + config.numberofYs)\n",
        "        # self.input_layer_norm = nn.LayerNorm(config.numberofPoints)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(self.num_units)\n",
        "        self.bn2 = nn.BatchNorm1d(2 * self.num_units)\n",
        "        self.bn3 = nn.BatchNorm1d(4 * self.num_units)\n",
        "        self.bn4 = nn.BatchNorm1d(2 * self.num_units)\n",
        "        self.bn5 = nn.BatchNorm1d(self.num_units)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: [batch, #features, #points]\n",
        "        :return:\n",
        "            logit: [batch, embedding_size]\n",
        "        \"\"\"\n",
        "        x = self.input_batch_norm(x)\n",
        "        x = self.activation_func(self.bn1(self.conv1(x)))\n",
        "        x = self.activation_func(self.bn2(self.conv2(x)))\n",
        "        x = self.activation_func(self.bn3(self.conv3(x)))\n",
        "        x, _ = torch.max(x, dim=2)  # global max pooling\n",
        "        assert x.size(1) == 4 * self.num_units\n",
        "\n",
        "        x = self.activation_func(self.bn4(self.fc1(x)))\n",
        "        x = self.activation_func(self.bn5(self.fc2(x)))\n",
        "        # x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# from https://github.com/juho-lee/set_transformer/blob/master/modules.py\n",
        "class MAB(nn.Module):\n",
        "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False):\n",
        "        super(MAB, self).__init__()\n",
        "        self.dim_V = dim_V\n",
        "        self.num_heads = num_heads\n",
        "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
        "        self.fc_k = nn.Linear(dim_K, dim_V)\n",
        "        self.fc_v = nn.Linear(dim_K, dim_V)\n",
        "        if ln:\n",
        "            self.ln0 = nn.LayerNorm(dim_V)\n",
        "            self.ln1 = nn.LayerNorm(dim_V)\n",
        "        self.fc_o = nn.Linear(dim_V, dim_V)\n",
        "\n",
        "    def forward(self, Q, K):\n",
        "        Q = self.fc_q(Q)\n",
        "        K, V = self.fc_k(K), self.fc_v(K)\n",
        "\n",
        "        dim_split = self.dim_V // self.num_heads\n",
        "        Q_ = torch.cat(Q.split(dim_split, 2), 0)\n",
        "        K_ = torch.cat(K.split(dim_split, 2), 0)\n",
        "        V_ = torch.cat(V.split(dim_split, 2), 0)\n",
        "\n",
        "        A = torch.softmax(Q_.bmm(K_.transpose(1, 2)) / math.sqrt(self.dim_V), 2)\n",
        "        O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), 0), 2)\n",
        "        O = O if getattr(self, \"ln0\", None) is None else self.ln0(O)\n",
        "        O = O + F.relu(self.fc_o(O))\n",
        "        O = O if getattr(self, \"ln1\", None) is None else self.ln1(O)\n",
        "        return O\n",
        "\n",
        "\n",
        "class SAB(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, num_heads, ln=False):\n",
        "        super(SAB, self).__init__()\n",
        "        self.mab = MAB(dim_in, dim_in, dim_out, num_heads, ln=ln)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.mab(X, X)\n",
        "\n",
        "\n",
        "class ISAB(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, num_heads, num_inds, ln=False):\n",
        "        super(ISAB, self).__init__()\n",
        "        self.I = nn.Parameter(torch.Tensor(1, num_inds, dim_out))\n",
        "        nn.init.xavier_uniform_(self.I)\n",
        "        self.mab0 = MAB(dim_out, dim_in, dim_out, num_heads, ln=ln)\n",
        "        self.mab1 = MAB(dim_in, dim_out, dim_out, num_heads, ln=ln)\n",
        "\n",
        "    def forward(self, X):\n",
        "        H = self.mab0(self.I.repeat(X.size(0), 1, 1), X)\n",
        "        return self.mab1(X, H)\n",
        "\n",
        "\n",
        "class PMA(nn.Module):\n",
        "    def __init__(self, dim, num_heads, num_seeds, ln=False):\n",
        "        super(PMA, self).__init__()\n",
        "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
        "        nn.init.xavier_uniform_(self.S)\n",
        "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.mab(self.S.repeat(X.size(0), 1, 1), X)\n",
        "\n",
        "\n",
        "# from https://github.com/juho-lee/set_transformer/blob/master/models.py\n",
        "class SetTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim_input,\n",
        "        num_outputs,\n",
        "        dim_output,\n",
        "        num_inds=32,\n",
        "        dim_hidden=128,\n",
        "        num_heads=4,\n",
        "        ln=False,\n",
        "    ):\n",
        "        super(SetTransformer, self).__init__()\n",
        "        self.enc = nn.Sequential(\n",
        "            ISAB(dim_input, dim_hidden, num_heads, num_inds, ln=ln),\n",
        "            ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln),\n",
        "        )\n",
        "        self.dec = nn.Sequential(\n",
        "            PMA(dim_hidden, num_heads, num_outputs, ln=ln),\n",
        "            SAB(dim_hidden, dim_hidden, num_heads, ln=ln),\n",
        "            SAB(dim_hidden, dim_hidden, num_heads, ln=ln),\n",
        "            nn.Linear(dim_hidden, dim_output),\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.dec(self.enc(X)).squeeze(1)\n",
        "\n",
        "\n",
        "class NoisePredictionTransformer(nn.Module):\n",
        "    def __init__(self, n_embd, max_seq_len, n_layer=6, n_head=8, max_timesteps=1000):\n",
        "        super().__init__()\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, max_seq_len, n_embd))\n",
        "        self.time_emb = nn.Embedding(max_timesteps, n_embd)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=n_embd,\n",
        "            nhead=n_head,\n",
        "            dim_feedforward=n_embd * 4,\n",
        "            activation=\"gelu\",\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layer)\n",
        "\n",
        "    def forward(self, x_t, t, condition):\n",
        "        _, L, _ = x_t.shape\n",
        "        pos_emb = self.pos_emb[:, :L, :]  # [1, L, n_embd]\n",
        "        time_emb = self.time_emb(t)\n",
        "        if time_emb.dim() == 1:  # Scalar t case, [n_embd]\n",
        "            time_emb = time_emb.unsqueeze(0)  # [1, n_embd]\n",
        "        time_emb = time_emb.unsqueeze(1)  # [1, 1, n_embd]\n",
        "        condition = condition.unsqueeze(1)  # [B, 1, n_embd]\n",
        "\n",
        "        x = x_t + pos_emb + time_emb + condition\n",
        "        return self.encoder(x)\n",
        "\n",
        "\n",
        "# influenced by https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/simple_diffusion.py\n",
        "class SymbolicGaussianDiffusion(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tnet_config: PointNetConfig,\n",
        "        vocab_size,\n",
        "        max_seq_len,\n",
        "        padding_idx: int = 0,\n",
        "        max_num_vars: int = 9,\n",
        "        n_layer=6,\n",
        "        n_head=8,\n",
        "        n_embd=512,\n",
        "        timesteps=1000,\n",
        "        beta_start=0.0001,\n",
        "        beta_end=0.02,\n",
        "        set_transformer=True,\n",
        "        ce_weight=1.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.padding_idx = padding_idx\n",
        "        self.n_embd = n_embd\n",
        "        self.timesteps = timesteps\n",
        "        self.set_transformer = set_transformer\n",
        "        self.ce_weight = ce_weight\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.tok_emb = nn.Embedding(vocab_size, n_embd, padding_idx=self.padding_idx)\n",
        "        self.vars_emb = nn.Embedding(max_num_vars, n_embd)\n",
        "\n",
        "        self.decoder = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "        self.decoder.weight = self.tok_emb.weight\n",
        "\n",
        "        if set_transformer:\n",
        "            dim_input = tnet_config.numberofVars + tnet_config.numberofYs\n",
        "            self.tnet = SetTransformer(\n",
        "                dim_input=dim_input,\n",
        "                num_outputs=1,\n",
        "                dim_output=tnet_config.embeddingSize,\n",
        "                num_inds=tnet_config.numberofPoints,\n",
        "                num_heads=4,\n",
        "            )\n",
        "        else:\n",
        "            self.tnet = tNet(tnet_config)\n",
        "\n",
        "        self.model = NoisePredictionTransformer(\n",
        "            n_embd, max_seq_len, n_layer, n_head, timesteps\n",
        "        )\n",
        "\n",
        "        # Noise schedule\n",
        "        self.register_buffer(\"beta\", torch.linspace(beta_start, beta_end, timesteps))\n",
        "        self.register_buffer(\"alpha\", 1.0 - self.beta)\n",
        "        self.register_buffer(\"alpha_bar\", torch.cumprod(self.alpha, dim=0))\n",
        "\n",
        "    def q_sample(self, x_start, t, noise=None):\n",
        "        noise = torch.randn_like(x_start)\n",
        "        sqrt_alpha_bar = torch.sqrt(self.alpha_bar[t]).view(-1, 1, 1)\n",
        "        sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bar[t]).view(-1, 1, 1)\n",
        "\n",
        "        x_t = sqrt_alpha_bar * x_start + sqrt_one_minus_alpha_bar * noise\n",
        "        return x_t\n",
        "\n",
        "    def p_mean_variance(self, x, t, t_next, condition):\n",
        "        alpha_t = self.alpha[t]\n",
        "        alpha_bar_t = self.alpha_bar[t]\n",
        "        alpha_bar_t_next = self.alpha_bar[t_next]\n",
        "        beta_t = self.beta[t]\n",
        "\n",
        "        x_start_pred = self.model(x, t.long(), condition)\n",
        "\n",
        "        coeff1 = torch.sqrt(alpha_bar_t_next) * beta_t / (1 - alpha_bar_t)\n",
        "        coeff2 = torch.sqrt(alpha_t) * (1 - alpha_bar_t_next) / (1 - alpha_bar_t)\n",
        "        mean = coeff1 * x_start_pred + coeff2 * x\n",
        "        variance = (1 - alpha_bar_t_next) / (1 - alpha_bar_t) * beta_t\n",
        "        return mean, variance\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample(self, x, t, t_next, condition):\n",
        "        mean, variance = self.p_mean_variance(x, t, t_next, condition)\n",
        "        if torch.all(t_next == 0):\n",
        "            return mean\n",
        "        noise = torch.randn_like(x)\n",
        "        return mean + torch.sqrt(variance) * noise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, points, variables, train_dataset, batch_size=16, ddim_step=1):\n",
        "        if self.set_transformer:\n",
        "            points = points.transpose(1, 2)\n",
        "\n",
        "        condition = self.tnet(points) + self.vars_emb(variables)\n",
        "        shape = (batch_size, self.max_seq_len, self.n_embd)\n",
        "        x = torch.randn(shape, device=self.device)\n",
        "        steps = torch.arange(self.timesteps - 1, -1, -1, device=self.device)\n",
        "\n",
        "        for i in range(0, self.timesteps, ddim_step):\n",
        "            t = steps[i]\n",
        "            t_next = (\n",
        "                steps[i + ddim_step]\n",
        "                if i + ddim_step < self.timesteps\n",
        "                else torch.tensor(0, device=self.device)\n",
        "            )\n",
        "            x = self.p_sample(x, t, t_next, condition)\n",
        "\n",
        "            # Print prediction every 250 steps\n",
        "            # if (i + 1) % 250 == 0:\n",
        "            #    logits = self.decoder(x)  # [B, L, vocab_size]\n",
        "            #    token_indices = torch.argmax(logits, dim=-1)  # [B, L]\n",
        "            #    for j in range(batch_size):\n",
        "            #       token_indices_j = token_indices[j]  # [L]\n",
        "            #        predicted_skeleton = get_predicted_skeleton(\n",
        "            #            token_indices_j, train_dataset\n",
        "            #        )\n",
        "            #        tqdm.write(f\" sample {j}: predicted_skeleton: {predicted_skeleton}\")\n",
        "\n",
        "        logits = self.decoder(x)  # [B, L, vocab_size]\n",
        "        token_indices = torch.argmax(logits, dim=-1)  # [B, L]\n",
        "        predicted_skeletons = []\n",
        "        for j in range(batch_size):\n",
        "            token_indices_j = token_indices[j]  # [L]\n",
        "            predicted_skeleton = get_predicted_skeleton(token_indices_j, train_dataset)\n",
        "            predicted_skeletons.append(predicted_skeleton)\n",
        "        return predicted_skeletons\n",
        "\n",
        "    def p_losses(\n",
        "        self,\n",
        "        x_start,\n",
        "        points,\n",
        "        tokens,\n",
        "        variables,\n",
        "        t,\n",
        "    ):\n",
        "        noise = torch.randn_like(x_start)\n",
        "        x_t = self.q_sample(x_start, t, noise)\n",
        "\n",
        "        if self.set_transformer:\n",
        "            points = points.transpose(1, 2)\n",
        "\n",
        "        condition = self.tnet(points) + self.vars_emb(variables)\n",
        "\n",
        "        x_start_pred = self.model(x_t, t.long(), condition)\n",
        "\n",
        "        # CE loss on tokens\n",
        "        logits = self.decoder(x_start_pred)  # [B, L, vocab_size]\n",
        "\n",
        "        ce_loss = F.cross_entropy(\n",
        "            logits.view(-1, self.vocab_size),  # [B*L, vocab_size]\n",
        "            tokens.view(-1),  # [B*L]\n",
        "            ignore_index=self.padding_idx,\n",
        "            reduction=\"mean\",\n",
        "        )\n",
        "\n",
        "        ce_loss = ce_loss * self.ce_weight\n",
        "\n",
        "        return ce_loss\n",
        "\n",
        "    def forward(self, points, tokens, variables, t):\n",
        "        token_emb = self.tok_emb(tokens)\n",
        "        ce_loss = self.p_losses(token_emb, points, tokens, variables, t)\n",
        "        return ce_loss"
      ],
      "metadata": {
        "trusted": true,
        "id": "TG7pnuBWgXo5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import tqdm\n",
        "from typing import Tuple\n",
        "\n",
        "def train_epoch(\n",
        "    model: SymbolicGaussianDiffusion,\n",
        "    train_loader: DataLoader,\n",
        "    optimizer: Adam,\n",
        "    train_dataset: CharDataset,\n",
        "    timesteps: int,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        "    num_epochs: int,\n",
        ") -> Tuple[float, float, float]:\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for i, (_, tokens, points, variables) in tqdm.tqdm(\n",
        "        enumerate(train_loader),\n",
        "        total=len(train_loader),\n",
        "        desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
        "    ):\n",
        "        points, tokens, variables = (\n",
        "            points.to(device),\n",
        "            tokens.to(device),\n",
        "            variables.to(device),\n",
        "        )\n",
        "        t = torch.randint(0, timesteps, (tokens.shape[0],), device=device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss = model(points, tokens, variables, t)\n",
        "\n",
        "        if (i + 1) % 250 == 0:\n",
        "            print(f\"Batch {i + 1}/{len(train_loader)}:\")\n",
        "            print(f\"total_loss: {total_loss}\")\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += total_loss.item()\n",
        "\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    return avg_train_loss\n",
        "\n",
        "\n",
        "def val_epoch(\n",
        "    model: SymbolicGaussianDiffusion,\n",
        "    val_loader: DataLoader,\n",
        "    train_dataset: CharDataset,\n",
        "    timesteps: int,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        "    num_epochs: int,\n",
        ") -> Tuple[float, float, float]:\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, tokens, points, variables in tqdm.tqdm(\n",
        "            val_loader, total=len(val_loader), desc=\"Validating\"\n",
        "        ):\n",
        "            points, tokens, variables = (\n",
        "                points.to(device),\n",
        "                tokens.to(device),\n",
        "                variables.to(device),\n",
        "            )\n",
        "            t = torch.randint(0, timesteps, (tokens.shape[0],), device=device)\n",
        "            total_loss = model(points, tokens, variables, t)\n",
        "\n",
        "            total_val_loss += total_loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    return avg_val_loss\n",
        "\n",
        "\n",
        "def train_single_gpu(\n",
        "    model: SymbolicGaussianDiffusion,\n",
        "    train_dataset: CharDataset,\n",
        "    val_dataset: CharDataset,\n",
        "    num_epochs=10,\n",
        "    save_every=2,\n",
        "    batch_size=32,\n",
        "    timesteps=1000,\n",
        "    learning_rate=1e-3,\n",
        "    path=None,\n",
        "):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        pin_memory=True,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        pin_memory=True,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "    )\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_train_loss = train_epoch(\n",
        "            model,\n",
        "            train_loader,\n",
        "            optimizer,\n",
        "            train_dataset,\n",
        "            timesteps,\n",
        "            device,\n",
        "            epoch,\n",
        "            num_epochs,\n",
        "        )\n",
        "\n",
        "        avg_val_loss = val_epoch(\n",
        "            model, val_loader, train_dataset, timesteps, device, epoch, num_epochs\n",
        "        )\n",
        "\n",
        "        scheduler.step(avg_val_loss)\n",
        "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "        print(\"\\nEpoch Summary:\")\n",
        "        print(\n",
        "            f\"Train Total Loss: {avg_train_loss:.4f}\"\n",
        "        )\n",
        "        print(\n",
        "            f\"Val Total Loss: {avg_val_loss:.4f}\"\n",
        "        )\n",
        "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            state_dict = model.state_dict()\n",
        "            torch.save(state_dict, path)\n",
        "            print(f\"New best model saved with val loss: {best_val_loss:.4f}\")\n",
        "\n",
        "        print(\"-\" * 50)"
      ],
      "metadata": {
        "trusted": true,
        "id": "I49-ljsmgXo6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 512\n",
        "timesteps = 1000\n",
        "batch_size = 64\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 5\n",
        "blockSize = 32\n",
        "numVars = 5\n",
        "numYs = 1\n",
        "numPoints = 250\n",
        "target = 'Skeleton'\n",
        "const_range = [-2.1, 2.1]\n",
        "trainRange = [-3.0, 3.0]\n",
        "decimals = 8\n",
        "addVars = False\n",
        "maxNumFiles = 100\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "DZTUNZM7gXo6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataDir = \"/kaggle/input/1-5-var-dataset\"\n",
        "dataFolder = \"1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points\""
      ],
      "metadata": {
        "trusted": true,
        "id": "6B3uif-BgXo7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "\n",
        "path = '{}/{}/Train/*.json'.format(dataDir, dataFolder)\n",
        "files = glob.glob(path)[:maxNumFiles]\n",
        "text = processDataFiles(files)\n",
        "text = text.split('\\n') # convert the raw text to a set of examples\n",
        "# skeletons = []\n",
        "skeletons = [json.loads(item)['Skeleton'] for item in text if item.strip()]\n",
        "all_tokens = set()\n",
        "for eq in skeletons:\n",
        "    all_tokens.update(tokenize_equation(eq))\n",
        "integers = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9'}\n",
        "all_tokens.update(integers)  # add all integers to the token set\n",
        "tokens = sorted(list(all_tokens) + ['_', 'T', '<', '>', ':'])  # special tokens\n",
        "trainText = text[:-1] if len(text[-1]) == 0 else text\n",
        "random.shuffle(trainText) # shuffle the dataset, it's important specailly for the combined number of variables experiment\n",
        "train_dataset = CharDataset(trainText, blockSize, tokens=tokens, numVars=numVars,\n",
        "                        numYs=numYs, numPoints=numPoints, target=target, addVars=addVars,\n",
        "                        const_range=const_range, xRange=trainRange, decimals=decimals)\n",
        "\n",
        "idx = np.random.randint(train_dataset.__len__())\n",
        "inputs, outputs, points, variables = train_dataset.__getitem__(idx)\n",
        "inputs = ''.join([train_dataset.itos[int(i)] for i in inputs])\n",
        "outputs = ''.join([train_dataset.itos[int(i)] for i in outputs])\n",
        "print('id:{}\\noutputs:{}\\nvariables:{}'.format(idx,outputs,variables))"
      ],
      "metadata": {
        "trusted": true,
        "id": "tk1Dq9PEgXo7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "path = '{}/{}/Val/*.json'.format(dataDir,dataFolder)\n",
        "files = glob.glob(path)\n",
        "textVal = processDataFiles([files[0]])\n",
        "textVal = textVal.split('\\n') # convert the raw text to a set of examples\n",
        "val_dataset = CharDataset(textVal, blockSize, tokens=tokens, numVars=numVars,\n",
        "                        numYs=numYs, numPoints=numPoints, target=target, addVars=addVars,\n",
        "                        const_range=const_range, xRange=trainRange, decimals=decimals)\n",
        "\n",
        "# print a random sample\n",
        "idx = np.random.randint(val_dataset.__len__())\n",
        "inputs, outputs, points, variables = val_dataset.__getitem__(idx)\n",
        "print(points.min(), points.max())\n",
        "inputs = ''.join([train_dataset.itos[int(i)] for i in inputs])\n",
        "outputs = ''.join([train_dataset.itos[int(i)] for i in outputs])\n",
        "print('id:{}\\noutputs:{}\\nvariables:{}'.format(idx,outputs,variables))"
      ],
      "metadata": {
        "trusted": true,
        "id": "EGzB_RyDgXo8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pconfig = PointNetConfig(\n",
        "    embeddingSize=n_embd,\n",
        "    numberofPoints=numPoints,\n",
        "    numberofVars=numVars,\n",
        "    numberofYs=numYs,\n",
        ")\n",
        "\n",
        "model = SymbolicGaussianDiffusion(\n",
        "    tnet_config=pconfig,\n",
        "    vocab_size=train_dataset.vocab_size,\n",
        "    max_seq_len=blockSize,\n",
        "    padding_idx=train_dataset.paddingID,\n",
        "    max_num_vars=9,\n",
        "    n_layer=4,\n",
        "    n_head=4,\n",
        "    n_embd=n_embd,\n",
        "    timesteps=timesteps,\n",
        "    beta_start=0.0001,\n",
        "    beta_end=0.02\n",
        ")\n",
        "\n",
        "train_single_gpu(\n",
        "    model,\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    num_epochs=num_epochs,\n",
        "    save_every=2,\n",
        "    batch_size=batch_size,\n",
        "    timesteps=timesteps,\n",
        "    learning_rate=learning_rate,\n",
        "    path=\"1_5_var_set_transformer\"\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "4qs534iWgXo8"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}