{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec3d6e4c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-09T04:36:53.566333Z",
     "iopub.status.busy": "2025-04-09T04:36:53.566116Z",
     "iopub.status.idle": "2025-04-09T04:36:54.421756Z",
     "shell.execute_reply": "2025-04-09T04:36:54.420738Z"
    },
    "papermill": {
     "duration": 0.860991,
     "end_time": "2025-04-09T04:36:54.423048",
     "exception": false,
     "start_time": "2025-04-09T04:36:53.562057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/xye_9var/pytorch/default/1/XYE_9Var.pt\n",
      "/kaggle/input/symbolic_diffusion_initial/pytorch/default/1/symbolic_diffusion_model.pth\n",
      "/kaggle/input/1-var-dataset/1_var_test.json\n",
      "/kaggle/input/1-var-dataset/1_var_val.json\n",
      "/kaggle/input/1-var-dataset/1_var_train.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/config.txt\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Val/0_2_0_13062021_184140.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Test/0_2_0_13062021_184319.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/4_2_0_15062021_123153.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/2_2_0_15062021_123153.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/7_2_0_15062021_123153.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/5_2_0_15062021_123153.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/0_2_0_15062021_123153.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/6_2_0_15062021_123153.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/1_2_0_15062021_123153.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/3_2_0_15062021_123153.json\n",
      "/kaggle/input/xye_1var/pytorch/default/1/XYE_1Var.pt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c7a64cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T04:36:54.430586Z",
     "iopub.status.busy": "2025-04-09T04:36:54.430210Z",
     "iopub.status.idle": "2025-04-09T04:36:58.843385Z",
     "shell.execute_reply": "2025-04-09T04:36:58.842719Z"
    },
    "papermill": {
     "duration": 4.418488,
     "end_time": "2025-04-09T04:36:58.844973",
     "exception": false,
     "start_time": "2025-04-09T04:36:54.426485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.optimize import minimize\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def generateDataStrEq(\n",
    "    eq, n_points=2, n_vars=3, decimals=4, supportPoints=None, min_x=0, max_x=3\n",
    "):\n",
    "    X = []\n",
    "    Y = []\n",
    "    # TODO: Need to make this faster\n",
    "    for p in range(n_points):\n",
    "        if supportPoints is None:\n",
    "            if type(min_x) == list:\n",
    "                x = []\n",
    "                for _ in range(n_vars):\n",
    "                    idx = np.random.randint(len(min_x))\n",
    "                    x += list(\n",
    "                        np.round(np.random.uniform(min_x[idx], max_x[idx], 1), decimals)\n",
    "                    )\n",
    "            else:\n",
    "                x = list(np.round(np.random.uniform(min_x, max_x, n_vars), decimals))\n",
    "            assert (\n",
    "                len(x) != 0\n",
    "            ), \"For some reason, we didn't generate the points correctly!\"\n",
    "        else:\n",
    "            x = supportPoints[p]\n",
    "\n",
    "        tmpEq = eq + \"\"\n",
    "        for nVID in range(n_vars):\n",
    "            tmpEq = tmpEq.replace(\"x{}\".format(nVID + 1), str(x[nVID]))\n",
    "        y = float(np.round(eval(tmpEq), decimals))\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# def processDataFiles(files):\n",
    "#     text = \"\"\n",
    "#     for f in tqdm(files):\n",
    "#         with open(f, 'r') as h:\n",
    "#             lines = h.read() # don't worry we won't run out of file handles\n",
    "#             if lines[-1]==-1:\n",
    "#                 lines = lines[:-1]\n",
    "#             #text += lines #json.loads(line)\n",
    "#             text = ''.join([lines,text])\n",
    "#     return text\n",
    "\n",
    "\n",
    "def processDataFiles(files):\n",
    "    text = \"\"\n",
    "    for f in files:\n",
    "        with open(f, \"r\") as h:\n",
    "            lines = h.read()  # don't worry we won't run out of file handles\n",
    "            if lines[-1] == -1:\n",
    "                lines = lines[:-1]\n",
    "            # text += lines #json.loads(line)\n",
    "            text = \"\".join([lines, text])\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize_equation(eq):\n",
    "    token_spec = [\n",
    "        (r\"\\*\\*\"),  # exponentiation\n",
    "        (r\"exp\"),  # exp function\n",
    "        (r\"[+\\-*/=()]\"),  # operators and parentheses\n",
    "        (r\"sin\"),  # sin function\n",
    "        (r\"cos\"),  # cos function\n",
    "        (r\"log\"),  # log function\n",
    "        (r\"x\\d+\"),  # variables like x1, x23, etc.\n",
    "        (r\"C\"),  # constants placeholder\n",
    "        (r\"-?\\d+\\.\\d+\"),  # decimal numbers\n",
    "        (r\"-?\\d+\"),  # integers\n",
    "        (r\"_\"),  # padding token\n",
    "    ]\n",
    "    token_regex = \"|\".join(f\"({pattern})\" for pattern in token_spec)\n",
    "    matches = re.finditer(token_regex, eq)\n",
    "    return [match.group(0) for match in matches]\n",
    "\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        block_size,\n",
    "        tokens,\n",
    "        numVars,\n",
    "        numYs,\n",
    "        numPoints,\n",
    "        target=\"Skeleton\",\n",
    "        addVars=False,\n",
    "        const_range=[-0.4, 0.4],\n",
    "        xRange=[-3.0, 3.0],\n",
    "        decimals=4,\n",
    "        augment=False,\n",
    "    ):\n",
    "\n",
    "        data_size, vocab_size = len(data), len(tokens)\n",
    "        print(\"data has %d examples, %d unique.\" % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = {tok: i for i, tok in enumerate(tokens)}\n",
    "        self.itos = {i: tok for i, tok in enumerate(tokens)}\n",
    "\n",
    "        self.numVars = numVars\n",
    "        self.numYs = numYs\n",
    "        self.numPoints = numPoints\n",
    "\n",
    "        # padding token\n",
    "        self.paddingToken = \"_\"\n",
    "        self.paddingID = self.stoi[\"_\"]  # or another ID not already used\n",
    "        self.stoi[self.paddingToken] = self.paddingID\n",
    "        self.itos[self.paddingID] = self.paddingToken\n",
    "\n",
    "        self.threshold = [-1000, 1000]\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data  # it should be a list of examples\n",
    "        self.target = target\n",
    "        self.addVars = addVars\n",
    "\n",
    "        self.const_range = const_range\n",
    "        self.xRange = xRange\n",
    "        self.decimals = decimals\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab an example from the data\n",
    "        chunk = self.data[idx]  # sequence of tokens including x, y, eq, etc.\n",
    "\n",
    "        try:\n",
    "            chunk = json.loads(chunk)  # convert the sequence tokens to a dictionary\n",
    "        except Exception as e:\n",
    "            print(\"Couldn't convert to json: {} \\n error is: {}\".format(chunk, e))\n",
    "            # try the previous example\n",
    "            idx = idx - 1\n",
    "            idx = idx if idx >= 0 else 0\n",
    "            chunk = self.data[idx]\n",
    "            chunk = json.loads(chunk)  # convert the sequence tokens to a dictionary\n",
    "\n",
    "        # find the number of variables in the equation\n",
    "        printInfoCondition = random.random() < 0.0000001\n",
    "        eq = chunk[self.target]\n",
    "        if printInfoCondition:\n",
    "            print(f\"\\nEquation: {eq}\")\n",
    "        vars = re.finditer(\"x[\\d]+\", eq)\n",
    "        numVars = 0\n",
    "        for v in vars:\n",
    "            v = v.group(0).strip(\"x\")\n",
    "            v = eval(v)\n",
    "            v = int(v)\n",
    "            if v > numVars:\n",
    "                numVars = v\n",
    "\n",
    "        if self.target == \"Skeleton\" and self.augment:\n",
    "            threshold = 5000\n",
    "            # randomly generate the constants\n",
    "            cleanEqn = \"\"\n",
    "            for chr in eq:\n",
    "                if chr == \"C\":\n",
    "                    # genereate a new random number\n",
    "                    chr = \"{}\".format(\n",
    "                        np.random.uniform(self.const_range[0], self.const_range[1])\n",
    "                    )\n",
    "                cleanEqn += chr\n",
    "\n",
    "            # update the points\n",
    "            nPoints = np.random.randint(\n",
    "                *self.numPoints\n",
    "            )  # if supportPoints is None else len(supportPoints)\n",
    "            try:\n",
    "                if printInfoCondition:\n",
    "                    print(\"Org:\", chunk[\"X\"], chunk[\"Y\"])\n",
    "\n",
    "                X, y = generateDataStrEq(\n",
    "                    cleanEqn,\n",
    "                    n_points=nPoints,\n",
    "                    n_vars=self.numVars,\n",
    "                    decimals=self.decimals,\n",
    "                    min_x=self.xRange[0],\n",
    "                    max_x=self.xRange[1],\n",
    "                )\n",
    "\n",
    "                # replace out of threshold with maximum numbers\n",
    "                y = [e if abs(e) < threshold else np.sign(e) * threshold for e in y]\n",
    "\n",
    "                # check if there is nan/inf/very large numbers in the y\n",
    "                conditions = (\n",
    "                    (np.isnan(y).any() or np.isinf(y).any())\n",
    "                    or len(y) == 0\n",
    "                    or (abs(min(y)) > threshold or abs(max(y)) > threshold)\n",
    "                )\n",
    "                if not conditions:\n",
    "                    chunk[\"X\"], chunk[\"Y\"] = X, y\n",
    "\n",
    "                if printInfoCondition:\n",
    "                    print(\"Evd:\", chunk[\"X\"], chunk[\"Y\"])\n",
    "            except Exception as e:\n",
    "                # for different reason this might happend including but not limited to division by zero\n",
    "                print(\n",
    "                    \"\".join(\n",
    "                        [\n",
    "                            f\"We just used the original equation and support points because of {e}. \",\n",
    "                            f\"The equation is {eq}, and we update the equation to {cleanEqn}\",\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # encode every character in the equation to an integer\n",
    "        # < is SOS, > is EOS\n",
    "        if self.addVars:\n",
    "            dix = [self.stoi[s] for s in \"<\" + str(numVars) + \":\" + eq + \">\"]\n",
    "        else:\n",
    "            eq_tokens = tokenize_equation(eq)\n",
    "            if self.addVars:\n",
    "                token_seq = [\"<\", str(numVars), \":\", *eq_tokens, \">\"]\n",
    "            else:\n",
    "                token_seq = [\"<\", *eq_tokens, \">\"]\n",
    "            dix = [self.stoi[tok] for tok in token_seq]\n",
    "\n",
    "        inputs = dix[:-1]\n",
    "        outputs = dix[1:]\n",
    "\n",
    "        # add the padding to the equations\n",
    "        paddingSize = max(self.block_size - len(inputs), 0)\n",
    "        paddingList = [self.paddingID] * paddingSize\n",
    "        inputs += paddingList\n",
    "        outputs += paddingList\n",
    "\n",
    "        # make sure it is not more than what should be\n",
    "        inputs = inputs[: self.block_size]\n",
    "        outputs = outputs[: self.block_size]\n",
    "\n",
    "        points = torch.zeros(self.numVars + self.numYs, self.numPoints - 1)\n",
    "        for idx, xy in enumerate(zip(chunk[\"X\"], chunk[\"Y\"])):\n",
    "\n",
    "            if not isinstance(xy[0], list) or not isinstance(\n",
    "                xy[1], (list, float, np.float64)\n",
    "            ):\n",
    "                print(f\"Unexpected types: {type(xy[0])}, {type(xy[1])}\")\n",
    "                continue  # Skip if types are incorrect\n",
    "\n",
    "            # don't let to exceed the maximum number of points\n",
    "            if idx >= self.numPoints - 1:\n",
    "                break\n",
    "\n",
    "            x = xy[0]\n",
    "            x = x + [0] * (max(self.numVars - len(x), 0))  # padding\n",
    "\n",
    "            y = [xy[1]] if type(xy[1]) == float or type(xy[1]) == np.float64 else xy[1]\n",
    "\n",
    "            y = y + [0] * (max(self.numYs - len(y), 0))  # padding\n",
    "            p = x + y  # because it is only one point\n",
    "            p = torch.tensor(p)\n",
    "            # replace nan and inf\n",
    "            p = torch.nan_to_num(\n",
    "                p,\n",
    "                nan=self.threshold[1],\n",
    "                posinf=self.threshold[1],\n",
    "                neginf=self.threshold[0],\n",
    "            )\n",
    "\n",
    "            points[:, idx] = p\n",
    "\n",
    "        points = torch.nan_to_num(\n",
    "            points,\n",
    "            nan=self.threshold[1],\n",
    "            posinf=self.threshold[1],\n",
    "            neginf=self.threshold[0],\n",
    "        )\n",
    "\n",
    "        inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "        outputs = torch.tensor(outputs, dtype=torch.long)\n",
    "        numVars = torch.tensor(numVars, dtype=torch.long)\n",
    "        return inputs, outputs, points, numVars\n",
    "\n",
    "\n",
    "# Relative Mean Square Error\n",
    "def relativeErr(y, yHat, info=False, eps=1e-5):\n",
    "    yHat = np.reshape(yHat, [1, -1])[0]\n",
    "    y = np.reshape(y, [1, -1])[0]\n",
    "    if len(y) > 0 and len(y) == len(yHat):\n",
    "        err = ((yHat - y)) ** 2 / np.linalg.norm(y + eps)\n",
    "        if info:\n",
    "            for _ in range(5):\n",
    "                i = np.random.randint(len(y))\n",
    "                # print(\"yPR,yTrue:{},{}, Err:{}\".format(yHat[i], y[i], err[i]))\n",
    "    else:\n",
    "        err = 100\n",
    "\n",
    "    return np.mean(err)\n",
    "\n",
    "\n",
    "def lossFunc(constants, eq, X, Y, eps=1e-5):\n",
    "    err = 0\n",
    "    eq = eq.replace(\"C\", \"{}\").format(*constants)\n",
    "\n",
    "    for x, y in zip(X, Y):\n",
    "        eqTemp = eq + \"\"\n",
    "        if type(x) == np.float32:\n",
    "            x = [x]\n",
    "        for i, e in enumerate(x):\n",
    "            # make sure e is not a tensor\n",
    "            if type(e) == torch.Tensor:\n",
    "                e = e.item()\n",
    "            eqTemp = eqTemp.replace(\"x{}\".format(i + 1), str(e))\n",
    "        try:\n",
    "            yHat = eval(eqTemp)\n",
    "        except:\n",
    "            # print(\"Exception has been occured! EQ: {}, OR: {}\".format(eqTemp, eq))\n",
    "            yHat = 100\n",
    "        try:\n",
    "            # handle overflow\n",
    "            err += relativeErr(y, yHat)  # (y-yHat)**2\n",
    "        except:\n",
    "            # print(\n",
    "            #    \"Exception has been occured! EQ: {}, OR: {}, y:{}-yHat:{}\".format(\n",
    "            #        eqTemp, eq, y, yHat\n",
    "            #    )\n",
    "            # )\n",
    "            err += 10\n",
    "\n",
    "    err /= len(Y)\n",
    "    return err\n",
    "\n",
    "\n",
    "def get_predicted_skeleton(generated_tokens, train_dataset: CharDataset):\n",
    "    predicted_tokens = generated_tokens.cpu().numpy()\n",
    "    predicted = \"\".join([train_dataset.itos[int(idx)] for idx in predicted_tokens])\n",
    "    predicted = predicted.strip(train_dataset.paddingToken).split(\">\")\n",
    "    predicted = predicted[0] if len(predicted[0]) >= 1 else predicted[1]\n",
    "    predicted = predicted.strip(\"<\").strip(\">\")\n",
    "    predicted = predicted.replace(\"Ce\", \"C*e\")\n",
    "\n",
    "    return predicted\n",
    "\n",
    "\n",
    "def sample_skeleton(model, points, variables, train_dataset, batch_size, ddim_step=20):\n",
    "    \"\"\"Sample skeletons from the model using DDIM or DDPM.\"\"\"\n",
    "    return model.sample(\n",
    "        points, variables, train_dataset, batch_size=batch_size, ddim_step=ddim_step\n",
    "    )\n",
    "\n",
    "\n",
    "def fit_constants(predicted_skeleton, X, Y):\n",
    "    \"\"\"Fit constants in the predicted skeleton using optimization.\"\"\"\n",
    "    c = [1.0 for i, x in enumerate(predicted_skeleton) if x == \"C\"]\n",
    "    b = [(-2, 2) for _, x in enumerate(predicted_skeleton) if x == \"C\"]\n",
    "    predicted = predicted_skeleton\n",
    "    if len(c) != 0:\n",
    "        try:\n",
    "            cHat = minimize(lossFunc, c, args=(predicted_skeleton, X, Y), bounds=b)\n",
    "            if cHat.success and cHat.fun != float(\"inf\"):\n",
    "                predicted = predicted_skeleton.replace(\"C\", \"{}\").format(*cHat.x)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid predicted equation or optimization failed: {predicted_skeleton}\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            raise ValueError(\n",
    "                f\"Error fitting constants: {e}, Equation: {predicted_skeleton}\"\n",
    "            )\n",
    "    return predicted\n",
    "\n",
    "\n",
    "def evaluate_equation(eq, xs, target=True):\n",
    "    \"\"\"Evaluate an equation at given points xs.\"\"\"\n",
    "    SAFE_GLOBALS = {\n",
    "        \"sin\": math.sin,\n",
    "        \"cos\": math.cos,\n",
    "        \"tan\": math.tan,\n",
    "        \"log\": math.log,\n",
    "        \"exp\": math.exp,\n",
    "        \"sqrt\": math.sqrt,\n",
    "        \"abs\": abs,\n",
    "        \"pow\": pow,\n",
    "        \"__builtins__\": {},\n",
    "    }\n",
    "    try:\n",
    "        eq_tmp = eq.replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "        for i, x in enumerate(xs):\n",
    "            eq_tmp = eq_tmp.replace(f\"x{i+1}\", str(x))\n",
    "            if \",\" in eq_tmp:\n",
    "                raise ValueError(\"There is a , in the equation!\")\n",
    "        y = eval(eq_tmp, SAFE_GLOBALS)\n",
    "        y = 0 if np.isnan(y) else y\n",
    "        y = 100 if np.isinf(y) else y\n",
    "    except Exception as e:\n",
    "        if target:\n",
    "            # print(f\"TA: Evaluation failed for equation: {eq_tmp}, Reason: {e}\")\n",
    "            pass\n",
    "        else:\n",
    "            # print(f\"PR: Evaluation failed for equation: {eq_tmp}, Reason: {e}\")\n",
    "            pass\n",
    "        y = 100\n",
    "    return y\n",
    "\n",
    "\n",
    "def evaluate_sample(target_eq, predicted_eq, test_points):\n",
    "    \"\"\"Evaluate target and predicted equations over test points.\"\"\"\n",
    "    Ys, Yhats = [], []\n",
    "    for xs in test_points:\n",
    "        Ys.append(evaluate_equation(target_eq, xs, target=True))\n",
    "        Yhats.append(evaluate_equation(predicted_eq, xs, target=False))\n",
    "    return Ys, Yhats\n",
    "\n",
    "\n",
    "def sample_and_evaluate(\n",
    "    model, points, variables, train_dataset, target_eq, t, ddim_step=None\n",
    "):\n",
    "    \"\"\"Sample a skeleton, fit constants, evaluate, and compute error.\"\"\"\n",
    "    predicted_skeleton = sample_skeleton(\n",
    "        model, points, variables, train_dataset, batch_size=1, ddim_step=ddim_step\n",
    "    )[0]\n",
    "\n",
    "    predicted = fit_constants(predicted_skeleton, t[\"X\"], t[\"Y\"])\n",
    "\n",
    "    Ys, Yhats = evaluate_sample(target_eq, predicted, t[\"XT\"])\n",
    "\n",
    "    err = relativeErr(Ys, Yhats, info=True)\n",
    "    return predicted_skeleton, predicted, err\n",
    "\n",
    "\n",
    "def sample_and_select_best(\n",
    "    model,\n",
    "    points,\n",
    "    variables,\n",
    "    train_dataset,\n",
    "    target_eq,\n",
    "    t,\n",
    "    num_tests,\n",
    "    ddim_step=None,\n",
    "):\n",
    "    \"\"\"Sample num_tests times and select the best sample based on error.\"\"\"\n",
    "    best_err = 10000000\n",
    "    best_predicted_skeleton = \"C\"\n",
    "    best_predicted = \"C\"\n",
    "\n",
    "    for _ in range(num_tests):\n",
    "        predicted_skeleton, predicted, err = sample_and_evaluate(\n",
    "            model, points, variables, train_dataset, target_eq, t, ddim_step\n",
    "        )\n",
    "        if err < best_err:\n",
    "            best_err = err\n",
    "            best_predicted_skeleton = predicted_skeleton\n",
    "            best_predicted = predicted\n",
    "\n",
    "    return best_predicted_skeleton, best_predicted, best_err\n",
    "\n",
    "\n",
    "def plot_and_save_results(\n",
    "    resultDict, fName, pconf, titleTemplate, textTest, modelKey=\"SymbolicDiffusion\"\n",
    "):\n",
    "    \"\"\"Plot cumulative error distribution and save results to a file.\n",
    "    Args:\n",
    "        resultDict: dict with results (e.g., {'err': [], 'trg': [], 'prd': []})\n",
    "        fName: str, output file name\n",
    "        pconf: PointNetConfig object with numberofVars\n",
    "        titleTemplate: str, template for plot title\n",
    "        textTest: list of test data\n",
    "        modelKey: str, key for the model in resultDict\n",
    "    \"\"\"\n",
    "    if isinstance(resultDict, dict):\n",
    "        num_eqns = len(resultDict[fName][modelKey][\"err\"])\n",
    "        num_vars = pconf.numberofVars\n",
    "        title = titleTemplate.format(num_eqns, num_vars)\n",
    "\n",
    "        models = list(\n",
    "            key\n",
    "            for key in resultDict[fName].keys()\n",
    "            if len(resultDict[fName][key][\"err\"]) == num_eqns\n",
    "        )\n",
    "        lists_of_error_scores = [\n",
    "            resultDict[fName][key][\"err\"]\n",
    "            for key in models\n",
    "            if len(resultDict[fName][key][\"err\"]) == num_eqns\n",
    "        ]\n",
    "        linestyles = [\"-\", \"dashdot\", \"dotted\", \"--\"]\n",
    "\n",
    "        eps = 0.00001\n",
    "        y, x, _ = plt.hist(\n",
    "            [\n",
    "                np.log([max(min(x + eps, 1e5), 1e-5) for x in e])\n",
    "                for e in lists_of_error_scores\n",
    "            ],\n",
    "            label=models,\n",
    "            cumulative=True,\n",
    "            histtype=\"step\",\n",
    "            bins=2000,\n",
    "            density=True,\n",
    "            log=False,\n",
    "        )\n",
    "        y = np.expand_dims(y, 0)\n",
    "\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for idx, m in enumerate(models):\n",
    "            plt.plot(x[:-1], y[idx] * 100, linestyle=linestyles[idx], label=m)\n",
    "\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Log of Relative Mean Square Error\")\n",
    "        plt.ylabel(\"Normalized Cumulative Frequency\")\n",
    "\n",
    "        name = \"{}.png\".format(fName.split(\".txt\")[0])\n",
    "        plt.savefig(name)\n",
    "        plt.close()\n",
    "\n",
    "        with open(fName, \"w\", encoding=\"utf-8\") as o:\n",
    "            for i in range(num_eqns):\n",
    "                err = resultDict[fName][modelKey][\"err\"][i]\n",
    "                eq = resultDict[fName][modelKey][\"trg\"][i]\n",
    "                predicted = resultDict[fName][modelKey][\"prd\"][i]\n",
    "                print(f\"Test Case {i}.\")\n",
    "                print(f\"Target: {eq}\\nSkeleton: {predicted}\")\n",
    "                print(f\"Err: {err}\\n\")\n",
    "\n",
    "                o.write(f\"Test Case {i}/{len(textTest)-1}.\\n\")\n",
    "                o.write(f\"{eq}\\n\")\n",
    "                o.write(f\"{modelKey}:\\n\")\n",
    "                o.write(f\"{predicted}\\n{err}\\n\\n\")\n",
    "\n",
    "            avg_err = np.mean(resultDict[fName][modelKey][\"err\"])\n",
    "            o.write(f\"Avg Err: {avg_err}\\n\")\n",
    "            print(f\"Avg Err: {avg_err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7836d7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T04:36:58.852122Z",
     "iopub.status.busy": "2025-04-09T04:36:58.851800Z",
     "iopub.status.idle": "2025-04-09T04:36:58.882498Z",
     "shell.execute_reply": "2025-04-09T04:36:58.881875Z"
    },
    "papermill": {
     "duration": 0.035407,
     "end_time": "2025-04-09T04:36:58.883593",
     "exception": false,
     "start_time": "2025-04-09T04:36:58.848186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "# from SymbolicGPT: https://github.com/mojivalipour/symbolicgpt/blob/master/models.py\n",
    "class PointNetConfig:\n",
    "    \"\"\"base PointNet config\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddingSize,\n",
    "        numberofPoints,\n",
    "        numberofVars,\n",
    "        numberofYs,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.embeddingSize = embeddingSize\n",
    "        self.numberofPoints = numberofPoints  # number of points\n",
    "        self.numberofVars = numberofVars  # input dimension (Xs)\n",
    "        self.numberofYs = numberofYs  # output dimension (Ys)\n",
    "\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "\n",
    "class tNet(nn.Module):\n",
    "    \"\"\"\n",
    "    The PointNet structure in the orginal PointNet paper:\n",
    "    PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation by Qi et. al. 2017\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(tNet, self).__init__()\n",
    "\n",
    "        self.activation_func = F.relu\n",
    "        self.num_units = config.embeddingSize\n",
    "\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            config.numberofVars + config.numberofYs, self.num_units, 1\n",
    "        )\n",
    "        self.conv2 = nn.Conv1d(self.num_units, 2 * self.num_units, 1)\n",
    "        self.conv3 = nn.Conv1d(2 * self.num_units, 4 * self.num_units, 1)\n",
    "        self.fc1 = nn.Linear(4 * self.num_units, 2 * self.num_units)\n",
    "        self.fc2 = nn.Linear(2 * self.num_units, self.num_units)\n",
    "\n",
    "        # self.relu = nn.ReLU()\n",
    "\n",
    "        self.input_batch_norm = nn.BatchNorm1d(config.numberofVars + config.numberofYs)\n",
    "        # self.input_layer_norm = nn.LayerNorm(config.numberofPoints)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(self.num_units)\n",
    "        self.bn2 = nn.BatchNorm1d(2 * self.num_units)\n",
    "        self.bn3 = nn.BatchNorm1d(4 * self.num_units)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * self.num_units)\n",
    "        self.bn5 = nn.BatchNorm1d(self.num_units)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch, #features, #points]\n",
    "        :return:\n",
    "            logit: [batch, embedding_size]\n",
    "        \"\"\"\n",
    "        x = self.input_batch_norm(x)\n",
    "        x = self.activation_func(self.bn1(self.conv1(x)))\n",
    "        x = self.activation_func(self.bn2(self.conv2(x)))\n",
    "        x = self.activation_func(self.bn3(self.conv3(x)))\n",
    "        x, _ = torch.max(x, dim=2)  # global max pooling\n",
    "        assert x.size(1) == 4 * self.num_units\n",
    "\n",
    "        x = self.activation_func(self.bn4(self.fc1(x)))\n",
    "        x = self.activation_func(self.bn5(self.fc2(x)))\n",
    "        # x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# from https://github.com/juho-lee/set_transformer/blob/master/modules.py\n",
    "class MAB(nn.Module):\n",
    "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False):\n",
    "        super(MAB, self).__init__()\n",
    "        self.dim_V = dim_V\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
    "        self.fc_k = nn.Linear(dim_K, dim_V)\n",
    "        self.fc_v = nn.Linear(dim_K, dim_V)\n",
    "        if ln:\n",
    "            self.ln0 = nn.LayerNorm(dim_V)\n",
    "            self.ln1 = nn.LayerNorm(dim_V)\n",
    "        self.fc_o = nn.Linear(dim_V, dim_V)\n",
    "\n",
    "    def forward(self, Q, K):\n",
    "        Q = self.fc_q(Q)\n",
    "        K, V = self.fc_k(K), self.fc_v(K)\n",
    "\n",
    "        dim_split = self.dim_V // self.num_heads\n",
    "        Q_ = torch.cat(Q.split(dim_split, 2), 0)\n",
    "        K_ = torch.cat(K.split(dim_split, 2), 0)\n",
    "        V_ = torch.cat(V.split(dim_split, 2), 0)\n",
    "\n",
    "        A = torch.softmax(Q_.bmm(K_.transpose(1, 2)) / math.sqrt(self.dim_V), 2)\n",
    "        O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), 0), 2)\n",
    "        O = O if getattr(self, \"ln0\", None) is None else self.ln0(O)\n",
    "        O = O + F.relu(self.fc_o(O))\n",
    "        O = O if getattr(self, \"ln1\", None) is None else self.ln1(O)\n",
    "        return O\n",
    "\n",
    "\n",
    "class SAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, ln=False):\n",
    "        super(SAB, self).__init__()\n",
    "        self.mab = MAB(dim_in, dim_in, dim_out, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.mab(X, X)\n",
    "\n",
    "\n",
    "class ISAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, num_inds, ln=False):\n",
    "        super(ISAB, self).__init__()\n",
    "        self.I = nn.Parameter(torch.Tensor(1, num_inds, dim_out))\n",
    "        nn.init.xavier_uniform_(self.I)\n",
    "        self.mab0 = MAB(dim_out, dim_in, dim_out, num_heads, ln=ln)\n",
    "        self.mab1 = MAB(dim_in, dim_out, dim_out, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        H = self.mab0(self.I.repeat(X.size(0), 1, 1), X)\n",
    "        return self.mab1(X, H)\n",
    "\n",
    "\n",
    "class PMA(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_seeds, ln=False):\n",
    "        super(PMA, self).__init__()\n",
    "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
    "        nn.init.xavier_uniform_(self.S)\n",
    "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.mab(self.S.repeat(X.size(0), 1, 1), X)\n",
    "\n",
    "\n",
    "# from https://github.com/juho-lee/set_transformer/blob/master/models.py\n",
    "class SetTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_input,\n",
    "        num_outputs,\n",
    "        dim_output,\n",
    "        num_inds=32,\n",
    "        dim_hidden=128,\n",
    "        num_heads=4,\n",
    "        ln=False,\n",
    "    ):\n",
    "        super(SetTransformer, self).__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            ISAB(dim_input, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "            ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            PMA(dim_hidden, num_heads, num_outputs, ln=ln),\n",
    "            SAB(dim_hidden, dim_hidden, num_heads, ln=ln),\n",
    "            SAB(dim_hidden, dim_hidden, num_heads, ln=ln),\n",
    "            nn.Linear(dim_hidden, dim_output),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dec(self.enc(X)).squeeze(1)\n",
    "\n",
    "\n",
    "class NoisePredictionTransformer(nn.Module):\n",
    "    def __init__(self, n_embd, max_seq_len, n_layer=6, n_head=8, max_timesteps=1000):\n",
    "        super().__init__()\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, max_seq_len, n_embd))\n",
    "        self.time_emb = nn.Embedding(max_timesteps, n_embd)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=n_embd,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=n_embd * 4,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layer)\n",
    "\n",
    "    def forward(self, x_t, t, condition):\n",
    "        _, L, _ = x_t.shape\n",
    "        pos_emb = self.pos_emb[:, :L, :]  # [1, L, n_embd]\n",
    "        time_emb = self.time_emb(t)\n",
    "        if time_emb.dim() == 1:  # Scalar t case, [n_embd]\n",
    "            time_emb = time_emb.unsqueeze(0)  # [1, n_embd]\n",
    "        time_emb = time_emb.unsqueeze(1)  # [1, 1, n_embd]\n",
    "        condition = condition.unsqueeze(1)  # [B, 1, n_embd]\n",
    "\n",
    "        x = x_t + pos_emb + time_emb + condition\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "# influenced by https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/simple_diffusion.py\n",
    "class SymbolicGaussianDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tnet_config: PointNetConfig,\n",
    "        vocab_size,\n",
    "        max_seq_len,\n",
    "        padding_idx: int = 0,\n",
    "        max_num_vars: int = 9,\n",
    "        n_layer=6,\n",
    "        n_head=8,\n",
    "        n_embd=512,\n",
    "        timesteps=1000,\n",
    "        beta_start=0.0001,\n",
    "        beta_end=0.02,\n",
    "        ce_weight=1.0,  # Weight for CE loss relative to MSE\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.padding_idx = padding_idx\n",
    "        self.n_embd = n_embd\n",
    "        self.timesteps = timesteps\n",
    "        self.ce_weight = ce_weight\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd, padding_idx=self.padding_idx)\n",
    "        self.vars_emb = nn.Embedding(max_num_vars, n_embd)\n",
    "\n",
    "        self.decoder = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        self.decoder.weight = self.tok_emb.weight\n",
    "\n",
    "        dim_input = tnet_config.numberofVars + tnet_config.numberofYs\n",
    "        self.tnet = SetTransformer(\n",
    "            dim_input=dim_input,\n",
    "            num_outputs=1,\n",
    "            dim_output=n_embd,\n",
    "            num_inds=tnet_config.numberofPoints,\n",
    "            dim_hidden=tnet_config.embeddingSize,\n",
    "            num_heads=4,\n",
    "        )\n",
    "        self.model = NoisePredictionTransformer(\n",
    "            n_embd, max_seq_len, n_layer, n_head, timesteps\n",
    "        )\n",
    "\n",
    "        # Noise schedule\n",
    "        self.register_buffer(\"beta\", torch.linspace(beta_start, beta_end, timesteps))\n",
    "        self.register_buffer(\"alpha\", 1.0 - self.beta)\n",
    "        self.register_buffer(\"alpha_bar\", torch.cumprod(self.alpha, dim=0))\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        noise = torch.randn_like(x_start)\n",
    "        sqrt_alpha_bar = torch.sqrt(self.alpha_bar[t]).view(-1, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bar[t]).view(-1, 1, 1)\n",
    "\n",
    "        x_t = sqrt_alpha_bar * x_start + sqrt_one_minus_alpha_bar * noise\n",
    "        return x_t\n",
    "\n",
    "    def p_mean_variance(self, x, t, t_next, condition):\n",
    "        alpha_t = self.alpha[t]\n",
    "        alpha_bar_t = self.alpha_bar[t]\n",
    "        alpha_bar_t_next = self.alpha_bar[t_next]\n",
    "        beta_t = self.beta[t]\n",
    "\n",
    "        x_start_pred = self.model(x, t.long(), condition)\n",
    "\n",
    "        coeff1 = torch.sqrt(alpha_bar_t_next) * beta_t / (1 - alpha_bar_t)\n",
    "        coeff2 = torch.sqrt(alpha_t) * (1 - alpha_bar_t_next) / (1 - alpha_bar_t)\n",
    "        mean = coeff1 * x_start_pred + coeff2 * x\n",
    "        variance = (1 - alpha_bar_t_next) / (1 - alpha_bar_t) * beta_t\n",
    "        return mean, variance\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t, t_next, condition):\n",
    "        mean, variance = self.p_mean_variance(x, t, t_next, condition)\n",
    "        if torch.all(t_next == 0):\n",
    "            return mean\n",
    "        noise = torch.randn_like(x)\n",
    "        return mean + torch.sqrt(variance) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, points, variables, train_dataset, batch_size=16, ddim_step=0):\n",
    "        points = points.transpose(1, 2)\n",
    "        condition = self.tnet(points) + self.vars_emb(variables)\n",
    "        shape = (batch_size, self.max_seq_len, self.n_embd)\n",
    "        x = torch.randn(shape, device=self.device)\n",
    "        steps = torch.arange(self.timesteps - 1, -1, -1, device=self.device)\n",
    "\n",
    "        for i in range(0, self.timesteps, ddim_step):\n",
    "            t = steps[i]\n",
    "            t_next = (\n",
    "                steps[i + ddim_step]\n",
    "                if i + ddim_step < self.timesteps\n",
    "                else torch.tensor(0, device=self.device)\n",
    "            )\n",
    "            x = self.p_sample(x, t, t_next, condition)\n",
    "\n",
    "            # Print prediction every 250 steps\n",
    "            # if (i + 1) % 250 == 0:\n",
    "            #    logits = self.decoder(x)  # [B, L, vocab_size]\n",
    "            #    token_indices = torch.argmax(logits, dim=-1)  # [B, L]\n",
    "            #    for j in range(batch_size):\n",
    "            #       token_indices_j = token_indices[j]  # [L]\n",
    "            #        predicted_skeleton = get_predicted_skeleton(\n",
    "            #            token_indices_j, train_dataset\n",
    "            #        )\n",
    "            #        tqdm.write(f\" sample {j}: predicted_skeleton: {predicted_skeleton}\")\n",
    "\n",
    "        logits = self.decoder(x)  # [B, L, vocab_size]\n",
    "        token_indices = torch.argmax(logits, dim=-1)  # [B, L]\n",
    "        predicted_skeletons = []\n",
    "        for j in range(batch_size):\n",
    "            token_indices_j = token_indices[j]  # [L]\n",
    "            predicted_skeleton = get_predicted_skeleton(token_indices_j, train_dataset)\n",
    "            predicted_skeletons.append(predicted_skeleton)\n",
    "        return predicted_skeletons\n",
    "\n",
    "    def p_losses(\n",
    "        self, x_start, points, tokens, variables, t, noise=None, mse: bool = False\n",
    "    ):\n",
    "        \"\"\"Hybrid loss: MSE on embeddings + CE on tokens.\"\"\"\n",
    "        noise = torch.randn_like(x_start)\n",
    "        x_t = self.q_sample(x_start, t, noise)\n",
    "        points = points.transpose(1, 2)\n",
    "        condition = self.tnet(points) + self.vars_emb(variables)\n",
    "        x_start_pred = self.model(x_t, t.long(), condition)\n",
    "\n",
    "        # MSE loss on embeddings\n",
    "        if mse:\n",
    "            mse_loss = F.mse_loss(x_start_pred, x_start)\n",
    "        else:\n",
    "            mse_loss = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        # CE loss on tokens\n",
    "        logits = self.decoder(x_start_pred)  # [B, L, vocab_size]\n",
    "        ce_loss = F.cross_entropy(\n",
    "            logits.view(-1, self.vocab_size),  # [B*L, vocab_size]\n",
    "            tokens.view(-1),  # [B*L]\n",
    "            ignore_index=self.padding_idx,\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "\n",
    "        total_loss = mse_loss + self.ce_weight * ce_loss\n",
    "        return total_loss, mse_loss, ce_loss\n",
    "\n",
    "    def forward(self, points, tokens, variables, t, mse=False):\n",
    "        token_emb = self.tok_emb(tokens)\n",
    "        total_loss, mse_loss, ce_loss = self.p_losses(\n",
    "            token_emb, points, tokens, variables, t, mse=mse\n",
    "        )\n",
    "        return total_loss, mse_loss, ce_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c63489dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T04:36:58.889980Z",
     "iopub.status.busy": "2025-04-09T04:36:58.889777Z",
     "iopub.status.idle": "2025-04-09T04:36:58.901754Z",
     "shell.execute_reply": "2025-04-09T04:36:58.901130Z"
    },
    "papermill": {
     "duration": 0.016526,
     "end_time": "2025-04-09T04:36:58.902925",
     "exception": false,
     "start_time": "2025-04-09T04:36:58.886399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import tqdm\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: SymbolicGaussianDiffusion,  \n",
    "    train_loader: DataLoader,\n",
    "    optimizer: Adam,\n",
    "    train_dataset: CharDataset,\n",
    "    timesteps: int,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    num_epochs: int,\n",
    ") -> Tuple[float, float, float]:  \n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_mse_loss = 0\n",
    "    total_ce_loss = 0\n",
    "\n",
    "    for i, (_, tokens, points, variables) in tqdm.tqdm(\n",
    "        enumerate(train_loader),\n",
    "        total=len(train_loader),\n",
    "        desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
    "    ):\n",
    "        points, tokens, variables = (\n",
    "            points.to(device),\n",
    "            tokens.to(device),\n",
    "            variables.to(device),\n",
    "        )\n",
    "        t = torch.randint(0, timesteps, (tokens.shape[0],), device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss, mse_loss, ce_loss = model(points, tokens, variables, t)\n",
    "\n",
    "        if (i + 1) % 250 == 0:\n",
    "            print(f\"Batch {i + 1}/{len(train_loader)}:\")\n",
    "            print(f\"total_loss: {total_loss}, mse: {mse_loss}, ce: {ce_loss}\")\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += total_loss.item()\n",
    "        total_mse_loss += mse_loss.item()\n",
    "        total_ce_loss += ce_loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    avg_mse_loss = total_mse_loss / len(train_loader)\n",
    "    avg_ce_loss = total_ce_loss / len(train_loader)\n",
    "    return avg_train_loss, avg_mse_loss, avg_ce_loss\n",
    "\n",
    "\n",
    "def val_epoch(\n",
    "    model: SymbolicGaussianDiffusion,  \n",
    "    val_loader: DataLoader,\n",
    "    train_dataset: CharDataset,\n",
    "    timesteps: int,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    num_epochs: int,\n",
    ") -> Tuple[float, float, float]:  \n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_mse_loss = 0\n",
    "    total_ce_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, tokens, points, variables in tqdm.tqdm(\n",
    "            val_loader, total=len(val_loader), desc=\"Validating\"\n",
    "        ):\n",
    "            points, tokens, variables = (\n",
    "                points.to(device),\n",
    "                tokens.to(device),\n",
    "                variables.to(device),\n",
    "            )\n",
    "            t = torch.randint(0, timesteps, (tokens.shape[0],), device=device)\n",
    "\n",
    "            total_loss, mse_loss, ce_loss = model(points, tokens, variables, t)\n",
    "\n",
    "            total_val_loss += total_loss.item()\n",
    "            total_mse_loss += mse_loss.item()\n",
    "            total_ce_loss += ce_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    avg_mse_loss = total_mse_loss / len(val_loader)\n",
    "    avg_ce_loss = total_ce_loss / len(val_loader)\n",
    "    return avg_val_loss, avg_mse_loss, avg_ce_loss\n",
    "\n",
    "\n",
    "def train_single_gpu(\n",
    "    model: SymbolicGaussianDiffusion,  \n",
    "    train_dataset: CharDataset,\n",
    "    val_dataset: CharDataset,\n",
    "    num_epochs=10,\n",
    "    save_every=2,\n",
    "    batch_size=32,\n",
    "    timesteps=1000,\n",
    "    learning_rate=1e-3,\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_train_loss, avg_mse_loss, avg_ce_loss = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            train_dataset,\n",
    "            timesteps,\n",
    "            device,\n",
    "            epoch,\n",
    "            num_epochs,\n",
    "        )\n",
    "\n",
    "        avg_val_loss, val_mse_loss, val_ce_loss = val_epoch(\n",
    "            model, val_loader, train_dataset, timesteps, device, epoch, num_epochs\n",
    "        )\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        print(\"\\nEpoch Summary:\")\n",
    "        print(\n",
    "            f\"Train Total Loss: {avg_train_loss:.4f} (MSE: {avg_mse_loss:.4f}, CE: {avg_ce_loss:.4f})\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Val Total Loss: {avg_val_loss:.4f} (MSE: {val_mse_loss:.4f}, CE: {val_ce_loss:.4f})\"\n",
    "        )\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            state_dict = model.state_dict()\n",
    "            torch.save(state_dict, \"best_model.pth\")\n",
    "            print(f\"New best model saved with val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40f82cb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T04:36:58.909108Z",
     "iopub.status.busy": "2025-04-09T04:36:58.908916Z",
     "iopub.status.idle": "2025-04-09T04:36:59.013494Z",
     "shell.execute_reply": "2025-04-09T04:36:59.012754Z"
    },
    "papermill": {
     "duration": 0.109049,
     "end_time": "2025-04-09T04:36:59.014764",
     "exception": false,
     "start_time": "2025-04-09T04:36:58.905715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_embd = 512\n",
    "timesteps = 1000\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 5\n",
    "blockSize = 32\n",
    "numVars = 1\n",
    "numYs = 1\n",
    "numPoints = 250\n",
    "target = 'Skeleton'\n",
    "const_range = [-2.1, 2.1]\n",
    "trainRange = [-3.0, 3.0]\n",
    "decimals = 8\n",
    "addVars = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1082b87e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T04:36:59.022690Z",
     "iopub.status.busy": "2025-04-09T04:36:59.022461Z",
     "iopub.status.idle": "2025-04-09T04:36:59.025458Z",
     "shell.execute_reply": "2025-04-09T04:36:59.024842Z"
    },
    "papermill": {
     "duration": 0.007644,
     "end_time": "2025-04-09T04:36:59.026676",
     "exception": false,
     "start_time": "2025-04-09T04:36:59.019032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = \"/kaggle/input/1-var-dataset/1_var_train.json\"\n",
    "val_path = \"/kaggle/input/1-var-dataset/1_var_val.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "949b9144",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T04:36:59.033270Z",
     "iopub.status.busy": "2025-04-09T04:36:59.033062Z",
     "iopub.status.idle": "2025-04-09T04:37:15.805532Z",
     "shell.execute_reply": "2025-04-09T04:37:15.804525Z"
    },
    "papermill": {
     "duration": 16.777345,
     "end_time": "2025-04-09T04:37:15.806956",
     "exception": false,
     "start_time": "2025-04-09T04:36:59.029611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 498795 examples, 27 unique.\n",
      "id:223357\n",
      "outputs:C*x1**4+C*x1**2+C>__________________\n",
      "variables:1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "\n",
    "files = glob.glob(train_path)\n",
    "text = processDataFiles(files)\n",
    "text = text.split('\\n') # convert the raw text to a set of examples\n",
    "# skeletons = []\n",
    "skeletons = [json.loads(item)['Skeleton'] for item in text if item.strip()]\n",
    "all_tokens = set()\n",
    "for eq in skeletons:\n",
    "    all_tokens.update(tokenize_equation(eq))\n",
    "integers = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9'}\n",
    "all_tokens.update(integers)  # add all integers to the token set\n",
    "tokens = sorted(list(all_tokens) + ['_', 'T', '<', '>', ':'])  # special tokens\n",
    "trainText = text[:-1] if len(text[-1]) == 0 else text\n",
    "random.shuffle(trainText) # shuffle the dataset, it's important specailly for the combined number of variables experiment\n",
    "train_dataset = CharDataset(trainText, blockSize, tokens=tokens, numVars=numVars,\n",
    "                        numYs=numYs, numPoints=numPoints, target=target, addVars=addVars,\n",
    "                        const_range=const_range, xRange=trainRange, decimals=decimals)\n",
    "\n",
    "idx = np.random.randint(train_dataset.__len__())\n",
    "inputs, outputs, points, variables = train_dataset.__getitem__(idx)\n",
    "inputs = ''.join([train_dataset.itos[int(i)] for i in inputs])\n",
    "outputs = ''.join([train_dataset.itos[int(i)] for i in outputs])\n",
    "print('id:{}\\noutputs:{}\\nvariables:{}'.format(idx,outputs,variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cec88e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T04:37:15.814858Z",
     "iopub.status.busy": "2025-04-09T04:37:15.814600Z",
     "iopub.status.idle": "2025-04-09T04:37:15.913109Z",
     "shell.execute_reply": "2025-04-09T04:37:15.912228Z"
    },
    "papermill": {
     "duration": 0.103734,
     "end_time": "2025-04-09T04:37:15.914323",
     "exception": false,
     "start_time": "2025-04-09T04:37:15.810589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 972 examples, 27 unique.\n",
      "tensor(-6.6171) tensor(2.9498)\n",
      "id:891\n",
      "outputs:C*log(C*x1**5+C*x1**4+C*x1**3+C*x1**2+C*x1+\n",
      "variables:1\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(val_path)\n",
    "textVal = processDataFiles([files[0]])\n",
    "textVal = textVal.split('\\n') # convert the raw text to a set of examples\n",
    "val_dataset = CharDataset(textVal, blockSize, tokens=tokens, numVars=numVars,\n",
    "                        numYs=numYs, numPoints=numPoints, target=target, addVars=addVars,\n",
    "                        const_range=const_range, xRange=trainRange, decimals=decimals)\n",
    "\n",
    "# print a random sample\n",
    "idx = np.random.randint(val_dataset.__len__())\n",
    "inputs, outputs, points, variables = val_dataset.__getitem__(idx)\n",
    "print(points.min(), points.max())\n",
    "inputs = ''.join([train_dataset.itos[int(i)] for i in inputs])\n",
    "outputs = ''.join([train_dataset.itos[int(i)] for i in outputs])\n",
    "print('id:{}\\noutputs:{}\\nvariables:{}'.format(idx,outputs,variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b54b4deb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T04:37:15.921801Z",
     "iopub.status.busy": "2025-04-09T04:37:15.921566Z",
     "iopub.status.idle": "2025-04-09T07:23:43.919953Z",
     "shell.execute_reply": "2025-04-09T07:23:43.919058Z"
    },
    "papermill": {
     "duration": 9988.003529,
     "end_time": "2025-04-09T07:23:43.921246",
     "exception": false,
     "start_time": "2025-04-09T04:37:15.917717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   3%|         | 250/7794 [00:52<27:00,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/7794:\n",
      "total_loss: 1.705775499343872, mse: 0.0, ce: 1.705775499343872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   6%|         | 500/7794 [01:49<29:10,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/7794:\n",
      "total_loss: 1.2318158149719238, mse: 0.0, ce: 1.2318158149719238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  10%|         | 750/7794 [02:52<30:29,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 750/7794:\n",
      "total_loss: 0.8529603481292725, mse: 0.0, ce: 0.8529603481292725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  13%|        | 1000/7794 [03:56<29:13,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/7794:\n",
      "total_loss: 0.9952191114425659, mse: 0.0, ce: 0.9952191114425659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  16%|        | 1250/7794 [05:00<27:28,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1250/7794:\n",
      "total_loss: 0.8922011852264404, mse: 0.0, ce: 0.8922011852264404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  19%|        | 1500/7794 [06:03<26:40,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500/7794:\n",
      "total_loss: 0.839065432548523, mse: 0.0, ce: 0.839065432548523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  22%|       | 1750/7794 [07:07<25:41,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1750/7794:\n",
      "total_loss: 0.6991020441055298, mse: 0.0, ce: 0.6991020441055298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  26%|       | 2000/7794 [08:11<24:42,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/7794:\n",
      "total_loss: 0.6518251895904541, mse: 0.0, ce: 0.6518251895904541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  29%|       | 2250/7794 [09:14<23:35,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2250/7794:\n",
      "total_loss: 0.9088393449783325, mse: 0.0, ce: 0.9088393449783325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  32%|      | 2500/7794 [10:18<22:17,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500/7794:\n",
      "total_loss: 0.5906674265861511, mse: 0.0, ce: 0.5906674265861511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  35%|      | 2750/7794 [11:22<21:21,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2750/7794:\n",
      "total_loss: 0.7346487641334534, mse: 0.0, ce: 0.7346487641334534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  38%|      | 3000/7794 [12:25<20:19,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000/7794:\n",
      "total_loss: 0.5404168963432312, mse: 0.0, ce: 0.5404168963432312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  42%|     | 3250/7794 [13:29<19:15,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3250/7794:\n",
      "total_loss: 0.726787805557251, mse: 0.0, ce: 0.726787805557251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  45%|     | 3500/7794 [14:32<18:18,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500/7794:\n",
      "total_loss: 0.8140419721603394, mse: 0.0, ce: 0.8140419721603394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  48%|     | 3750/7794 [15:36<17:09,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3750/7794:\n",
      "total_loss: 0.6897780299186707, mse: 0.0, ce: 0.6897780299186707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  51%|    | 4000/7794 [16:40<16:14,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000/7794:\n",
      "total_loss: 0.5270150303840637, mse: 0.0, ce: 0.5270150303840637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  55%|    | 4250/7794 [17:44<15:04,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4250/7794:\n",
      "total_loss: 0.446198433637619, mse: 0.0, ce: 0.446198433637619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  58%|    | 4500/7794 [18:48<13:59,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4500/7794:\n",
      "total_loss: 0.4764685034751892, mse: 0.0, ce: 0.4764685034751892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  61%|    | 4750/7794 [19:51<12:55,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4750/7794:\n",
      "total_loss: 0.4865122139453888, mse: 0.0, ce: 0.4865122139453888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  64%|   | 5000/7794 [20:55<11:54,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000/7794:\n",
      "total_loss: 0.4880017340183258, mse: 0.0, ce: 0.4880017340183258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  67%|   | 5250/7794 [21:59<10:49,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5250/7794:\n",
      "total_loss: 0.4828474819660187, mse: 0.0, ce: 0.4828474819660187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  71%|   | 5500/7794 [23:03<09:46,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/7794:\n",
      "total_loss: 0.5442200303077698, mse: 0.0, ce: 0.5442200303077698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  74%|  | 5750/7794 [24:07<08:42,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5750/7794:\n",
      "total_loss: 0.6045358777046204, mse: 0.0, ce: 0.6045358777046204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  77%|  | 6000/7794 [25:11<07:38,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000/7794:\n",
      "total_loss: 0.5384818911552429, mse: 0.0, ce: 0.5384818911552429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  80%|  | 6250/7794 [26:14<06:33,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6250/7794:\n",
      "total_loss: 0.291995108127594, mse: 0.0, ce: 0.291995108127594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  83%| | 6500/7794 [27:18<05:30,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6500/7794:\n",
      "total_loss: 0.4761177897453308, mse: 0.0, ce: 0.4761177897453308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  87%| | 6750/7794 [28:22<04:26,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6750/7794:\n",
      "total_loss: 0.710332989692688, mse: 0.0, ce: 0.710332989692688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  90%| | 7000/7794 [29:26<03:23,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000/7794:\n",
      "total_loss: 0.313102662563324, mse: 0.0, ce: 0.313102662563324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  93%|| 7250/7794 [30:30<02:19,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7250/7794:\n",
      "total_loss: 0.36666813492774963, mse: 0.0, ce: 0.36666813492774963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  96%|| 7500/7794 [31:34<01:15,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500/7794:\n",
      "total_loss: 0.5181324481964111, mse: 0.0, ce: 0.5181324481964111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  99%|| 7750/7794 [32:38<00:11,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7750/7794:\n",
      "total_loss: 0.4355525076389313, mse: 0.0, ce: 0.4355525076389313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|| 7794/7794 [32:49<00:00,  3.96it/s]\n",
      "Validating: 100%|| 16/16 [00:01<00:00,  9.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Total Loss: 0.7211 (MSE: 0.0000, CE: 0.7211)\n",
      "Val Total Loss: 0.4016 (MSE: 0.0000, CE: 0.4016)\n",
      "Learning Rate: 0.000100\n",
      "New best model saved with val loss: 0.4016\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/5:   3%|         | 250/7794 [01:03<32:10,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/7794:\n",
      "total_loss: 0.43980464339256287, mse: 0.0, ce: 0.43980464339256287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:   6%|         | 500/7794 [02:07<31:04,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/7794:\n",
      "total_loss: 0.7894595861434937, mse: 0.0, ce: 0.7894595861434937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  10%|         | 750/7794 [03:11<30:00,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 750/7794:\n",
      "total_loss: 0.5834562182426453, mse: 0.0, ce: 0.5834562182426453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  13%|        | 1000/7794 [04:15<28:54,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/7794:\n",
      "total_loss: 0.4501655101776123, mse: 0.0, ce: 0.4501655101776123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  16%|        | 1250/7794 [05:19<27:52,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1250/7794:\n",
      "total_loss: 0.36186593770980835, mse: 0.0, ce: 0.36186593770980835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  19%|        | 1500/7794 [06:23<26:50,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500/7794:\n",
      "total_loss: 0.6298288106918335, mse: 0.0, ce: 0.6298288106918335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  22%|       | 1750/7794 [07:27<25:39,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1750/7794:\n",
      "total_loss: 0.37163764238357544, mse: 0.0, ce: 0.37163764238357544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  26%|       | 2000/7794 [08:31<24:36,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/7794:\n",
      "total_loss: 0.3641672432422638, mse: 0.0, ce: 0.3641672432422638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  29%|       | 2250/7794 [09:35<23:37,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2250/7794:\n",
      "total_loss: 0.38722437620162964, mse: 0.0, ce: 0.38722437620162964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  32%|      | 2500/7794 [10:39<22:29,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500/7794:\n",
      "total_loss: 0.43785709142684937, mse: 0.0, ce: 0.43785709142684937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  35%|      | 2750/7794 [11:42<21:29,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2750/7794:\n",
      "total_loss: 0.36612752079963684, mse: 0.0, ce: 0.36612752079963684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  38%|      | 3000/7794 [12:46<20:15,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000/7794:\n",
      "total_loss: 0.5785322785377502, mse: 0.0, ce: 0.5785322785377502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  42%|     | 3250/7794 [13:50<19:18,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3250/7794:\n",
      "total_loss: 0.37187737226486206, mse: 0.0, ce: 0.37187737226486206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  45%|     | 3500/7794 [14:54<18:09,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500/7794:\n",
      "total_loss: 0.37335923314094543, mse: 0.0, ce: 0.37335923314094543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  48%|     | 3750/7794 [15:57<17:08,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3750/7794:\n",
      "total_loss: 0.29696759581565857, mse: 0.0, ce: 0.29696759581565857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  51%|    | 4000/7794 [17:01<16:14,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000/7794:\n",
      "total_loss: 0.25822460651397705, mse: 0.0, ce: 0.25822460651397705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  55%|    | 4250/7794 [18:05<15:06,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4250/7794:\n",
      "total_loss: 0.36884817481040955, mse: 0.0, ce: 0.36884817481040955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  58%|    | 4500/7794 [19:09<14:08,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4500/7794:\n",
      "total_loss: 0.47850140929222107, mse: 0.0, ce: 0.47850140929222107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  61%|    | 4750/7794 [20:14<13:01,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4750/7794:\n",
      "total_loss: 0.4122616648674011, mse: 0.0, ce: 0.4122616648674011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  64%|   | 5000/7794 [21:18<11:59,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000/7794:\n",
      "total_loss: 0.378512978553772, mse: 0.0, ce: 0.378512978553772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  67%|   | 5250/7794 [22:22<10:52,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5250/7794:\n",
      "total_loss: 0.31652045249938965, mse: 0.0, ce: 0.31652045249938965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  71%|   | 5500/7794 [23:27<09:51,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/7794:\n",
      "total_loss: 0.5405281782150269, mse: 0.0, ce: 0.5405281782150269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  74%|  | 5750/7794 [24:31<08:41,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5750/7794:\n",
      "total_loss: 0.30956006050109863, mse: 0.0, ce: 0.30956006050109863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  77%|  | 6000/7794 [25:35<07:36,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000/7794:\n",
      "total_loss: 0.37693941593170166, mse: 0.0, ce: 0.37693941593170166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  80%|  | 6250/7794 [26:39<06:33,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6250/7794:\n",
      "total_loss: 0.36785319447517395, mse: 0.0, ce: 0.36785319447517395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  83%| | 6500/7794 [27:43<05:32,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6500/7794:\n",
      "total_loss: 0.45760124921798706, mse: 0.0, ce: 0.45760124921798706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  87%| | 6750/7794 [28:47<04:28,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6750/7794:\n",
      "total_loss: 0.3983871638774872, mse: 0.0, ce: 0.3983871638774872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  90%| | 7000/7794 [29:51<03:24,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000/7794:\n",
      "total_loss: 0.3658277094364166, mse: 0.0, ce: 0.3658277094364166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  93%|| 7250/7794 [30:56<02:20,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7250/7794:\n",
      "total_loss: 0.23877228796482086, mse: 0.0, ce: 0.23877228796482086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  96%|| 7500/7794 [32:00<01:15,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500/7794:\n",
      "total_loss: 0.5564554929733276, mse: 0.0, ce: 0.5564554929733276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  99%|| 7750/7794 [33:04<00:11,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7750/7794:\n",
      "total_loss: 0.3265575170516968, mse: 0.0, ce: 0.3265575170516968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|| 7794/7794 [33:16<00:00,  3.90it/s]\n",
      "Validating: 100%|| 16/16 [00:01<00:00, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Total Loss: 0.4425 (MSE: 0.0000, CE: 0.4425)\n",
      "Val Total Loss: 0.3747 (MSE: 0.0000, CE: 0.3747)\n",
      "Learning Rate: 0.000100\n",
      "New best model saved with val loss: 0.3747\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:   3%|         | 250/7794 [01:04<32:22,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/7794:\n",
      "total_loss: 0.45196881890296936, mse: 0.0, ce: 0.45196881890296936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:   6%|         | 500/7794 [02:08<31:24,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/7794:\n",
      "total_loss: 0.35803452134132385, mse: 0.0, ce: 0.35803452134132385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  10%|         | 750/7794 [03:13<30:10,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 750/7794:\n",
      "total_loss: 0.4720551669597626, mse: 0.0, ce: 0.4720551669597626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  13%|        | 1000/7794 [04:17<28:55,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/7794:\n",
      "total_loss: 0.510735809803009, mse: 0.0, ce: 0.510735809803009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  16%|        | 1250/7794 [05:21<27:48,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1250/7794:\n",
      "total_loss: 0.3385296165943146, mse: 0.0, ce: 0.3385296165943146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  19%|        | 1500/7794 [06:25<26:52,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500/7794:\n",
      "total_loss: 0.4422198235988617, mse: 0.0, ce: 0.4422198235988617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  22%|       | 1750/7794 [07:29<25:43,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1750/7794:\n",
      "total_loss: 0.2635929584503174, mse: 0.0, ce: 0.2635929584503174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  26%|       | 2000/7794 [08:33<24:39,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/7794:\n",
      "total_loss: 0.3326506018638611, mse: 0.0, ce: 0.3326506018638611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  29%|       | 2250/7794 [09:36<23:37,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2250/7794:\n",
      "total_loss: 0.5785184502601624, mse: 0.0, ce: 0.5785184502601624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  32%|      | 2500/7794 [10:40<22:24,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500/7794:\n",
      "total_loss: 0.19709990918636322, mse: 0.0, ce: 0.19709990918636322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  35%|      | 2750/7794 [11:44<21:28,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2750/7794:\n",
      "total_loss: 0.453517347574234, mse: 0.0, ce: 0.453517347574234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  38%|      | 3000/7794 [12:48<20:35,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000/7794:\n",
      "total_loss: 0.442423939704895, mse: 0.0, ce: 0.442423939704895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  42%|     | 3250/7794 [13:52<19:28,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3250/7794:\n",
      "total_loss: 0.30101287364959717, mse: 0.0, ce: 0.30101287364959717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  45%|     | 3500/7794 [14:57<18:21,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500/7794:\n",
      "total_loss: 0.3488680422306061, mse: 0.0, ce: 0.3488680422306061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  48%|     | 3750/7794 [16:00<17:06,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3750/7794:\n",
      "total_loss: 0.4129810631275177, mse: 0.0, ce: 0.4129810631275177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  51%|    | 4000/7794 [17:04<16:13,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000/7794:\n",
      "total_loss: 0.34902259707450867, mse: 0.0, ce: 0.34902259707450867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  55%|    | 4250/7794 [18:08<15:05,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4250/7794:\n",
      "total_loss: 0.33697834610939026, mse: 0.0, ce: 0.33697834610939026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  58%|    | 4500/7794 [19:12<14:05,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4500/7794:\n",
      "total_loss: 0.3393348157405853, mse: 0.0, ce: 0.3393348157405853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  61%|    | 4750/7794 [20:17<13:02,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4750/7794:\n",
      "total_loss: 0.24855290353298187, mse: 0.0, ce: 0.24855290353298187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  64%|   | 5000/7794 [21:21<11:59,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000/7794:\n",
      "total_loss: 0.33552852272987366, mse: 0.0, ce: 0.33552852272987366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  67%|   | 5250/7794 [22:25<10:55,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5250/7794:\n",
      "total_loss: 0.53309565782547, mse: 0.0, ce: 0.53309565782547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  71%|   | 5500/7794 [23:30<09:51,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/7794:\n",
      "total_loss: 0.3183742165565491, mse: 0.0, ce: 0.3183742165565491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  74%|  | 5750/7794 [24:34<08:46,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5750/7794:\n",
      "total_loss: 0.15803854167461395, mse: 0.0, ce: 0.15803854167461395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  77%|  | 6000/7794 [25:38<07:39,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000/7794:\n",
      "total_loss: 0.46108025312423706, mse: 0.0, ce: 0.46108025312423706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  80%|  | 6250/7794 [26:43<06:37,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6250/7794:\n",
      "total_loss: 0.25653719902038574, mse: 0.0, ce: 0.25653719902038574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  83%| | 6500/7794 [27:47<05:29,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6500/7794:\n",
      "total_loss: 0.3451078236103058, mse: 0.0, ce: 0.3451078236103058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  87%| | 6750/7794 [28:51<04:26,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6750/7794:\n",
      "total_loss: 0.5225911140441895, mse: 0.0, ce: 0.5225911140441895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  90%| | 7000/7794 [29:55<03:22,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000/7794:\n",
      "total_loss: 0.2663317918777466, mse: 0.0, ce: 0.2663317918777466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  93%|| 7250/7794 [30:59<02:19,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7250/7794:\n",
      "total_loss: 0.5848854184150696, mse: 0.0, ce: 0.5848854184150696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  96%|| 7500/7794 [32:03<01:15,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500/7794:\n",
      "total_loss: 0.2786600887775421, mse: 0.0, ce: 0.2786600887775421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  99%|| 7750/7794 [33:07<00:11,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7750/7794:\n",
      "total_loss: 0.36143094301223755, mse: 0.0, ce: 0.36143094301223755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|| 7794/7794 [33:18<00:00,  3.90it/s]\n",
      "Validating: 100%|| 16/16 [00:01<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Total Loss: 0.3633 (MSE: 0.0000, CE: 0.3633)\n",
      "Val Total Loss: 0.3080 (MSE: 0.0000, CE: 0.3080)\n",
      "Learning Rate: 0.000100\n",
      "New best model saved with val loss: 0.3080\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:   3%|         | 250/7794 [01:03<32:05,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/7794:\n",
      "total_loss: 0.4851796627044678, mse: 0.0, ce: 0.4851796627044678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:   6%|         | 500/7794 [02:08<31:11,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/7794:\n",
      "total_loss: 0.2915257513523102, mse: 0.0, ce: 0.2915257513523102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  10%|         | 750/7794 [03:12<30:28,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 750/7794:\n",
      "total_loss: 0.15448017418384552, mse: 0.0, ce: 0.15448017418384552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  13%|        | 1000/7794 [04:17<29:14,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/7794:\n",
      "total_loss: 0.4060796797275543, mse: 0.0, ce: 0.4060796797275543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  16%|        | 1250/7794 [05:21<27:57,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1250/7794:\n",
      "total_loss: 0.1997637003660202, mse: 0.0, ce: 0.1997637003660202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  19%|        | 1500/7794 [06:25<26:50,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500/7794:\n",
      "total_loss: 0.21544525027275085, mse: 0.0, ce: 0.21544525027275085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  22%|       | 1750/7794 [07:29<25:54,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1750/7794:\n",
      "total_loss: 0.3060319423675537, mse: 0.0, ce: 0.3060319423675537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  26%|       | 2000/7794 [08:34<24:48,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/7794:\n",
      "total_loss: 0.35989105701446533, mse: 0.0, ce: 0.35989105701446533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  29%|       | 2250/7794 [09:38<23:46,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2250/7794:\n",
      "total_loss: 0.35236719250679016, mse: 0.0, ce: 0.35236719250679016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  32%|      | 2500/7794 [10:42<22:42,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500/7794:\n",
      "total_loss: 0.3662908971309662, mse: 0.0, ce: 0.3662908971309662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  35%|      | 2750/7794 [11:46<21:32,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2750/7794:\n",
      "total_loss: 0.2375355213880539, mse: 0.0, ce: 0.2375355213880539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  38%|      | 3000/7794 [12:50<20:32,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000/7794:\n",
      "total_loss: 0.29493486881256104, mse: 0.0, ce: 0.29493486881256104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  42%|     | 3250/7794 [13:55<19:29,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3250/7794:\n",
      "total_loss: 0.39867860078811646, mse: 0.0, ce: 0.39867860078811646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  45%|     | 3500/7794 [14:59<18:25,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500/7794:\n",
      "total_loss: 0.2554866075515747, mse: 0.0, ce: 0.2554866075515747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  48%|     | 3750/7794 [16:03<17:26,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3750/7794:\n",
      "total_loss: 0.3682539761066437, mse: 0.0, ce: 0.3682539761066437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  51%|    | 4000/7794 [17:08<16:18,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000/7794:\n",
      "total_loss: 0.42514127492904663, mse: 0.0, ce: 0.42514127492904663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  55%|    | 4250/7794 [18:12<15:11,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4250/7794:\n",
      "total_loss: 0.35223421454429626, mse: 0.0, ce: 0.35223421454429626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  58%|    | 4500/7794 [19:16<14:09,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4500/7794:\n",
      "total_loss: 0.2924885153770447, mse: 0.0, ce: 0.2924885153770447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  61%|    | 4750/7794 [20:20<12:59,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4750/7794:\n",
      "total_loss: 0.2548619210720062, mse: 0.0, ce: 0.2548619210720062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  64%|   | 5000/7794 [21:25<11:58,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000/7794:\n",
      "total_loss: 0.38076674938201904, mse: 0.0, ce: 0.38076674938201904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  67%|   | 5250/7794 [22:29<10:55,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5250/7794:\n",
      "total_loss: 0.2957996726036072, mse: 0.0, ce: 0.2957996726036072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  71%|   | 5500/7794 [23:33<09:47,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/7794:\n",
      "total_loss: 0.26836293935775757, mse: 0.0, ce: 0.26836293935775757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  74%|  | 5750/7794 [24:38<08:46,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5750/7794:\n",
      "total_loss: 0.24475722014904022, mse: 0.0, ce: 0.24475722014904022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  77%|  | 6000/7794 [25:42<07:42,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000/7794:\n",
      "total_loss: 0.18955957889556885, mse: 0.0, ce: 0.18955957889556885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  80%|  | 6250/7794 [26:46<06:36,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6250/7794:\n",
      "total_loss: 0.21251842379570007, mse: 0.0, ce: 0.21251842379570007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  83%| | 6500/7794 [27:51<05:33,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6500/7794:\n",
      "total_loss: 0.36646389961242676, mse: 0.0, ce: 0.36646389961242676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  87%| | 6750/7794 [28:55<04:29,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6750/7794:\n",
      "total_loss: 0.2475842982530594, mse: 0.0, ce: 0.2475842982530594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  90%| | 7000/7794 [29:59<03:23,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000/7794:\n",
      "total_loss: 0.2950681149959564, mse: 0.0, ce: 0.2950681149959564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  93%|| 7250/7794 [31:03<02:19,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7250/7794:\n",
      "total_loss: 0.3159819543361664, mse: 0.0, ce: 0.3159819543361664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  96%|| 7500/7794 [32:08<01:15,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500/7794:\n",
      "total_loss: 0.09885882586240768, mse: 0.0, ce: 0.09885882586240768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  99%|| 7750/7794 [33:12<00:11,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7750/7794:\n",
      "total_loss: 0.3156377971172333, mse: 0.0, ce: 0.3156377971172333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|| 7794/7794 [33:23<00:00,  3.89it/s]\n",
      "Validating: 100%|| 16/16 [00:01<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Total Loss: 0.3154 (MSE: 0.0000, CE: 0.3154)\n",
      "Val Total Loss: 0.2489 (MSE: 0.0000, CE: 0.2489)\n",
      "Learning Rate: 0.000100\n",
      "New best model saved with val loss: 0.2489\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:   3%|         | 250/7794 [01:04<32:31,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/7794:\n",
      "total_loss: 0.30015265941619873, mse: 0.0, ce: 0.30015265941619873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:   6%|         | 500/7794 [02:09<31:18,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/7794:\n",
      "total_loss: 0.39262446761131287, mse: 0.0, ce: 0.39262446761131287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  10%|         | 750/7794 [03:13<30:10,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 750/7794:\n",
      "total_loss: 0.3494066298007965, mse: 0.0, ce: 0.3494066298007965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  13%|        | 1000/7794 [04:17<29:08,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/7794:\n",
      "total_loss: 0.3772345781326294, mse: 0.0, ce: 0.3772345781326294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  16%|        | 1250/7794 [05:21<27:55,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1250/7794:\n",
      "total_loss: 0.30918365716934204, mse: 0.0, ce: 0.30918365716934204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  19%|        | 1500/7794 [06:25<26:53,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500/7794:\n",
      "total_loss: 0.3344707787036896, mse: 0.0, ce: 0.3344707787036896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  22%|       | 1750/7794 [07:29<25:53,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1750/7794:\n",
      "total_loss: 0.16226226091384888, mse: 0.0, ce: 0.16226226091384888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  26%|       | 2000/7794 [08:34<24:48,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/7794:\n",
      "total_loss: 0.27773037552833557, mse: 0.0, ce: 0.27773037552833557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  29%|       | 2250/7794 [09:38<23:40,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2250/7794:\n",
      "total_loss: 0.31067174673080444, mse: 0.0, ce: 0.31067174673080444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  32%|      | 2500/7794 [10:42<22:34,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500/7794:\n",
      "total_loss: 0.3164146840572357, mse: 0.0, ce: 0.3164146840572357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  35%|      | 2750/7794 [11:46<21:38,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2750/7794:\n",
      "total_loss: 0.3762204349040985, mse: 0.0, ce: 0.3762204349040985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  38%|      | 3000/7794 [12:50<20:35,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000/7794:\n",
      "total_loss: 0.23477225005626678, mse: 0.0, ce: 0.23477225005626678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  42%|     | 3250/7794 [13:55<19:34,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3250/7794:\n",
      "total_loss: 0.35300594568252563, mse: 0.0, ce: 0.35300594568252563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  45%|     | 3500/7794 [14:59<18:30,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500/7794:\n",
      "total_loss: 0.2122686356306076, mse: 0.0, ce: 0.2122686356306076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  48%|     | 3750/7794 [16:04<17:22,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3750/7794:\n",
      "total_loss: 0.3392455279827118, mse: 0.0, ce: 0.3392455279827118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  51%|    | 4000/7794 [17:08<16:18,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000/7794:\n",
      "total_loss: 0.32157379388809204, mse: 0.0, ce: 0.32157379388809204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  55%|    | 4250/7794 [18:13<15:17,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4250/7794:\n",
      "total_loss: 0.30382904410362244, mse: 0.0, ce: 0.30382904410362244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  58%|    | 4500/7794 [19:17<14:11,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4500/7794:\n",
      "total_loss: 0.1776878982782364, mse: 0.0, ce: 0.1776878982782364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  61%|    | 4750/7794 [20:22<13:10,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4750/7794:\n",
      "total_loss: 0.2955493927001953, mse: 0.0, ce: 0.2955493927001953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  64%|   | 5000/7794 [21:27<11:56,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000/7794:\n",
      "total_loss: 0.3920242488384247, mse: 0.0, ce: 0.3920242488384247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  67%|   | 5250/7794 [22:31<10:54,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5250/7794:\n",
      "total_loss: 0.1642235666513443, mse: 0.0, ce: 0.1642235666513443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  71%|   | 5500/7794 [23:35<09:50,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/7794:\n",
      "total_loss: 0.2859693765640259, mse: 0.0, ce: 0.2859693765640259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  74%|  | 5750/7794 [24:39<08:44,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5750/7794:\n",
      "total_loss: 0.3676275610923767, mse: 0.0, ce: 0.3676275610923767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  77%|  | 6000/7794 [25:44<07:40,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000/7794:\n",
      "total_loss: 0.22182682156562805, mse: 0.0, ce: 0.22182682156562805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  80%|  | 6250/7794 [26:48<06:35,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6250/7794:\n",
      "total_loss: 0.2844837009906769, mse: 0.0, ce: 0.2844837009906769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  83%| | 6500/7794 [27:52<05:31,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6500/7794:\n",
      "total_loss: 0.17703086137771606, mse: 0.0, ce: 0.17703086137771606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  87%| | 6750/7794 [28:57<04:30,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6750/7794:\n",
      "total_loss: 0.3429790139198303, mse: 0.0, ce: 0.3429790139198303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  90%| | 7000/7794 [30:01<03:24,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000/7794:\n",
      "total_loss: 0.16299396753311157, mse: 0.0, ce: 0.16299396753311157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  93%|| 7250/7794 [31:05<02:20,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7250/7794:\n",
      "total_loss: 0.1932993084192276, mse: 0.0, ce: 0.1932993084192276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  96%|| 7500/7794 [32:10<01:15,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500/7794:\n",
      "total_loss: 0.23865948617458344, mse: 0.0, ce: 0.23865948617458344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  99%|| 7750/7794 [33:14<00:11,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7750/7794:\n",
      "total_loss: 0.36300820112228394, mse: 0.0, ce: 0.36300820112228394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|| 7794/7794 [33:26<00:00,  3.88it/s]\n",
      "Validating: 100%|| 16/16 [00:01<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Total Loss: 0.2798 (MSE: 0.0000, CE: 0.2798)\n",
      "Val Total Loss: 0.2139 (MSE: 0.0000, CE: 0.2139)\n",
      "Learning Rate: 0.000100\n",
      "New best model saved with val loss: 0.2139\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pconfig = PointNetConfig(\n",
    "    embeddingSize=n_embd,\n",
    "    numberofPoints=numPoints,\n",
    "    numberofVars=numVars,\n",
    "    numberofYs=numYs,\n",
    ")\n",
    "\n",
    "model = SymbolicGaussianDiffusion(\n",
    "    tnet_config=pconfig,  \n",
    "    vocab_size=train_dataset.vocab_size,\n",
    "    max_seq_len=blockSize,\n",
    "    padding_idx=train_dataset.paddingID,\n",
    "    max_num_vars=9,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    n_embd=n_embd,\n",
    "    timesteps=timesteps,\n",
    "    beta_start=0.0001,\n",
    "    beta_end=0.02,\n",
    "    ce_weight=1.0  \n",
    ")\n",
    "\n",
    "train_single_gpu(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    num_epochs=num_epochs,\n",
    "    save_every=2,\n",
    "    batch_size=batch_size,\n",
    "    timesteps=timesteps,\n",
    "    learning_rate=learning_rate\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6977314,
     "sourceId": 11178716,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7089758,
     "sourceId": 11333846,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 282263,
     "modelInstanceId": 261112,
     "sourceId": 306062,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 290784,
     "modelInstanceId": 269794,
     "sourceId": 319741,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 290853,
     "modelInstanceId": 269860,
     "sourceId": 319828,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10018.029584,
   "end_time": "2025-04-09T07:23:48.813413",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-09T04:36:50.783829",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
