{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c42c30",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-09T02:03:13.131948Z",
     "iopub.status.busy": "2025-04-09T02:03:13.131696Z",
     "iopub.status.idle": "2025-04-09T02:03:13.978474Z",
     "shell.execute_reply": "2025-04-09T02:03:13.977420Z"
    },
    "papermill": {
     "duration": 0.852273,
     "end_time": "2025-04-09T02:03:13.979828",
     "exception": false,
     "start_time": "2025-04-09T02:03:13.127555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/1-var-dataset/1_var_test.json\n",
      "/kaggle/input/1-var-dataset/1_var_val.json\n",
      "/kaggle/input/1-var-dataset/1_var_train.json\n",
      "/kaggle/input/xye_1var/pytorch/default/1/XYE_1Var.pt\n",
      "/kaggle/input/symbolic_diffusion_initial/pytorch/default/1/symbolic_diffusion_model.pth\n",
      "/kaggle/input/xye_9var/pytorch/default/1/XYE_9Var.pt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e084135",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T02:03:13.987622Z",
     "iopub.status.busy": "2025-04-09T02:03:13.987237Z",
     "iopub.status.idle": "2025-04-09T02:03:18.483793Z",
     "shell.execute_reply": "2025-04-09T02:03:18.483099Z"
    },
    "papermill": {
     "duration": 4.502093,
     "end_time": "2025-04-09T02:03:18.485394",
     "exception": false,
     "start_time": "2025-04-09T02:03:13.983301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.optimize import minimize\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def generateDataStrEq(\n",
    "    eq, n_points=2, n_vars=3, decimals=4, supportPoints=None, min_x=0, max_x=3\n",
    "):\n",
    "    X = []\n",
    "    Y = []\n",
    "    # TODO: Need to make this faster\n",
    "    for p in range(n_points):\n",
    "        if supportPoints is None:\n",
    "            if type(min_x) == list:\n",
    "                x = []\n",
    "                for _ in range(n_vars):\n",
    "                    idx = np.random.randint(len(min_x))\n",
    "                    x += list(\n",
    "                        np.round(np.random.uniform(min_x[idx], max_x[idx], 1), decimals)\n",
    "                    )\n",
    "            else:\n",
    "                x = list(np.round(np.random.uniform(min_x, max_x, n_vars), decimals))\n",
    "            assert (\n",
    "                len(x) != 0\n",
    "            ), \"For some reason, we didn't generate the points correctly!\"\n",
    "        else:\n",
    "            x = supportPoints[p]\n",
    "\n",
    "        tmpEq = eq + \"\"\n",
    "        for nVID in range(n_vars):\n",
    "            tmpEq = tmpEq.replace(\"x{}\".format(nVID + 1), str(x[nVID]))\n",
    "        y = float(np.round(eval(tmpEq), decimals))\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# def processDataFiles(files):\n",
    "#     text = \"\"\n",
    "#     for f in tqdm(files):\n",
    "#         with open(f, 'r') as h:\n",
    "#             lines = h.read() # don't worry we won't run out of file handles\n",
    "#             if lines[-1]==-1:\n",
    "#                 lines = lines[:-1]\n",
    "#             #text += lines #json.loads(line)\n",
    "#             text = ''.join([lines,text])\n",
    "#     return text\n",
    "\n",
    "\n",
    "def processDataFiles(files):\n",
    "    text = \"\"\n",
    "    for f in files:\n",
    "        with open(f, \"r\") as h:\n",
    "            lines = h.read()  # don't worry we won't run out of file handles\n",
    "            if lines[-1] == -1:\n",
    "                lines = lines[:-1]\n",
    "            # text += lines #json.loads(line)\n",
    "            text = \"\".join([lines, text])\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize_equation(eq):\n",
    "    token_spec = [\n",
    "        (r\"\\*\\*\"),  # exponentiation\n",
    "        (r\"exp\"),  # exp function\n",
    "        (r\"[+\\-*/=()]\"),  # operators and parentheses\n",
    "        (r\"sin\"),  # sin function\n",
    "        (r\"cos\"),  # cos function\n",
    "        (r\"log\"),  # log function\n",
    "        (r\"x\\d+\"),  # variables like x1, x23, etc.\n",
    "        (r\"C\"),  # constants placeholder\n",
    "        (r\"-?\\d+\\.\\d+\"),  # decimal numbers\n",
    "        (r\"-?\\d+\"),  # integers\n",
    "        (r\"_\"),  # padding token\n",
    "    ]\n",
    "    token_regex = \"|\".join(f\"({pattern})\" for pattern in token_spec)\n",
    "    matches = re.finditer(token_regex, eq)\n",
    "    return [match.group(0) for match in matches]\n",
    "\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        block_size,\n",
    "        tokens,\n",
    "        numVars,\n",
    "        numYs,\n",
    "        numPoints,\n",
    "        target=\"Skeleton\",\n",
    "        addVars=False,\n",
    "        const_range=[-0.4, 0.4],\n",
    "        xRange=[-3.0, 3.0],\n",
    "        decimals=4,\n",
    "        augment=False,\n",
    "    ):\n",
    "\n",
    "        data_size, vocab_size = len(data), len(tokens)\n",
    "        print(\"data has %d examples, %d unique.\" % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = {tok: i for i, tok in enumerate(tokens)}\n",
    "        self.itos = {i: tok for i, tok in enumerate(tokens)}\n",
    "\n",
    "        self.numVars = numVars\n",
    "        self.numYs = numYs\n",
    "        self.numPoints = numPoints\n",
    "\n",
    "        # padding token\n",
    "        self.paddingToken = \"_\"\n",
    "        self.paddingID = self.stoi[\"_\"]  # or another ID not already used\n",
    "        self.stoi[self.paddingToken] = self.paddingID\n",
    "        self.itos[self.paddingID] = self.paddingToken\n",
    "\n",
    "        self.threshold = [-1000, 1000]\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data  # it should be a list of examples\n",
    "        self.target = target\n",
    "        self.addVars = addVars\n",
    "\n",
    "        self.const_range = const_range\n",
    "        self.xRange = xRange\n",
    "        self.decimals = decimals\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab an example from the data\n",
    "        chunk = self.data[idx]  # sequence of tokens including x, y, eq, etc.\n",
    "\n",
    "        try:\n",
    "            chunk = json.loads(chunk)  # convert the sequence tokens to a dictionary\n",
    "        except Exception as e:\n",
    "            print(\"Couldn't convert to json: {} \\n error is: {}\".format(chunk, e))\n",
    "            # try the previous example\n",
    "            idx = idx - 1\n",
    "            idx = idx if idx >= 0 else 0\n",
    "            chunk = self.data[idx]\n",
    "            chunk = json.loads(chunk)  # convert the sequence tokens to a dictionary\n",
    "\n",
    "        # find the number of variables in the equation\n",
    "        printInfoCondition = random.random() < 0.0000001\n",
    "        eq = chunk[self.target]\n",
    "        if printInfoCondition:\n",
    "            print(f\"\\nEquation: {eq}\")\n",
    "        vars = re.finditer(\"x[\\d]+\", eq)\n",
    "        numVars = 0\n",
    "        for v in vars:\n",
    "            v = v.group(0).strip(\"x\")\n",
    "            v = eval(v)\n",
    "            v = int(v)\n",
    "            if v > numVars:\n",
    "                numVars = v\n",
    "\n",
    "        if self.target == \"Skeleton\" and self.augment:\n",
    "            threshold = 5000\n",
    "            # randomly generate the constants\n",
    "            cleanEqn = \"\"\n",
    "            for chr in eq:\n",
    "                if chr == \"C\":\n",
    "                    # genereate a new random number\n",
    "                    chr = \"{}\".format(\n",
    "                        np.random.uniform(self.const_range[0], self.const_range[1])\n",
    "                    )\n",
    "                cleanEqn += chr\n",
    "\n",
    "            # update the points\n",
    "            nPoints = np.random.randint(\n",
    "                *self.numPoints\n",
    "            )  # if supportPoints is None else len(supportPoints)\n",
    "            try:\n",
    "                if printInfoCondition:\n",
    "                    print(\"Org:\", chunk[\"X\"], chunk[\"Y\"])\n",
    "\n",
    "                X, y = generateDataStrEq(\n",
    "                    cleanEqn,\n",
    "                    n_points=nPoints,\n",
    "                    n_vars=self.numVars,\n",
    "                    decimals=self.decimals,\n",
    "                    min_x=self.xRange[0],\n",
    "                    max_x=self.xRange[1],\n",
    "                )\n",
    "\n",
    "                # replace out of threshold with maximum numbers\n",
    "                y = [e if abs(e) < threshold else np.sign(e) * threshold for e in y]\n",
    "\n",
    "                # check if there is nan/inf/very large numbers in the y\n",
    "                conditions = (\n",
    "                    (np.isnan(y).any() or np.isinf(y).any())\n",
    "                    or len(y) == 0\n",
    "                    or (abs(min(y)) > threshold or abs(max(y)) > threshold)\n",
    "                )\n",
    "                if not conditions:\n",
    "                    chunk[\"X\"], chunk[\"Y\"] = X, y\n",
    "\n",
    "                if printInfoCondition:\n",
    "                    print(\"Evd:\", chunk[\"X\"], chunk[\"Y\"])\n",
    "            except Exception as e:\n",
    "                # for different reason this might happend including but not limited to division by zero\n",
    "                print(\n",
    "                    \"\".join(\n",
    "                        [\n",
    "                            f\"We just used the original equation and support points because of {e}. \",\n",
    "                            f\"The equation is {eq}, and we update the equation to {cleanEqn}\",\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # encode every character in the equation to an integer\n",
    "        # < is SOS, > is EOS\n",
    "        if self.addVars:\n",
    "            dix = [self.stoi[s] for s in \"<\" + str(numVars) + \":\" + eq + \">\"]\n",
    "        else:\n",
    "            eq_tokens = tokenize_equation(eq)\n",
    "            if self.addVars:\n",
    "                token_seq = [\"<\", str(numVars), \":\", *eq_tokens, \">\"]\n",
    "            else:\n",
    "                token_seq = [\"<\", *eq_tokens, \">\"]\n",
    "            dix = [self.stoi[tok] for tok in token_seq]\n",
    "\n",
    "        inputs = dix[:-1]\n",
    "        outputs = dix[1:]\n",
    "\n",
    "        # add the padding to the equations\n",
    "        paddingSize = max(self.block_size - len(inputs), 0)\n",
    "        paddingList = [self.paddingID] * paddingSize\n",
    "        inputs += paddingList\n",
    "        outputs += paddingList\n",
    "\n",
    "        # make sure it is not more than what should be\n",
    "        inputs = inputs[: self.block_size]\n",
    "        outputs = outputs[: self.block_size]\n",
    "\n",
    "        points = torch.zeros(self.numVars + self.numYs, self.numPoints - 1)\n",
    "        for idx, xy in enumerate(zip(chunk[\"X\"], chunk[\"Y\"])):\n",
    "\n",
    "            if not isinstance(xy[0], list) or not isinstance(\n",
    "                xy[1], (list, float, np.float64)\n",
    "            ):\n",
    "                print(f\"Unexpected types: {type(xy[0])}, {type(xy[1])}\")\n",
    "                continue  # Skip if types are incorrect\n",
    "\n",
    "            # don't let to exceed the maximum number of points\n",
    "            if idx >= self.numPoints - 1:\n",
    "                break\n",
    "\n",
    "            x = xy[0]\n",
    "            x = x + [0] * (max(self.numVars - len(x), 0))  # padding\n",
    "\n",
    "            y = [xy[1]] if type(xy[1]) == float or type(xy[1]) == np.float64 else xy[1]\n",
    "\n",
    "            y = y + [0] * (max(self.numYs - len(y), 0))  # padding\n",
    "            p = x + y  # because it is only one point\n",
    "            p = torch.tensor(p)\n",
    "            # replace nan and inf\n",
    "            p = torch.nan_to_num(\n",
    "                p,\n",
    "                nan=self.threshold[1],\n",
    "                posinf=self.threshold[1],\n",
    "                neginf=self.threshold[0],\n",
    "            )\n",
    "\n",
    "            points[:, idx] = p\n",
    "\n",
    "        points = torch.nan_to_num(\n",
    "            points,\n",
    "            nan=self.threshold[1],\n",
    "            posinf=self.threshold[1],\n",
    "            neginf=self.threshold[0],\n",
    "        )\n",
    "\n",
    "        inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "        outputs = torch.tensor(outputs, dtype=torch.long)\n",
    "        numVars = torch.tensor(numVars, dtype=torch.long)\n",
    "        return inputs, outputs, points, numVars\n",
    "\n",
    "\n",
    "# Relative Mean Square Error\n",
    "def relativeErr(y, yHat, info=False, eps=1e-5):\n",
    "    yHat = np.reshape(yHat, [1, -1])[0]\n",
    "    y = np.reshape(y, [1, -1])[0]\n",
    "    if len(y) > 0 and len(y) == len(yHat):\n",
    "        err = ((yHat - y)) ** 2 / np.linalg.norm(y + eps)\n",
    "        if info:\n",
    "            for _ in range(5):\n",
    "                i = np.random.randint(len(y))\n",
    "                # print(\"yPR,yTrue:{},{}, Err:{}\".format(yHat[i], y[i], err[i]))\n",
    "    else:\n",
    "        err = 100\n",
    "\n",
    "    return np.mean(err)\n",
    "\n",
    "\n",
    "def lossFunc(constants, eq, X, Y, eps=1e-5):\n",
    "    err = 0\n",
    "    eq = eq.replace(\"C\", \"{}\").format(*constants)\n",
    "\n",
    "    for x, y in zip(X, Y):\n",
    "        eqTemp = eq + \"\"\n",
    "        if type(x) == np.float32:\n",
    "            x = [x]\n",
    "        for i, e in enumerate(x):\n",
    "            # make sure e is not a tensor\n",
    "            if type(e) == torch.Tensor:\n",
    "                e = e.item()\n",
    "            eqTemp = eqTemp.replace(\"x{}\".format(i + 1), str(e))\n",
    "        try:\n",
    "            yHat = eval(eqTemp)\n",
    "        except:\n",
    "            # print(\"Exception has been occured! EQ: {}, OR: {}\".format(eqTemp, eq))\n",
    "            yHat = 100\n",
    "        try:\n",
    "            # handle overflow\n",
    "            err += relativeErr(y, yHat)  # (y-yHat)**2\n",
    "        except:\n",
    "            # print(\n",
    "            #    \"Exception has been occured! EQ: {}, OR: {}, y:{}-yHat:{}\".format(\n",
    "            #        eqTemp, eq, y, yHat\n",
    "            #    )\n",
    "            # )\n",
    "            err += 10\n",
    "\n",
    "    err /= len(Y)\n",
    "    return err\n",
    "\n",
    "\n",
    "def get_predicted_skeleton(generated_tokens, train_dataset: CharDataset):\n",
    "    predicted_tokens = generated_tokens.cpu().numpy()\n",
    "    predicted = \"\".join([train_dataset.itos[int(idx)] for idx in predicted_tokens])\n",
    "    predicted = predicted.strip(train_dataset.paddingToken).split(\">\")\n",
    "    predicted = predicted[0] if len(predicted[0]) >= 1 else predicted[1]\n",
    "    predicted = predicted.strip(\"<\").strip(\">\")\n",
    "    predicted = predicted.replace(\"Ce\", \"C*e\")\n",
    "\n",
    "    return predicted\n",
    "\n",
    "\n",
    "def sample_skeleton(model, points, variables, train_dataset, batch_size, ddim_step=20):\n",
    "    \"\"\"Sample skeletons from the model using DDIM or DDPM.\"\"\"\n",
    "    return model.sample(\n",
    "        points, variables, train_dataset, batch_size=batch_size, ddim_step=ddim_step\n",
    "    )\n",
    "\n",
    "\n",
    "def fit_constants(predicted_skeleton, X, Y):\n",
    "    \"\"\"Fit constants in the predicted skeleton using optimization.\"\"\"\n",
    "    c = [1.0 for i, x in enumerate(predicted_skeleton) if x == \"C\"]\n",
    "    b = [(-2, 2) for _, x in enumerate(predicted_skeleton) if x == \"C\"]\n",
    "    predicted = predicted_skeleton\n",
    "    if len(c) != 0:\n",
    "        try:\n",
    "            cHat = minimize(lossFunc, c, args=(predicted_skeleton, X, Y), bounds=b)\n",
    "            if cHat.success and cHat.fun != float(\"inf\"):\n",
    "                predicted = predicted_skeleton.replace(\"C\", \"{}\").format(*cHat.x)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid predicted equation or optimization failed: {predicted_skeleton}\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            raise ValueError(\n",
    "                f\"Error fitting constants: {e}, Equation: {predicted_skeleton}\"\n",
    "            )\n",
    "    return predicted\n",
    "\n",
    "\n",
    "def evaluate_equation(eq, xs, target=True):\n",
    "    \"\"\"Evaluate an equation at given points xs.\"\"\"\n",
    "    SAFE_GLOBALS = {\n",
    "        \"sin\": math.sin,\n",
    "        \"cos\": math.cos,\n",
    "        \"tan\": math.tan,\n",
    "        \"log\": math.log,\n",
    "        \"exp\": math.exp,\n",
    "        \"sqrt\": math.sqrt,\n",
    "        \"abs\": abs,\n",
    "        \"pow\": pow,\n",
    "        \"__builtins__\": {},\n",
    "    }\n",
    "    try:\n",
    "        eq_tmp = eq.replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "        for i, x in enumerate(xs):\n",
    "            eq_tmp = eq_tmp.replace(f\"x{i+1}\", str(x))\n",
    "            if \",\" in eq_tmp:\n",
    "                raise ValueError(\"There is a , in the equation!\")\n",
    "        y = eval(eq_tmp, SAFE_GLOBALS)\n",
    "        y = 0 if np.isnan(y) else y\n",
    "        y = 100 if np.isinf(y) else y\n",
    "    except Exception as e:\n",
    "        if target:\n",
    "            # print(f\"TA: Evaluation failed for equation: {eq_tmp}, Reason: {e}\")\n",
    "            pass\n",
    "        else:\n",
    "            # print(f\"PR: Evaluation failed for equation: {eq_tmp}, Reason: {e}\")\n",
    "            pass\n",
    "        y = 100\n",
    "    return y\n",
    "\n",
    "\n",
    "def evaluate_sample(target_eq, predicted_eq, test_points):\n",
    "    \"\"\"Evaluate target and predicted equations over test points.\"\"\"\n",
    "    Ys, Yhats = [], []\n",
    "    for xs in test_points:\n",
    "        Ys.append(evaluate_equation(target_eq, xs, target=True))\n",
    "        Yhats.append(evaluate_equation(predicted_eq, xs, target=False))\n",
    "    return Ys, Yhats\n",
    "\n",
    "\n",
    "def sample_and_evaluate(\n",
    "    model, points, variables, train_dataset, target_eq, t, ddim_step=None\n",
    "):\n",
    "    \"\"\"Sample a skeleton, fit constants, evaluate, and compute error.\"\"\"\n",
    "    predicted_skeleton = sample_skeleton(\n",
    "        model, points, variables, train_dataset, batch_size=1, ddim_step=ddim_step\n",
    "    )[0]\n",
    "\n",
    "    predicted = fit_constants(predicted_skeleton, t[\"X\"], t[\"Y\"])\n",
    "\n",
    "    Ys, Yhats = evaluate_sample(target_eq, predicted, t[\"XT\"])\n",
    "\n",
    "    err = relativeErr(Ys, Yhats, info=True)\n",
    "    return predicted_skeleton, predicted, err\n",
    "\n",
    "\n",
    "def sample_and_select_best(\n",
    "    model,\n",
    "    points,\n",
    "    variables,\n",
    "    train_dataset,\n",
    "    target_eq,\n",
    "    t,\n",
    "    num_tests,\n",
    "    ddim_step=None,\n",
    "):\n",
    "    \"\"\"Sample num_tests times and select the best sample based on error.\"\"\"\n",
    "    best_err = 10000000\n",
    "    best_predicted_skeleton = \"C\"\n",
    "    best_predicted = \"C\"\n",
    "\n",
    "    for _ in range(num_tests):\n",
    "        predicted_skeleton, predicted, err = sample_and_evaluate(\n",
    "            model, points, variables, train_dataset, target_eq, t, ddim_step\n",
    "        )\n",
    "        if err < best_err:\n",
    "            best_err = err\n",
    "            best_predicted_skeleton = predicted_skeleton\n",
    "            best_predicted = predicted\n",
    "\n",
    "    return best_predicted_skeleton, best_predicted, best_err\n",
    "\n",
    "\n",
    "def plot_and_save_results(\n",
    "    resultDict, fName, pconf, titleTemplate, textTest, modelKey=\"SymbolicDiffusion\"\n",
    "):\n",
    "    \"\"\"Plot cumulative error distribution and save results to a file.\n",
    "    Args:\n",
    "        resultDict: dict with results (e.g., {'err': [], 'trg': [], 'prd': []})\n",
    "        fName: str, output file name\n",
    "        pconf: PointNetConfig object with numberofVars\n",
    "        titleTemplate: str, template for plot title\n",
    "        textTest: list of test data\n",
    "        modelKey: str, key for the model in resultDict\n",
    "    \"\"\"\n",
    "    if isinstance(resultDict, dict):\n",
    "        num_eqns = len(resultDict[fName][modelKey][\"err\"])\n",
    "        num_vars = pconf.numberofVars\n",
    "        title = titleTemplate.format(num_eqns, num_vars)\n",
    "\n",
    "        models = list(\n",
    "            key\n",
    "            for key in resultDict[fName].keys()\n",
    "            if len(resultDict[fName][key][\"err\"]) == num_eqns\n",
    "        )\n",
    "        lists_of_error_scores = [\n",
    "            resultDict[fName][key][\"err\"]\n",
    "            for key in models\n",
    "            if len(resultDict[fName][key][\"err\"]) == num_eqns\n",
    "        ]\n",
    "        linestyles = [\"-\", \"dashdot\", \"dotted\", \"--\"]\n",
    "\n",
    "        eps = 0.00001\n",
    "        y, x, _ = plt.hist(\n",
    "            [\n",
    "                np.log([max(min(x + eps, 1e5), 1e-5) for x in e])\n",
    "                for e in lists_of_error_scores\n",
    "            ],\n",
    "            label=models,\n",
    "            cumulative=True,\n",
    "            histtype=\"step\",\n",
    "            bins=2000,\n",
    "            density=True,\n",
    "            log=False,\n",
    "        )\n",
    "        y = np.expand_dims(y, 0)\n",
    "\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for idx, m in enumerate(models):\n",
    "            plt.plot(x[:-1], y[idx] * 100, linestyle=linestyles[idx], label=m)\n",
    "\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Log of Relative Mean Square Error\")\n",
    "        plt.ylabel(\"Normalized Cumulative Frequency\")\n",
    "\n",
    "        name = \"{}.png\".format(fName.split(\".txt\")[0])\n",
    "        plt.savefig(name)\n",
    "        plt.close()\n",
    "\n",
    "        with open(fName, \"w\", encoding=\"utf-8\") as o:\n",
    "            for i in range(num_eqns):\n",
    "                err = resultDict[fName][modelKey][\"err\"][i]\n",
    "                eq = resultDict[fName][modelKey][\"trg\"][i]\n",
    "                predicted = resultDict[fName][modelKey][\"prd\"][i]\n",
    "                print(f\"Test Case {i}.\")\n",
    "                print(f\"Target: {eq}\\nSkeleton: {predicted}\")\n",
    "                print(f\"Err: {err}\\n\")\n",
    "\n",
    "                o.write(f\"Test Case {i}/{len(textTest)-1}.\\n\")\n",
    "                o.write(f\"{eq}\\n\")\n",
    "                o.write(f\"{modelKey}:\\n\")\n",
    "                o.write(f\"{predicted}\\n{err}\\n\\n\")\n",
    "\n",
    "            avg_err = np.mean(resultDict[fName][modelKey][\"err\"])\n",
    "            o.write(f\"Avg Err: {avg_err}\\n\")\n",
    "            print(f\"Avg Err: {avg_err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1676e03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T02:03:18.494083Z",
     "iopub.status.busy": "2025-04-09T02:03:18.493741Z",
     "iopub.status.idle": "2025-04-09T02:03:18.525236Z",
     "shell.execute_reply": "2025-04-09T02:03:18.524630Z"
    },
    "papermill": {
     "duration": 0.037769,
     "end_time": "2025-04-09T02:03:18.526482",
     "exception": false,
     "start_time": "2025-04-09T02:03:18.488713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "# from SymbolicGPT: https://github.com/mojivalipour/symbolicgpt/blob/master/models.py\n",
    "class PointNetConfig:\n",
    "    \"\"\"base PointNet config\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddingSize,\n",
    "        numberofPoints,\n",
    "        numberofVars,\n",
    "        numberofYs,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.embeddingSize = embeddingSize\n",
    "        self.numberofPoints = numberofPoints  # number of points\n",
    "        self.numberofVars = numberofVars  # input dimension (Xs)\n",
    "        self.numberofYs = numberofYs  # output dimension (Ys)\n",
    "\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "\n",
    "class tNet(nn.Module):\n",
    "    \"\"\"\n",
    "    The PointNet structure in the orginal PointNet paper:\n",
    "    PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation by Qi et. al. 2017\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(tNet, self).__init__()\n",
    "\n",
    "        self.activation_func = F.relu\n",
    "        self.num_units = config.embeddingSize\n",
    "\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            config.numberofVars + config.numberofYs, self.num_units, 1\n",
    "        )\n",
    "        self.conv2 = nn.Conv1d(self.num_units, 2 * self.num_units, 1)\n",
    "        self.conv3 = nn.Conv1d(2 * self.num_units, 4 * self.num_units, 1)\n",
    "        self.fc1 = nn.Linear(4 * self.num_units, 2 * self.num_units)\n",
    "        self.fc2 = nn.Linear(2 * self.num_units, self.num_units)\n",
    "\n",
    "        # self.relu = nn.ReLU()\n",
    "\n",
    "        self.input_batch_norm = nn.BatchNorm1d(config.numberofVars + config.numberofYs)\n",
    "        # self.input_layer_norm = nn.LayerNorm(config.numberofPoints)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(self.num_units)\n",
    "        self.bn2 = nn.BatchNorm1d(2 * self.num_units)\n",
    "        self.bn3 = nn.BatchNorm1d(4 * self.num_units)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * self.num_units)\n",
    "        self.bn5 = nn.BatchNorm1d(self.num_units)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch, #features, #points]\n",
    "        :return:\n",
    "            logit: [batch, embedding_size]\n",
    "        \"\"\"\n",
    "        x = self.input_batch_norm(x)\n",
    "        x = self.activation_func(self.bn1(self.conv1(x)))\n",
    "        x = self.activation_func(self.bn2(self.conv2(x)))\n",
    "        x = self.activation_func(self.bn3(self.conv3(x)))\n",
    "        x, _ = torch.max(x, dim=2)  # global max pooling\n",
    "        assert x.size(1) == 4 * self.num_units\n",
    "\n",
    "        x = self.activation_func(self.bn4(self.fc1(x)))\n",
    "        x = self.activation_func(self.bn5(self.fc2(x)))\n",
    "        # x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# from https://github.com/juho-lee/set_transformer/blob/master/modules.py\n",
    "class MAB(nn.Module):\n",
    "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False):\n",
    "        super(MAB, self).__init__()\n",
    "        self.dim_V = dim_V\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
    "        self.fc_k = nn.Linear(dim_K, dim_V)\n",
    "        self.fc_v = nn.Linear(dim_K, dim_V)\n",
    "        if ln:\n",
    "            self.ln0 = nn.LayerNorm(dim_V)\n",
    "            self.ln1 = nn.LayerNorm(dim_V)\n",
    "        self.fc_o = nn.Linear(dim_V, dim_V)\n",
    "\n",
    "    def forward(self, Q, K):\n",
    "        Q = self.fc_q(Q)\n",
    "        K, V = self.fc_k(K), self.fc_v(K)\n",
    "\n",
    "        dim_split = self.dim_V // self.num_heads\n",
    "        Q_ = torch.cat(Q.split(dim_split, 2), 0)\n",
    "        K_ = torch.cat(K.split(dim_split, 2), 0)\n",
    "        V_ = torch.cat(V.split(dim_split, 2), 0)\n",
    "\n",
    "        A = torch.softmax(Q_.bmm(K_.transpose(1, 2)) / math.sqrt(self.dim_V), 2)\n",
    "        O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), 0), 2)\n",
    "        O = O if getattr(self, \"ln0\", None) is None else self.ln0(O)\n",
    "        O = O + F.relu(self.fc_o(O))\n",
    "        O = O if getattr(self, \"ln1\", None) is None else self.ln1(O)\n",
    "        return O\n",
    "\n",
    "\n",
    "class SAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, ln=False):\n",
    "        super(SAB, self).__init__()\n",
    "        self.mab = MAB(dim_in, dim_in, dim_out, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.mab(X, X)\n",
    "\n",
    "\n",
    "class ISAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, num_inds, ln=False):\n",
    "        super(ISAB, self).__init__()\n",
    "        self.I = nn.Parameter(torch.Tensor(1, num_inds, dim_out))\n",
    "        nn.init.xavier_uniform_(self.I)\n",
    "        self.mab0 = MAB(dim_out, dim_in, dim_out, num_heads, ln=ln)\n",
    "        self.mab1 = MAB(dim_in, dim_out, dim_out, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        H = self.mab0(self.I.repeat(X.size(0), 1, 1), X)\n",
    "        return self.mab1(X, H)\n",
    "\n",
    "\n",
    "class PMA(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_seeds, ln=False):\n",
    "        super(PMA, self).__init__()\n",
    "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
    "        nn.init.xavier_uniform_(self.S)\n",
    "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.mab(self.S.repeat(X.size(0), 1, 1), X)\n",
    "\n",
    "\n",
    "# from https://github.com/juho-lee/set_transformer/blob/master/models.py\n",
    "class SetTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_input,\n",
    "        num_outputs,\n",
    "        dim_output,\n",
    "        num_inds=32,\n",
    "        dim_hidden=128,\n",
    "        num_heads=4,\n",
    "        ln=False,\n",
    "    ):\n",
    "        super(SetTransformer, self).__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            ISAB(dim_input, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "            ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            PMA(dim_hidden, num_heads, num_outputs, ln=ln),\n",
    "            SAB(dim_hidden, dim_hidden, num_heads, ln=ln),\n",
    "            SAB(dim_hidden, dim_hidden, num_heads, ln=ln),\n",
    "            nn.Linear(dim_hidden, dim_output),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dec(self.enc(X)).squeeze(1)\n",
    "\n",
    "\n",
    "class NoisePredictionTransformer(nn.Module):\n",
    "    def __init__(self, n_embd, max_seq_len, n_layer=6, n_head=8, max_timesteps=1000):\n",
    "        super().__init__()\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, max_seq_len, n_embd))\n",
    "        self.time_emb = nn.Embedding(max_timesteps, n_embd)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=n_embd,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=n_embd * 4,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layer)\n",
    "\n",
    "    def forward(self, x_t, t, condition):\n",
    "        _, L, _ = x_t.shape\n",
    "        pos_emb = self.pos_emb[:, :L, :]  # [1, L, n_embd]\n",
    "        time_emb = self.time_emb(t)\n",
    "        if time_emb.dim() == 1:  # Scalar t case, [n_embd]\n",
    "            time_emb = time_emb.unsqueeze(0)  # [1, n_embd]\n",
    "        time_emb = time_emb.unsqueeze(1)  # [1, 1, n_embd]\n",
    "        condition = condition.unsqueeze(1)  # [B, 1, n_embd]\n",
    "\n",
    "        x = x_t + pos_emb + time_emb + condition\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "# influenced by https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/simple_diffusion.py\n",
    "class SymbolicGaussianDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tnet_config: PointNetConfig,\n",
    "        vocab_size,\n",
    "        max_seq_len,\n",
    "        padding_idx: int = 0,\n",
    "        max_num_vars: int = 9,\n",
    "        n_layer=6,\n",
    "        n_head=8,\n",
    "        n_embd=512,\n",
    "        timesteps=1000,\n",
    "        beta_start=0.0001,\n",
    "        beta_end=0.02,\n",
    "        ce_weight=1.0,  # Weight for CE loss relative to MSE\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.padding_idx = padding_idx\n",
    "        self.n_embd = n_embd\n",
    "        self.timesteps = timesteps\n",
    "        self.ce_weight = ce_weight\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd, padding_idx=self.padding_idx)\n",
    "        self.vars_emb = nn.Embedding(max_num_vars, n_embd)\n",
    "\n",
    "        self.decoder = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        self.decoder.weight = self.tok_emb.weight\n",
    "\n",
    "        dim_input = tnet_config.numberofVars + tnet_config.numberofYs\n",
    "        self.tnet = SetTransformer(\n",
    "            dim_input=dim_input,\n",
    "            num_outputs=1,\n",
    "            dim_output=n_embd,\n",
    "            num_inds=tnet_config.numberofPoints,\n",
    "        )\n",
    "        self.model = NoisePredictionTransformer(\n",
    "            n_embd, max_seq_len, n_layer, n_head, timesteps\n",
    "        )\n",
    "\n",
    "        # Noise schedule\n",
    "        self.register_buffer(\"beta\", torch.linspace(beta_start, beta_end, timesteps))\n",
    "        self.register_buffer(\"alpha\", 1.0 - self.beta)\n",
    "        self.register_buffer(\"alpha_bar\", torch.cumprod(self.alpha, dim=0))\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        noise = torch.randn_like(x_start)\n",
    "        sqrt_alpha_bar = torch.sqrt(self.alpha_bar[t]).view(-1, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bar[t]).view(-1, 1, 1)\n",
    "\n",
    "        x_t = sqrt_alpha_bar * x_start + sqrt_one_minus_alpha_bar * noise\n",
    "        return x_t\n",
    "\n",
    "    def p_mean_variance(self, x, t, t_next, condition):\n",
    "        alpha_t = self.alpha[t]\n",
    "        alpha_bar_t = self.alpha_bar[t]\n",
    "        alpha_bar_t_next = self.alpha_bar[t_next]\n",
    "        beta_t = self.beta[t]\n",
    "\n",
    "        x_start_pred = self.model(x, t.long(), condition)\n",
    "\n",
    "        coeff1 = torch.sqrt(alpha_bar_t_next) * beta_t / (1 - alpha_bar_t)\n",
    "        coeff2 = torch.sqrt(alpha_t) * (1 - alpha_bar_t_next) / (1 - alpha_bar_t)\n",
    "        mean = coeff1 * x_start_pred + coeff2 * x\n",
    "        variance = (1 - alpha_bar_t_next) / (1 - alpha_bar_t) * beta_t\n",
    "        return mean, variance\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t, t_next, condition):\n",
    "        mean, variance = self.p_mean_variance(x, t, t_next, condition)\n",
    "        if torch.all(t_next == 0):\n",
    "            return mean\n",
    "        noise = torch.randn_like(x)\n",
    "        return mean + torch.sqrt(variance) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, points, variables, train_dataset, batch_size=16, ddim_step=0):\n",
    "        points = points.transpose(1, 2)\n",
    "        condition = self.tnet(points) + self.vars_emb(variables)\n",
    "        shape = (batch_size, self.max_seq_len, self.n_embd)\n",
    "        x = torch.randn(shape, device=self.device)\n",
    "        steps = torch.arange(self.timesteps - 1, -1, -1, device=self.device)\n",
    "\n",
    "        for i in range(0, self.timesteps, ddim_step):\n",
    "            t = steps[i]\n",
    "            t_next = (\n",
    "                steps[i + ddim_step]\n",
    "                if i + ddim_step < self.timesteps\n",
    "                else torch.tensor(0, device=self.device)\n",
    "            )\n",
    "            x = self.p_sample(x, t, t_next, condition)\n",
    "\n",
    "            # Print prediction every 250 steps\n",
    "            # if (i + 1) % 250 == 0:\n",
    "            #    logits = self.decoder(x)  # [B, L, vocab_size]\n",
    "            #    token_indices = torch.argmax(logits, dim=-1)  # [B, L]\n",
    "            #    for j in range(batch_size):\n",
    "            #       token_indices_j = token_indices[j]  # [L]\n",
    "            #        predicted_skeleton = get_predicted_skeleton(\n",
    "            #            token_indices_j, train_dataset\n",
    "            #        )\n",
    "            #        tqdm.write(f\" sample {j}: predicted_skeleton: {predicted_skeleton}\")\n",
    "\n",
    "        logits = self.decoder(x)  # [B, L, vocab_size]\n",
    "        token_indices = torch.argmax(logits, dim=-1)  # [B, L]\n",
    "        predicted_skeletons = []\n",
    "        for j in range(batch_size):\n",
    "            token_indices_j = token_indices[j]  # [L]\n",
    "            predicted_skeleton = get_predicted_skeleton(token_indices_j, train_dataset)\n",
    "            predicted_skeletons.append(predicted_skeleton)\n",
    "        return predicted_skeletons\n",
    "\n",
    "    def p_losses(\n",
    "        self, x_start, points, tokens, variables, t, noise=None, mse: bool = False\n",
    "    ):\n",
    "        \"\"\"Hybrid loss: MSE on embeddings + CE on tokens.\"\"\"\n",
    "        noise = torch.randn_like(x_start)\n",
    "        x_t = self.q_sample(x_start, t, noise)\n",
    "        points = points.transpose(1, 2)\n",
    "        condition = self.tnet(points) + self.vars_emb(variables)\n",
    "        x_start_pred = self.model(x_t, t.long(), condition)\n",
    "\n",
    "        # MSE loss on embeddings\n",
    "        if mse:\n",
    "            mse_loss = F.mse_loss(x_start_pred, x_start)\n",
    "        else:\n",
    "            mse_loss = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        # CE loss on tokens\n",
    "        logits = self.decoder(x_start_pred)  # [B, L, vocab_size]\n",
    "        ce_loss = F.cross_entropy(\n",
    "            logits.view(-1, self.vocab_size),  # [B*L, vocab_size]\n",
    "            tokens.view(-1),  # [B*L]\n",
    "            ignore_index=self.padding_idx,\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "\n",
    "        total_loss = mse_loss + self.ce_weight * ce_loss\n",
    "        return total_loss, mse_loss, ce_loss\n",
    "\n",
    "    def forward(self, points, tokens, variables, t, mse=False):\n",
    "        token_emb = self.tok_emb(tokens)\n",
    "        total_loss, mse_loss, ce_loss = self.p_losses(\n",
    "            token_emb, points, tokens, variables, t, mse=mse\n",
    "        )\n",
    "        return total_loss, mse_loss, ce_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffad3679",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T02:03:18.532912Z",
     "iopub.status.busy": "2025-04-09T02:03:18.532682Z",
     "iopub.status.idle": "2025-04-09T02:03:18.544814Z",
     "shell.execute_reply": "2025-04-09T02:03:18.544174Z"
    },
    "papermill": {
     "duration": 0.016762,
     "end_time": "2025-04-09T02:03:18.546042",
     "exception": false,
     "start_time": "2025-04-09T02:03:18.529280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import tqdm\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: SymbolicGaussianDiffusion,  \n",
    "    train_loader: DataLoader,\n",
    "    optimizer: Adam,\n",
    "    train_dataset: CharDataset,\n",
    "    timesteps: int,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    num_epochs: int,\n",
    ") -> Tuple[float, float, float]:  \n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_mse_loss = 0\n",
    "    total_ce_loss = 0\n",
    "\n",
    "    for i, (_, tokens, points, variables) in tqdm.tqdm(\n",
    "        enumerate(train_loader),\n",
    "        total=len(train_loader),\n",
    "        desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
    "    ):\n",
    "        points, tokens, variables = (\n",
    "            points.to(device),\n",
    "            tokens.to(device),\n",
    "            variables.to(device),\n",
    "        )\n",
    "        t = torch.randint(0, timesteps, (tokens.shape[0],), device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss, mse_loss, ce_loss = model(points, tokens, variables, t)\n",
    "\n",
    "        if (i + 1) % 250 == 0:\n",
    "            print(f\"Batch {i + 1}/{len(train_loader)}:\")\n",
    "            print(f\"total_loss: {total_loss}, mse: {mse_loss}, ce: {ce_loss}\")\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += total_loss.item()\n",
    "        total_mse_loss += mse_loss.item()\n",
    "        total_ce_loss += ce_loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    avg_mse_loss = total_mse_loss / len(train_loader)\n",
    "    avg_ce_loss = total_ce_loss / len(train_loader)\n",
    "    return avg_train_loss, avg_mse_loss, avg_ce_loss\n",
    "\n",
    "\n",
    "def val_epoch(\n",
    "    model: SymbolicGaussianDiffusion,  \n",
    "    val_loader: DataLoader,\n",
    "    train_dataset: CharDataset,\n",
    "    timesteps: int,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    num_epochs: int,\n",
    ") -> Tuple[float, float, float]:  \n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_mse_loss = 0\n",
    "    total_ce_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, tokens, points, variables in tqdm.tqdm(\n",
    "            val_loader, total=len(val_loader), desc=\"Validating\"\n",
    "        ):\n",
    "            points, tokens, variables = (\n",
    "                points.to(device),\n",
    "                tokens.to(device),\n",
    "                variables.to(device),\n",
    "            )\n",
    "            t = torch.randint(0, timesteps, (tokens.shape[0],), device=device)\n",
    "\n",
    "            total_loss, mse_loss, ce_loss = model(points, tokens, variables, t)\n",
    "\n",
    "            total_val_loss += total_loss.item()\n",
    "            total_mse_loss += mse_loss.item()\n",
    "            total_ce_loss += ce_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    avg_mse_loss = total_mse_loss / len(val_loader)\n",
    "    avg_ce_loss = total_ce_loss / len(val_loader)\n",
    "    return avg_val_loss, avg_mse_loss, avg_ce_loss\n",
    "\n",
    "\n",
    "def train_single_gpu(\n",
    "    model: SymbolicGaussianDiffusion,  \n",
    "    train_dataset: CharDataset,\n",
    "    val_dataset: CharDataset,\n",
    "    num_epochs=10,\n",
    "    save_every=2,\n",
    "    batch_size=32,\n",
    "    timesteps=1000,\n",
    "    learning_rate=1e-3,\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_train_loss, avg_mse_loss, avg_ce_loss = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            train_dataset,\n",
    "            timesteps,\n",
    "            device,\n",
    "            epoch,\n",
    "            num_epochs,\n",
    "        )\n",
    "\n",
    "        avg_val_loss, val_mse_loss, val_ce_loss = val_epoch(\n",
    "            model, val_loader, train_dataset, timesteps, device, epoch, num_epochs\n",
    "        )\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        print(\"\\nEpoch Summary:\")\n",
    "        print(\n",
    "            f\"Train Total Loss: {avg_train_loss:.4f} (MSE: {avg_mse_loss:.4f}, CE: {avg_ce_loss:.4f})\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Val Total Loss: {avg_val_loss:.4f} (MSE: {val_mse_loss:.4f}, CE: {val_ce_loss:.4f})\"\n",
    "        )\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            state_dict = model.state_dict()\n",
    "            torch.save(state_dict, \"best_model.pth\")\n",
    "            print(f\"New best model saved with val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b45e19e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T02:03:18.552485Z",
     "iopub.status.busy": "2025-04-09T02:03:18.552264Z",
     "iopub.status.idle": "2025-04-09T02:03:18.622453Z",
     "shell.execute_reply": "2025-04-09T02:03:18.621755Z"
    },
    "papermill": {
     "duration": 0.074806,
     "end_time": "2025-04-09T02:03:18.623704",
     "exception": false,
     "start_time": "2025-04-09T02:03:18.548898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_embd = 512\n",
    "timesteps = 1000\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 5\n",
    "blockSize = 32\n",
    "numVars = 1\n",
    "numYs = 1\n",
    "numPoints = 250\n",
    "target = 'Skeleton'\n",
    "const_range = [-2.1, 2.1]\n",
    "trainRange = [-3.0, 3.0]\n",
    "decimals = 8\n",
    "addVars = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9baea5ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T02:03:18.630470Z",
     "iopub.status.busy": "2025-04-09T02:03:18.630247Z",
     "iopub.status.idle": "2025-04-09T02:03:18.633269Z",
     "shell.execute_reply": "2025-04-09T02:03:18.632618Z"
    },
    "papermill": {
     "duration": 0.007504,
     "end_time": "2025-04-09T02:03:18.634337",
     "exception": false,
     "start_time": "2025-04-09T02:03:18.626833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = \"/kaggle/input/1-var-dataset/1_var_train.json\"\n",
    "val_path = \"/kaggle/input/1-var-dataset/1_var_val.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a5d6dfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T02:03:18.640882Z",
     "iopub.status.busy": "2025-04-09T02:03:18.640658Z",
     "iopub.status.idle": "2025-04-09T02:03:34.843549Z",
     "shell.execute_reply": "2025-04-09T02:03:34.842680Z"
    },
    "papermill": {
     "duration": 16.207553,
     "end_time": "2025-04-09T02:03:34.844862",
     "exception": false,
     "start_time": "2025-04-09T02:03:18.637309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 498795 examples, 27 unique.\n",
      "id:18682\n",
      "outputs:C*x1**4+C>________________________\n",
      "variables:1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "\n",
    "files = glob.glob(train_path)\n",
    "text = processDataFiles(files)\n",
    "text = text.split('\\n') # convert the raw text to a set of examples\n",
    "# skeletons = []\n",
    "skeletons = [json.loads(item)['Skeleton'] for item in text if item.strip()]\n",
    "all_tokens = set()\n",
    "for eq in skeletons:\n",
    "    all_tokens.update(tokenize_equation(eq))\n",
    "integers = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9'}\n",
    "all_tokens.update(integers)  # add all integers to the token set\n",
    "tokens = sorted(list(all_tokens) + ['_', 'T', '<', '>', ':'])  # special tokens\n",
    "trainText = text[:-1] if len(text[-1]) == 0 else text\n",
    "random.shuffle(trainText) # shuffle the dataset, it's important specailly for the combined number of variables experiment\n",
    "train_dataset = CharDataset(trainText, blockSize, tokens=tokens, numVars=numVars,\n",
    "                        numYs=numYs, numPoints=numPoints, target=target, addVars=addVars,\n",
    "                        const_range=const_range, xRange=trainRange, decimals=decimals)\n",
    "\n",
    "idx = np.random.randint(train_dataset.__len__())\n",
    "inputs, outputs, points, variables = train_dataset.__getitem__(idx)\n",
    "inputs = ''.join([train_dataset.itos[int(i)] for i in inputs])\n",
    "outputs = ''.join([train_dataset.itos[int(i)] for i in outputs])\n",
    "print('id:{}\\noutputs:{}\\nvariables:{}'.format(idx,outputs,variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "148e7c8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T02:03:34.852220Z",
     "iopub.status.busy": "2025-04-09T02:03:34.851998Z",
     "iopub.status.idle": "2025-04-09T02:03:34.922104Z",
     "shell.execute_reply": "2025-04-09T02:03:34.921195Z"
    },
    "papermill": {
     "duration": 0.075059,
     "end_time": "2025-04-09T02:03:34.923358",
     "exception": false,
     "start_time": "2025-04-09T02:03:34.848299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 972 examples, 27 unique.\n",
      "tensor(-18.2363) tensor(12.4496)\n",
      "id:514\n",
      "outputs:C*x1**2+C>________________________\n",
      "variables:1\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(val_path)\n",
    "textVal = processDataFiles([files[0]])\n",
    "textVal = textVal.split('\\n') # convert the raw text to a set of examples\n",
    "val_dataset = CharDataset(textVal, blockSize, tokens=tokens, numVars=numVars,\n",
    "                        numYs=numYs, numPoints=numPoints, target=target, addVars=addVars,\n",
    "                        const_range=const_range, xRange=trainRange, decimals=decimals)\n",
    "\n",
    "# print a random sample\n",
    "idx = np.random.randint(val_dataset.__len__())\n",
    "inputs, outputs, points, variables = val_dataset.__getitem__(idx)\n",
    "print(points.min(), points.max())\n",
    "inputs = ''.join([train_dataset.itos[int(i)] for i in inputs])\n",
    "outputs = ''.join([train_dataset.itos[int(i)] for i in outputs])\n",
    "print('id:{}\\noutputs:{}\\nvariables:{}'.format(idx,outputs,variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf6aefdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T02:03:34.930567Z",
     "iopub.status.busy": "2025-04-09T02:03:34.930342Z",
     "iopub.status.idle": "2025-04-09T03:18:16.948925Z",
     "shell.execute_reply": "2025-04-09T03:18:16.947817Z"
    },
    "papermill": {
     "duration": 4482.023624,
     "end_time": "2025-04-09T03:18:16.950306",
     "exception": false,
     "start_time": "2025-04-09T02:03:34.926682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   3%|         | 251/7794 [00:25<12:23, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/7794:\n",
      "total_loss: 1.9147385358810425, mse: 0.0, ce: 1.9147385358810425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   6%|         | 501/7794 [00:50<12:19,  9.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/7794:\n",
      "total_loss: 1.1680240631103516, mse: 0.0, ce: 1.1680240631103516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  10%|         | 751/7794 [01:16<12:33,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 750/7794:\n",
      "total_loss: 0.794838547706604, mse: 0.0, ce: 0.794838547706604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  13%|        | 1001/7794 [01:44<13:00,  8.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/7794:\n",
      "total_loss: 0.8917779922485352, mse: 0.0, ce: 0.8917779922485352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  16%|        | 1251/7794 [02:14<13:04,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1250/7794:\n",
      "total_loss: 0.976093053817749, mse: 0.0, ce: 0.976093053817749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  19%|        | 1501/7794 [02:42<11:53,  8.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500/7794:\n",
      "total_loss: 0.5075061917304993, mse: 0.0, ce: 0.5075061917304993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  22%|       | 1751/7794 [03:11<11:40,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1750/7794:\n",
      "total_loss: 0.6710942387580872, mse: 0.0, ce: 0.6710942387580872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  26%|       | 2001/7794 [03:40<11:08,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/7794:\n",
      "total_loss: 0.7373420000076294, mse: 0.0, ce: 0.7373420000076294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  29%|       | 2251/7794 [04:08<10:33,  8.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2250/7794:\n",
      "total_loss: 0.8249247670173645, mse: 0.0, ce: 0.8249247670173645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  32%|      | 2501/7794 [04:37<10:04,  8.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500/7794:\n",
      "total_loss: 0.9142760634422302, mse: 0.0, ce: 0.9142760634422302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  35%|      | 2751/7794 [05:06<09:39,  8.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2750/7794:\n",
      "total_loss: 0.4680773913860321, mse: 0.0, ce: 0.4680773913860321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  39%|      | 3001/7794 [05:34<09:06,  8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000/7794:\n",
      "total_loss: 0.6929225921630859, mse: 0.0, ce: 0.6929225921630859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  42%|     | 3251/7794 [06:03<08:40,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3250/7794:\n",
      "total_loss: 0.6881923079490662, mse: 0.0, ce: 0.6881923079490662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  45%|     | 3501/7794 [06:31<08:06,  8.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500/7794:\n",
      "total_loss: 0.41894248127937317, mse: 0.0, ce: 0.41894248127937317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  48%|     | 3751/7794 [06:59<07:36,  8.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3750/7794:\n",
      "total_loss: 0.5422505140304565, mse: 0.0, ce: 0.5422505140304565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  51%|    | 4001/7794 [07:28<07:12,  8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000/7794:\n",
      "total_loss: 0.4986065626144409, mse: 0.0, ce: 0.4986065626144409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  55%|    | 4251/7794 [07:56<06:42,  8.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4250/7794:\n",
      "total_loss: 0.38510772585868835, mse: 0.0, ce: 0.38510772585868835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  58%|    | 4501/7794 [08:25<06:19,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4500/7794:\n",
      "total_loss: 0.3544669449329376, mse: 0.0, ce: 0.3544669449329376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  61%|    | 4751/7794 [08:53<05:45,  8.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4750/7794:\n",
      "total_loss: 0.45952022075653076, mse: 0.0, ce: 0.45952022075653076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  64%|   | 5001/7794 [09:22<05:17,  8.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000/7794:\n",
      "total_loss: 0.28392136096954346, mse: 0.0, ce: 0.28392136096954346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  67%|   | 5251/7794 [09:51<04:51,  8.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5250/7794:\n",
      "total_loss: 0.3058319389820099, mse: 0.0, ce: 0.3058319389820099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  71%|   | 5501/7794 [10:19<04:24,  8.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/7794:\n",
      "total_loss: 0.6044626235961914, mse: 0.0, ce: 0.6044626235961914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  74%|  | 5751/7794 [10:48<03:56,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5750/7794:\n",
      "total_loss: 0.4427870512008667, mse: 0.0, ce: 0.4427870512008667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  77%|  | 6001/7794 [11:17<03:26,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000/7794:\n",
      "total_loss: 0.5322107076644897, mse: 0.0, ce: 0.5322107076644897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  80%|  | 6251/7794 [11:46<02:58,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6250/7794:\n",
      "total_loss: 0.41403019428253174, mse: 0.0, ce: 0.41403019428253174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  83%| | 6501/7794 [12:15<02:28,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6500/7794:\n",
      "total_loss: 0.5959781408309937, mse: 0.0, ce: 0.5959781408309937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  87%| | 6751/7794 [12:43<02:00,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6750/7794:\n",
      "total_loss: 0.4316052496433258, mse: 0.0, ce: 0.4316052496433258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  90%| | 7001/7794 [13:12<01:31,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000/7794:\n",
      "total_loss: 0.5238215327262878, mse: 0.0, ce: 0.5238215327262878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  93%|| 7251/7794 [13:41<01:02,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7250/7794:\n",
      "total_loss: 0.5818496942520142, mse: 0.0, ce: 0.5818496942520142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  96%|| 7501/7794 [14:10<00:34,  8.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500/7794:\n",
      "total_loss: 0.37752699851989746, mse: 0.0, ce: 0.37752699851989746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  99%|| 7751/7794 [14:39<00:04,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7750/7794:\n",
      "total_loss: 0.5519454479217529, mse: 0.0, ce: 0.5519454479217529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|| 7794/7794 [14:44<00:00,  8.82it/s]\n",
      "Validating: 100%|| 16/16 [00:00<00:00, 17.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Total Loss: 0.7103 (MSE: 0.0000, CE: 0.7103)\n",
      "Val Total Loss: 0.4597 (MSE: 0.0000, CE: 0.4597)\n",
      "Learning Rate: 0.000100\n",
      "New best model saved with val loss: 0.4597\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:   3%|         | 251/7794 [00:29<14:35,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/7794:\n",
      "total_loss: 0.5555924773216248, mse: 0.0, ce: 0.5555924773216248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:   6%|         | 501/7794 [00:58<14:01,  8.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/7794:\n",
      "total_loss: 0.41270703077316284, mse: 0.0, ce: 0.41270703077316284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  10%|         | 751/7794 [01:26<13:36,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 750/7794:\n",
      "total_loss: 0.5256460309028625, mse: 0.0, ce: 0.5256460309028625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  13%|        | 1001/7794 [01:55<12:59,  8.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/7794:\n",
      "total_loss: 0.4725441634654999, mse: 0.0, ce: 0.4725441634654999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  16%|        | 1251/7794 [02:24<12:25,  8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1250/7794:\n",
      "total_loss: 0.5229190587997437, mse: 0.0, ce: 0.5229190587997437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  19%|        | 1501/7794 [02:53<11:58,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500/7794:\n",
      "total_loss: 0.36023861169815063, mse: 0.0, ce: 0.36023861169815063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  22%|       | 1751/7794 [03:21<11:37,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1750/7794:\n",
      "total_loss: 0.2635965943336487, mse: 0.0, ce: 0.2635965943336487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  26%|       | 2001/7794 [03:50<11:05,  8.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/7794:\n",
      "total_loss: 0.40720170736312866, mse: 0.0, ce: 0.40720170736312866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  29%|       | 2251/7794 [04:19<10:33,  8.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2250/7794:\n",
      "total_loss: 0.4212731420993805, mse: 0.0, ce: 0.4212731420993805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  32%|      | 2501/7794 [04:47<10:08,  8.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500/7794:\n",
      "total_loss: 0.35160526633262634, mse: 0.0, ce: 0.35160526633262634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  35%|      | 2751/7794 [05:16<09:38,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2750/7794:\n",
      "total_loss: 0.42271313071250916, mse: 0.0, ce: 0.42271313071250916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  39%|      | 3001/7794 [05:45<09:14,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000/7794:\n",
      "total_loss: 0.26216357946395874, mse: 0.0, ce: 0.26216357946395874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  42%|     | 3251/7794 [06:14<08:49,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3250/7794:\n",
      "total_loss: 0.2757847011089325, mse: 0.0, ce: 0.2757847011089325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  45%|     | 3501/7794 [06:43<08:18,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500/7794:\n",
      "total_loss: 0.583404004573822, mse: 0.0, ce: 0.583404004573822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  48%|     | 3751/7794 [07:12<07:53,  8.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3750/7794:\n",
      "total_loss: 0.4607712924480438, mse: 0.0, ce: 0.4607712924480438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  51%|    | 4001/7794 [07:41<07:18,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000/7794:\n",
      "total_loss: 0.36531102657318115, mse: 0.0, ce: 0.36531102657318115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  55%|    | 4251/7794 [08:09<06:51,  8.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4250/7794:\n",
      "total_loss: 0.35364094376564026, mse: 0.0, ce: 0.35364094376564026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  58%|    | 4501/7794 [08:38<06:19,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4500/7794:\n",
      "total_loss: 0.4577775299549103, mse: 0.0, ce: 0.4577775299549103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  61%|    | 4751/7794 [09:07<05:51,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4750/7794:\n",
      "total_loss: 0.39569398760795593, mse: 0.0, ce: 0.39569398760795593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  64%|   | 5001/7794 [09:36<05:20,  8.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000/7794:\n",
      "total_loss: 0.25155073404312134, mse: 0.0, ce: 0.25155073404312134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  67%|   | 5251/7794 [10:04<04:50,  8.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5250/7794:\n",
      "total_loss: 0.38150280714035034, mse: 0.0, ce: 0.38150280714035034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  71%|   | 5501/7794 [10:33<04:24,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/7794:\n",
      "total_loss: 0.23434455692768097, mse: 0.0, ce: 0.23434455692768097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  74%|  | 5751/7794 [11:02<03:53,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5750/7794:\n",
      "total_loss: 0.3926100730895996, mse: 0.0, ce: 0.3926100730895996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  77%|  | 6001/7794 [11:30<03:26,  8.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000/7794:\n",
      "total_loss: 0.29831016063690186, mse: 0.0, ce: 0.29831016063690186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  80%|  | 6251/7794 [11:59<02:56,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6250/7794:\n",
      "total_loss: 0.503050684928894, mse: 0.0, ce: 0.503050684928894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  83%| | 6501/7794 [12:28<02:29,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6500/7794:\n",
      "total_loss: 0.47391277551651, mse: 0.0, ce: 0.47391277551651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  87%| | 6751/7794 [12:57<02:00,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6750/7794:\n",
      "total_loss: 0.42167866230010986, mse: 0.0, ce: 0.42167866230010986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  90%| | 7001/7794 [13:26<01:32,  8.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000/7794:\n",
      "total_loss: 0.5045170187950134, mse: 0.0, ce: 0.5045170187950134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  93%|| 7251/7794 [13:55<01:02,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7250/7794:\n",
      "total_loss: 0.39063456654548645, mse: 0.0, ce: 0.39063456654548645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  96%|| 7501/7794 [14:23<00:33,  8.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500/7794:\n",
      "total_loss: 0.3626077473163605, mse: 0.0, ce: 0.3626077473163605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  99%|| 7751/7794 [14:52<00:04,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7750/7794:\n",
      "total_loss: 0.47497785091400146, mse: 0.0, ce: 0.47497785091400146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|| 7794/7794 [14:57<00:00,  8.68it/s]\n",
      "Validating: 100%|| 16/16 [00:00<00:00, 18.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Total Loss: 0.4321 (MSE: 0.0000, CE: 0.4321)\n",
      "Val Total Loss: 0.3759 (MSE: 0.0000, CE: 0.3759)\n",
      "Learning Rate: 0.000100\n",
      "New best model saved with val loss: 0.3759\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:   3%|         | 251/7794 [00:29<14:35,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/7794:\n",
      "total_loss: 0.44902199506759644, mse: 0.0, ce: 0.44902199506759644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:   6%|         | 501/7794 [00:58<14:04,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/7794:\n",
      "total_loss: 0.5410180687904358, mse: 0.0, ce: 0.5410180687904358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  10%|         | 751/7794 [01:27<13:32,  8.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 750/7794:\n",
      "total_loss: 0.4922640919685364, mse: 0.0, ce: 0.4922640919685364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  13%|        | 1001/7794 [01:55<13:03,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/7794:\n",
      "total_loss: 0.4137097895145416, mse: 0.0, ce: 0.4137097895145416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  16%|        | 1251/7794 [02:24<12:30,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1250/7794:\n",
      "total_loss: 0.43252426385879517, mse: 0.0, ce: 0.43252426385879517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  19%|        | 1501/7794 [02:53<12:00,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500/7794:\n",
      "total_loss: 0.47737428545951843, mse: 0.0, ce: 0.47737428545951843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  22%|       | 1751/7794 [03:22<11:36,  8.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1750/7794:\n",
      "total_loss: 0.311583936214447, mse: 0.0, ce: 0.311583936214447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  26%|       | 2001/7794 [03:50<11:05,  8.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/7794:\n",
      "total_loss: 0.33393001556396484, mse: 0.0, ce: 0.33393001556396484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  29%|       | 2251/7794 [04:19<10:35,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2250/7794:\n",
      "total_loss: 0.27735885977745056, mse: 0.0, ce: 0.27735885977745056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  32%|      | 2501/7794 [04:48<10:05,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500/7794:\n",
      "total_loss: 0.3983685374259949, mse: 0.0, ce: 0.3983685374259949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  35%|      | 2751/7794 [05:16<09:38,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2750/7794:\n",
      "total_loss: 0.35338690876960754, mse: 0.0, ce: 0.35338690876960754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  39%|      | 3001/7794 [05:45<09:08,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000/7794:\n",
      "total_loss: 0.6349974870681763, mse: 0.0, ce: 0.6349974870681763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  42%|     | 3251/7794 [06:14<08:43,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3250/7794:\n",
      "total_loss: 0.429348886013031, mse: 0.0, ce: 0.429348886013031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  45%|     | 3501/7794 [06:42<08:09,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500/7794:\n",
      "total_loss: 0.39378121495246887, mse: 0.0, ce: 0.39378121495246887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  48%|     | 3751/7794 [07:11<07:48,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3750/7794:\n",
      "total_loss: 0.3111211955547333, mse: 0.0, ce: 0.3111211955547333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  51%|    | 4001/7794 [07:40<07:12,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000/7794:\n",
      "total_loss: 0.2837434709072113, mse: 0.0, ce: 0.2837434709072113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  55%|    | 4251/7794 [08:08<06:46,  8.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4250/7794:\n",
      "total_loss: 0.41182759404182434, mse: 0.0, ce: 0.41182759404182434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  58%|    | 4501/7794 [08:37<06:20,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4500/7794:\n",
      "total_loss: 0.2661867141723633, mse: 0.0, ce: 0.2661867141723633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  61%|    | 4751/7794 [09:06<05:48,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4750/7794:\n",
      "total_loss: 0.3729622960090637, mse: 0.0, ce: 0.3729622960090637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  64%|   | 5001/7794 [09:34<05:22,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000/7794:\n",
      "total_loss: 0.543934166431427, mse: 0.0, ce: 0.543934166431427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  67%|   | 5251/7794 [10:03<04:50,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5250/7794:\n",
      "total_loss: 0.4566991627216339, mse: 0.0, ce: 0.4566991627216339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  71%|   | 5501/7794 [10:31<04:22,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/7794:\n",
      "total_loss: 0.3841487765312195, mse: 0.0, ce: 0.3841487765312195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  74%|  | 5751/7794 [11:00<03:55,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5750/7794:\n",
      "total_loss: 0.2958306670188904, mse: 0.0, ce: 0.2958306670188904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  77%|  | 6001/7794 [11:29<03:25,  8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000/7794:\n",
      "total_loss: 0.39643630385398865, mse: 0.0, ce: 0.39643630385398865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  80%|  | 6251/7794 [11:57<02:58,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6250/7794:\n",
      "total_loss: 0.38208213448524475, mse: 0.0, ce: 0.38208213448524475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  83%| | 6501/7794 [12:27<02:30,  8.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6500/7794:\n",
      "total_loss: 0.22432425618171692, mse: 0.0, ce: 0.22432425618171692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  87%| | 6751/7794 [12:56<02:00,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6750/7794:\n",
      "total_loss: 0.2051161229610443, mse: 0.0, ce: 0.2051161229610443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  90%| | 7001/7794 [13:24<01:30,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000/7794:\n",
      "total_loss: 0.36231347918510437, mse: 0.0, ce: 0.36231347918510437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  93%|| 7251/7794 [13:53<01:01,  8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7250/7794:\n",
      "total_loss: 0.23020902276039124, mse: 0.0, ce: 0.23020902276039124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  96%|| 7501/7794 [14:22<00:33,  8.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500/7794:\n",
      "total_loss: 0.3561672568321228, mse: 0.0, ce: 0.3561672568321228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  99%|| 7751/7794 [14:50<00:04,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7750/7794:\n",
      "total_loss: 0.4561530351638794, mse: 0.0, ce: 0.4561530351638794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|| 7794/7794 [14:55<00:00,  8.70it/s]\n",
      "Validating: 100%|| 16/16 [00:00<00:00, 19.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Total Loss: 0.3661 (MSE: 0.0000, CE: 0.3661)\n",
      "Val Total Loss: 0.2844 (MSE: 0.0000, CE: 0.2844)\n",
      "Learning Rate: 0.000100\n",
      "New best model saved with val loss: 0.2844\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:   3%|         | 251/7794 [00:29<14:33,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/7794:\n",
      "total_loss: 0.28348344564437866, mse: 0.0, ce: 0.28348344564437866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:   6%|         | 501/7794 [00:58<14:10,  8.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/7794:\n",
      "total_loss: 0.36334502696990967, mse: 0.0, ce: 0.36334502696990967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  10%|         | 751/7794 [01:27<13:32,  8.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 750/7794:\n",
      "total_loss: 0.3500862419605255, mse: 0.0, ce: 0.3500862419605255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  13%|        | 1001/7794 [01:56<13:00,  8.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/7794:\n",
      "total_loss: 0.3174585998058319, mse: 0.0, ce: 0.3174585998058319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  16%|        | 1251/7794 [02:25<12:39,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1250/7794:\n",
      "total_loss: 0.27352175116539, mse: 0.0, ce: 0.27352175116539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  19%|        | 1501/7794 [02:54<12:09,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500/7794:\n",
      "total_loss: 0.4309214949607849, mse: 0.0, ce: 0.4309214949607849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  22%|       | 1751/7794 [03:22<11:29,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1750/7794:\n",
      "total_loss: 0.38774383068084717, mse: 0.0, ce: 0.38774383068084717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  26%|       | 2001/7794 [03:51<11:12,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/7794:\n",
      "total_loss: 0.29338493943214417, mse: 0.0, ce: 0.29338493943214417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  29%|       | 2251/7794 [04:20<10:35,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2250/7794:\n",
      "total_loss: 0.29802319407463074, mse: 0.0, ce: 0.29802319407463074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  32%|      | 2501/7794 [04:49<10:10,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500/7794:\n",
      "total_loss: 0.33022943139076233, mse: 0.0, ce: 0.33022943139076233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  35%|      | 2751/7794 [05:17<09:37,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2750/7794:\n",
      "total_loss: 0.3005981147289276, mse: 0.0, ce: 0.3005981147289276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  39%|      | 3001/7794 [05:46<09:09,  8.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000/7794:\n",
      "total_loss: 0.3882293403148651, mse: 0.0, ce: 0.3882293403148651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  42%|     | 3251/7794 [06:15<08:38,  8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3250/7794:\n",
      "total_loss: 0.32288074493408203, mse: 0.0, ce: 0.32288074493408203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  45%|     | 3501/7794 [06:43<08:11,  8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500/7794:\n",
      "total_loss: 0.11728587001562119, mse: 0.0, ce: 0.11728587001562119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  48%|     | 3751/7794 [07:12<07:42,  8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3750/7794:\n",
      "total_loss: 0.2608303129673004, mse: 0.0, ce: 0.2608303129673004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  51%|    | 4001/7794 [07:41<07:17,  8.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000/7794:\n",
      "total_loss: 0.19856040179729462, mse: 0.0, ce: 0.19856040179729462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  55%|    | 4251/7794 [08:09<06:43,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4250/7794:\n",
      "total_loss: 0.31100812554359436, mse: 0.0, ce: 0.31100812554359436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  58%|    | 4501/7794 [08:38<06:18,  8.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4500/7794:\n",
      "total_loss: 0.24007944762706757, mse: 0.0, ce: 0.24007944762706757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  61%|    | 4751/7794 [09:07<05:47,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4750/7794:\n",
      "total_loss: 0.19923023879528046, mse: 0.0, ce: 0.19923023879528046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  64%|   | 5001/7794 [09:35<05:19,  8.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000/7794:\n",
      "total_loss: 0.11007852107286453, mse: 0.0, ce: 0.11007852107286453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  67%|   | 5251/7794 [10:04<04:53,  8.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5250/7794:\n",
      "total_loss: 0.41591960191726685, mse: 0.0, ce: 0.41591960191726685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  71%|   | 5501/7794 [10:33<04:21,  8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/7794:\n",
      "total_loss: 0.22554203867912292, mse: 0.0, ce: 0.22554203867912292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  74%|  | 5751/7794 [11:01<03:55,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5750/7794:\n",
      "total_loss: 0.4552847146987915, mse: 0.0, ce: 0.4552847146987915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  77%|  | 6001/7794 [11:30<03:24,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000/7794:\n",
      "total_loss: 0.14575329422950745, mse: 0.0, ce: 0.14575329422950745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  80%|  | 6251/7794 [11:59<02:59,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6250/7794:\n",
      "total_loss: 0.20192983746528625, mse: 0.0, ce: 0.20192983746528625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  83%| | 6501/7794 [12:27<02:27,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6500/7794:\n",
      "total_loss: 0.2670281231403351, mse: 0.0, ce: 0.2670281231403351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  87%| | 6751/7794 [12:56<02:01,  8.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6750/7794:\n",
      "total_loss: 0.3755512237548828, mse: 0.0, ce: 0.3755512237548828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  90%| | 7001/7794 [13:25<01:30,  8.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000/7794:\n",
      "total_loss: 0.24112971127033234, mse: 0.0, ce: 0.24112971127033234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  93%|| 7251/7794 [13:53<01:02,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7250/7794:\n",
      "total_loss: 0.24013635516166687, mse: 0.0, ce: 0.24013635516166687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  96%|| 7501/7794 [14:22<00:33,  8.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500/7794:\n",
      "total_loss: 0.3464743494987488, mse: 0.0, ce: 0.3464743494987488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  99%|| 7751/7794 [14:51<00:04,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7750/7794:\n",
      "total_loss: 0.2935014069080353, mse: 0.0, ce: 0.2935014069080353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|| 7794/7794 [14:56<00:00,  8.70it/s]\n",
      "Validating: 100%|| 16/16 [00:00<00:00, 18.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Total Loss: 0.3193 (MSE: 0.0000, CE: 0.3193)\n",
      "Val Total Loss: 0.2760 (MSE: 0.0000, CE: 0.2760)\n",
      "Learning Rate: 0.000100\n",
      "New best model saved with val loss: 0.2760\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:   3%|         | 251/7794 [00:29<14:27,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/7794:\n",
      "total_loss: 0.34782496094703674, mse: 0.0, ce: 0.34782496094703674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:   6%|         | 501/7794 [00:58<14:06,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/7794:\n",
      "total_loss: 0.38672342896461487, mse: 0.0, ce: 0.38672342896461487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  10%|         | 751/7794 [01:27<13:38,  8.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 750/7794:\n",
      "total_loss: 0.27816903591156006, mse: 0.0, ce: 0.27816903591156006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  13%|        | 1001/7794 [01:56<13:09,  8.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/7794:\n",
      "total_loss: 0.32402461767196655, mse: 0.0, ce: 0.32402461767196655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  16%|        | 1251/7794 [02:25<12:38,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1250/7794:\n",
      "total_loss: 0.17650781571865082, mse: 0.0, ce: 0.17650781571865082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  19%|        | 1501/7794 [02:53<12:07,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500/7794:\n",
      "total_loss: 0.24549946188926697, mse: 0.0, ce: 0.24549946188926697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  22%|       | 1751/7794 [03:22<11:36,  8.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1750/7794:\n",
      "total_loss: 0.43484243750572205, mse: 0.0, ce: 0.43484243750572205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  26%|       | 2001/7794 [03:51<11:04,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/7794:\n",
      "total_loss: 0.21782054007053375, mse: 0.0, ce: 0.21782054007053375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  29%|       | 2251/7794 [04:20<10:38,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2250/7794:\n",
      "total_loss: 0.18875761330127716, mse: 0.0, ce: 0.18875761330127716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  32%|      | 2501/7794 [04:49<10:14,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500/7794:\n",
      "total_loss: 0.20861206948757172, mse: 0.0, ce: 0.20861206948757172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  35%|      | 2751/7794 [05:17<09:46,  8.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2750/7794:\n",
      "total_loss: 0.27176302671432495, mse: 0.0, ce: 0.27176302671432495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  39%|      | 3001/7794 [05:46<09:09,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000/7794:\n",
      "total_loss: 0.3088768422603607, mse: 0.0, ce: 0.3088768422603607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  42%|     | 3251/7794 [06:15<08:42,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3250/7794:\n",
      "total_loss: 0.10510020703077316, mse: 0.0, ce: 0.10510020703077316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  45%|     | 3501/7794 [06:44<08:15,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500/7794:\n",
      "total_loss: 0.2853284478187561, mse: 0.0, ce: 0.2853284478187561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  48%|     | 3751/7794 [07:12<07:46,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3750/7794:\n",
      "total_loss: 0.36027780175209045, mse: 0.0, ce: 0.36027780175209045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  51%|    | 4001/7794 [07:41<07:15,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000/7794:\n",
      "total_loss: 0.23194482922554016, mse: 0.0, ce: 0.23194482922554016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  55%|    | 4251/7794 [08:10<06:44,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4250/7794:\n",
      "total_loss: 0.28086093068122864, mse: 0.0, ce: 0.28086093068122864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  58%|    | 4501/7794 [08:39<06:15,  8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4500/7794:\n",
      "total_loss: 0.27259600162506104, mse: 0.0, ce: 0.27259600162506104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  61%|    | 4751/7794 [09:07<05:47,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4750/7794:\n",
      "total_loss: 0.1532682627439499, mse: 0.0, ce: 0.1532682627439499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  64%|   | 5001/7794 [09:36<05:19,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000/7794:\n",
      "total_loss: 0.3062325417995453, mse: 0.0, ce: 0.3062325417995453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  67%|   | 5251/7794 [10:05<04:52,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5250/7794:\n",
      "total_loss: 0.29048165678977966, mse: 0.0, ce: 0.29048165678977966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  71%|   | 5501/7794 [10:34<04:22,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/7794:\n",
      "total_loss: 0.24690461158752441, mse: 0.0, ce: 0.24690461158752441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  74%|  | 5751/7794 [11:03<03:55,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5750/7794:\n",
      "total_loss: 0.26632383465766907, mse: 0.0, ce: 0.26632383465766907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  77%|  | 6001/7794 [11:31<03:26,  8.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000/7794:\n",
      "total_loss: 0.31342798471450806, mse: 0.0, ce: 0.31342798471450806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  80%|  | 6251/7794 [12:00<02:57,  8.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6250/7794:\n",
      "total_loss: 0.17901115119457245, mse: 0.0, ce: 0.17901115119457245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  83%| | 6501/7794 [12:29<02:29,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6500/7794:\n",
      "total_loss: 0.2982303202152252, mse: 0.0, ce: 0.2982303202152252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  87%| | 6751/7794 [12:58<02:00,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6750/7794:\n",
      "total_loss: 0.32657065987586975, mse: 0.0, ce: 0.32657065987586975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  90%| | 7001/7794 [13:26<01:31,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000/7794:\n",
      "total_loss: 0.1927371472120285, mse: 0.0, ce: 0.1927371472120285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  93%|| 7251/7794 [13:55<01:03,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7250/7794:\n",
      "total_loss: 0.1535712033510208, mse: 0.0, ce: 0.1535712033510208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  96%|| 7501/7794 [14:24<00:33,  8.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500/7794:\n",
      "total_loss: 0.3200993239879608, mse: 0.0, ce: 0.3200993239879608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  99%|| 7751/7794 [14:53<00:05,  8.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7750/7794:\n",
      "total_loss: 0.08205714076757431, mse: 0.0, ce: 0.08205714076757431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|| 7794/7794 [14:58<00:00,  8.68it/s]\n",
      "Validating: 100%|| 16/16 [00:00<00:00, 18.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Total Loss: 0.2774 (MSE: 0.0000, CE: 0.2774)\n",
      "Val Total Loss: 0.2104 (MSE: 0.0000, CE: 0.2104)\n",
      "Learning Rate: 0.000100\n",
      "New best model saved with val loss: 0.2104\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pconfig = PointNetConfig(\n",
    "    embeddingSize=n_embd,\n",
    "    numberofPoints=numPoints,\n",
    "    numberofVars=numVars,\n",
    "    numberofYs=numYs,\n",
    ")\n",
    "\n",
    "model = SymbolicGaussianDiffusion(\n",
    "    tnet_config=pconfig,  \n",
    "    vocab_size=train_dataset.vocab_size,\n",
    "    max_seq_len=blockSize,\n",
    "    padding_idx=train_dataset.paddingID,\n",
    "    max_num_vars=9,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    n_embd=n_embd,\n",
    "    timesteps=timesteps,\n",
    "    beta_start=0.0001,\n",
    "    beta_end=0.02,\n",
    "    ce_weight=1.0  \n",
    ")\n",
    "\n",
    "train_single_gpu(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    num_epochs=num_epochs,\n",
    "    save_every=2,\n",
    "    batch_size=batch_size,\n",
    "    timesteps=timesteps,\n",
    "    learning_rate=learning_rate\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6977314,
     "sourceId": 11178716,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 282263,
     "modelInstanceId": 261112,
     "sourceId": 306062,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 290784,
     "modelInstanceId": 269794,
     "sourceId": 319741,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 290853,
     "modelInstanceId": 269860,
     "sourceId": 319828,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4510.721171,
   "end_time": "2025-04-09T03:18:21.129854",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-09T02:03:10.408683",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
