{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ced063a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-12T23:01:35.934215Z",
     "iopub.status.busy": "2025-04-12T23:01:35.933975Z",
     "iopub.status.idle": "2025-04-12T23:01:36.997693Z",
     "shell.execute_reply": "2025-04-12T23:01:36.996632Z"
    },
    "papermill": {
     "duration": 1.069152,
     "end_time": "2025-04-12T23:01:36.999295",
     "exception": false,
     "start_time": "2025-04-12T23:01:35.930143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/config.txt\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Val/0_5_4_17062021_094043.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Val/0_4_3_17062021_094043.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Val/0_2_1_17062021_094043.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Val/0_1_0_17062021_094043.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Val/0_3_2_17062021_094043.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Test/0_5_4_17062021_094123.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Test/0_1_0_17062021_094123.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Test/0_3_2_17062021_094123.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Test/0_2_1_17062021_094123.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Test/0_4_3_17062021_094123.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/3_2_1_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/0_3_2_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/2_1_0_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/8_5_4_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/4_3_2_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/5_3_2_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/1_4_3_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/2_4_3_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/1_2_1_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/6_5_4_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/4_5_4_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/2_2_1_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/3_5_4_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/0_5_4_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/0_2_1_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/2_3_2_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/3_3_2_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/7_5_4_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/1_3_2_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/2_5_4_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/3_4_3_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/1_1_0_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/4_2_1_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/5_5_4_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/0_4_3_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/0_1_0_17062021_081949.json\n",
      "/kaggle/input/1-5-var-dataset/1-5Var_RandSupport_RandLength_-3to3_-5.0to-3.0-3.0to5.0_10to200Points/Train/1_5_4_17062021_081949.json\n",
      "/kaggle/input/xye_9var/pytorch/default/1/XYE_9Var.pt\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/config.txt\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Val/0_2_0_13062021_184140.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Test/0_2_0_13062021_184319.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/4_2_0_15062021_123153.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/2_2_0_15062021_123153.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/7_2_0_15062021_123153.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/5_2_0_15062021_123153.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/0_2_0_15062021_123153.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/6_2_0_15062021_123153.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/1_2_0_15062021_123153.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/3_2_0_15062021_123153.json\n",
      "/kaggle/input/xye_1var/pytorch/default/1/XYE_1Var.pt\n",
      "/kaggle/input/1-var-dataset/1_var_test.json\n",
      "/kaggle/input/1-var-dataset/1_var_val.json\n",
      "/kaggle/input/1-var-dataset/1_var_train.json\n",
      "/kaggle/input/symbolic_diffusion_initial/pytorch/default/1/symbolic_diffusion_model.pth\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28ea184b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:01:37.006551Z",
     "iopub.status.busy": "2025-04-12T23:01:37.006146Z",
     "iopub.status.idle": "2025-04-12T23:01:41.993172Z",
     "shell.execute_reply": "2025-04-12T23:01:41.992233Z"
    },
    "papermill": {
     "duration": 4.992132,
     "end_time": "2025-04-12T23:01:41.994720",
     "exception": false,
     "start_time": "2025-04-12T23:01:37.002588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.optimize import minimize\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def generateDataStrEq(\n",
    "    eq, n_points=2, n_vars=3, decimals=4, supportPoints=None, min_x=0, max_x=3\n",
    "):\n",
    "    X = []\n",
    "    Y = []\n",
    "    # TODO: Need to make this faster\n",
    "    for p in range(n_points):\n",
    "        if supportPoints is None:\n",
    "            if type(min_x) == list:\n",
    "                x = []\n",
    "                for _ in range(n_vars):\n",
    "                    idx = np.random.randint(len(min_x))\n",
    "                    x += list(\n",
    "                        np.round(np.random.uniform(min_x[idx], max_x[idx], 1), decimals)\n",
    "                    )\n",
    "            else:\n",
    "                x = list(np.round(np.random.uniform(min_x, max_x, n_vars), decimals))\n",
    "            assert (\n",
    "                len(x) != 0\n",
    "            ), \"For some reason, we didn't generate the points correctly!\"\n",
    "        else:\n",
    "            x = supportPoints[p]\n",
    "\n",
    "        tmpEq = eq + \"\"\n",
    "        for nVID in range(n_vars):\n",
    "            tmpEq = tmpEq.replace(\"x{}\".format(nVID + 1), str(x[nVID]))\n",
    "        y = float(np.round(eval(tmpEq), decimals))\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# def processDataFiles(files):\n",
    "#     text = \"\"\n",
    "#     for f in tqdm(files):\n",
    "#         with open(f, 'r') as h:\n",
    "#             lines = h.read() # don't worry we won't run out of file handles\n",
    "#             if lines[-1]==-1:\n",
    "#                 lines = lines[:-1]\n",
    "#             #text += lines #json.loads(line)\n",
    "#             text = ''.join([lines,text])\n",
    "#     return text\n",
    "\n",
    "\n",
    "def processDataFiles(files):\n",
    "    text = \"\"\n",
    "    for f in files:\n",
    "        with open(f, \"r\") as h:\n",
    "            lines = h.read()  # don't worry we won't run out of file handles\n",
    "            if lines[-1] == -1:\n",
    "                lines = lines[:-1]\n",
    "            # text += lines #json.loads(line)\n",
    "            text = \"\".join([lines, text])\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize_equation(eq):\n",
    "    token_spec = [\n",
    "        (r\"\\*\\*\"),  # exponentiation\n",
    "        (r\"exp\"),  # exp function\n",
    "        (r\"[+\\-*/=()]\"),  # operators and parentheses\n",
    "        (r\"sin\"),  # sin function\n",
    "        (r\"cos\"),  # cos function\n",
    "        (r\"log\"),  # log function\n",
    "        (r\"x\\d+\"),  # variables like x1, x23, etc.\n",
    "        (r\"C\"),  # constants placeholder\n",
    "        (r\"-?\\d+\\.\\d+\"),  # decimal numbers\n",
    "        (r\"-?\\d+\"),  # integers\n",
    "        (r\"_\"),  # padding token\n",
    "    ]\n",
    "    token_regex = \"|\".join(f\"({pattern})\" for pattern in token_spec)\n",
    "    matches = re.finditer(token_regex, eq)\n",
    "    return [match.group(0) for match in matches]\n",
    "\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        block_size,\n",
    "        tokens,\n",
    "        numVars,\n",
    "        numYs,\n",
    "        numPoints,\n",
    "        target=\"Skeleton\",\n",
    "        addVars=False,\n",
    "        const_range=[-0.4, 0.4],\n",
    "        xRange=[-3.0, 3.0],\n",
    "        decimals=4,\n",
    "        augment=False,\n",
    "    ):\n",
    "\n",
    "        data_size, vocab_size = len(data), len(tokens)\n",
    "        print(\"data has %d examples, %d unique.\" % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = {tok: i for i, tok in enumerate(tokens)}\n",
    "        self.itos = {i: tok for i, tok in enumerate(tokens)}\n",
    "\n",
    "        self.numVars = numVars\n",
    "        self.numYs = numYs\n",
    "        self.numPoints = numPoints\n",
    "\n",
    "        # padding token\n",
    "        self.paddingToken = \"_\"\n",
    "        self.paddingID = self.stoi[\"_\"]  # or another ID not already used\n",
    "        self.stoi[self.paddingToken] = self.paddingID\n",
    "        self.itos[self.paddingID] = self.paddingToken\n",
    "\n",
    "        self.threshold = [-1000, 1000]\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data  # it should be a list of examples\n",
    "        self.target = target\n",
    "        self.addVars = addVars\n",
    "\n",
    "        self.const_range = const_range\n",
    "        self.xRange = xRange\n",
    "        self.decimals = decimals\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab an example from the data\n",
    "        chunk = self.data[idx]  # sequence of tokens including x, y, eq, etc.\n",
    "\n",
    "        try:\n",
    "            chunk = json.loads(chunk)  # convert the sequence tokens to a dictionary\n",
    "        except Exception as e:\n",
    "            print(\"Couldn't convert to json: {} \\n error is: {}\".format(chunk, e))\n",
    "            # try the previous example\n",
    "            idx = idx - 1\n",
    "            idx = idx if idx >= 0 else 0\n",
    "            chunk = self.data[idx]\n",
    "            chunk = json.loads(chunk)  # convert the sequence tokens to a dictionary\n",
    "\n",
    "        # find the number of variables in the equation\n",
    "        printInfoCondition = random.random() < 0.0000001\n",
    "        eq = chunk[self.target]\n",
    "        if printInfoCondition:\n",
    "            print(f\"\\nEquation: {eq}\")\n",
    "        vars = re.finditer(\"x[\\d]+\", eq)\n",
    "        numVars = 0\n",
    "        for v in vars:\n",
    "            v = v.group(0).strip(\"x\")\n",
    "            v = eval(v)\n",
    "            v = int(v)\n",
    "            if v > numVars:\n",
    "                numVars = v\n",
    "\n",
    "        if self.target == \"Skeleton\" and self.augment:\n",
    "            threshold = 5000\n",
    "            # randomly generate the constants\n",
    "            cleanEqn = \"\"\n",
    "            for chr in eq:\n",
    "                if chr == \"C\":\n",
    "                    # genereate a new random number\n",
    "                    chr = \"{}\".format(\n",
    "                        np.random.uniform(self.const_range[0], self.const_range[1])\n",
    "                    )\n",
    "                cleanEqn += chr\n",
    "\n",
    "            # update the points\n",
    "            nPoints = np.random.randint(\n",
    "                *self.numPoints\n",
    "            )  # if supportPoints is None else len(supportPoints)\n",
    "            try:\n",
    "                if printInfoCondition:\n",
    "                    print(\"Org:\", chunk[\"X\"], chunk[\"Y\"])\n",
    "\n",
    "                X, y = generateDataStrEq(\n",
    "                    cleanEqn,\n",
    "                    n_points=nPoints,\n",
    "                    n_vars=self.numVars,\n",
    "                    decimals=self.decimals,\n",
    "                    min_x=self.xRange[0],\n",
    "                    max_x=self.xRange[1],\n",
    "                )\n",
    "\n",
    "                # replace out of threshold with maximum numbers\n",
    "                y = [e if abs(e) < threshold else np.sign(e) * threshold for e in y]\n",
    "\n",
    "                # check if there is nan/inf/very large numbers in the y\n",
    "                conditions = (\n",
    "                    (np.isnan(y).any() or np.isinf(y).any())\n",
    "                    or len(y) == 0\n",
    "                    or (abs(min(y)) > threshold or abs(max(y)) > threshold)\n",
    "                )\n",
    "                if not conditions:\n",
    "                    chunk[\"X\"], chunk[\"Y\"] = X, y\n",
    "\n",
    "                if printInfoCondition:\n",
    "                    print(\"Evd:\", chunk[\"X\"], chunk[\"Y\"])\n",
    "            except Exception as e:\n",
    "                # for different reason this might happend including but not limited to division by zero\n",
    "                print(\n",
    "                    \"\".join(\n",
    "                        [\n",
    "                            f\"We just used the original equation and support points because of {e}. \",\n",
    "                            f\"The equation is {eq}, and we update the equation to {cleanEqn}\",\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # encode every character in the equation to an integer\n",
    "        # < is SOS, > is EOS\n",
    "        if self.addVars:\n",
    "            dix = [self.stoi[s] for s in \"<\" + str(numVars) + \":\" + eq + \">\"]\n",
    "        else:\n",
    "            eq_tokens = tokenize_equation(eq)\n",
    "            if self.addVars:\n",
    "                token_seq = [\"<\", str(numVars), \":\", *eq_tokens, \">\"]\n",
    "            else:\n",
    "                token_seq = [\"<\", *eq_tokens, \">\"]\n",
    "            dix = [self.stoi[tok] for tok in token_seq]\n",
    "\n",
    "        inputs = dix[:-1]\n",
    "        outputs = dix[1:]\n",
    "\n",
    "        # add the padding to the equations\n",
    "        paddingSize = max(self.block_size - len(inputs), 0)\n",
    "        paddingList = [self.paddingID] * paddingSize\n",
    "        inputs += paddingList\n",
    "        outputs += paddingList\n",
    "\n",
    "        # make sure it is not more than what should be\n",
    "        inputs = inputs[: self.block_size]\n",
    "        outputs = outputs[: self.block_size]\n",
    "\n",
    "        points = torch.zeros(self.numVars + self.numYs, self.numPoints - 1)\n",
    "        for idx, xy in enumerate(zip(chunk[\"X\"], chunk[\"Y\"])):\n",
    "\n",
    "            if not isinstance(xy[0], list) or not isinstance(\n",
    "                xy[1], (list, float, np.float64)\n",
    "            ):\n",
    "                print(f\"Unexpected types: {type(xy[0])}, {type(xy[1])}\")\n",
    "                continue  # Skip if types are incorrect\n",
    "\n",
    "            # don't let to exceed the maximum number of points\n",
    "            if idx >= self.numPoints - 1:\n",
    "                break\n",
    "\n",
    "            x = xy[0]\n",
    "            x = x + [0] * (max(self.numVars - len(x), 0))  # padding\n",
    "\n",
    "            y = [xy[1]] if type(xy[1]) == float or type(xy[1]) == np.float64 else xy[1]\n",
    "\n",
    "            y = y + [0] * (max(self.numYs - len(y), 0))  # padding\n",
    "            p = x + y  # because it is only one point\n",
    "            p = torch.tensor(p)\n",
    "            # replace nan and inf\n",
    "            p = torch.nan_to_num(\n",
    "                p,\n",
    "                nan=self.threshold[1],\n",
    "                posinf=self.threshold[1],\n",
    "                neginf=self.threshold[0],\n",
    "            )\n",
    "\n",
    "            points[:, idx] = p\n",
    "\n",
    "        points = torch.nan_to_num(\n",
    "            points,\n",
    "            nan=self.threshold[1],\n",
    "            posinf=self.threshold[1],\n",
    "            neginf=self.threshold[0],\n",
    "        )\n",
    "\n",
    "        inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "        outputs = torch.tensor(outputs, dtype=torch.long)\n",
    "        numVars = torch.tensor(numVars, dtype=torch.long)\n",
    "        return inputs, outputs, points, numVars\n",
    "\n",
    "\n",
    "# Relative Mean Square Error\n",
    "def relativeErr(y, yHat, info=False, eps=1e-5):\n",
    "    yHat = np.reshape(yHat, [1, -1])[0]\n",
    "    y = np.reshape(y, [1, -1])[0]\n",
    "    if len(y) > 0 and len(y) == len(yHat):\n",
    "        err = ((yHat - y)) ** 2 / np.linalg.norm(y + eps)\n",
    "        if info:\n",
    "            for _ in range(5):\n",
    "                i = np.random.randint(len(y))\n",
    "                # print(\"yPR,yTrue:{},{}, Err:{}\".format(yHat[i], y[i], err[i]))\n",
    "    else:\n",
    "        err = 100\n",
    "\n",
    "    return np.mean(err)\n",
    "\n",
    "\n",
    "def lossFunc(constants, eq, X, Y, eps=1e-5):\n",
    "    err = 0\n",
    "    eq = eq.replace(\"C\", \"{}\").format(*constants)\n",
    "\n",
    "    for x, y in zip(X, Y):\n",
    "        eqTemp = eq + \"\"\n",
    "        if type(x) == np.float32:\n",
    "            x = [x]\n",
    "        for i, e in enumerate(x):\n",
    "            # make sure e is not a tensor\n",
    "            if type(e) == torch.Tensor:\n",
    "                e = e.item()\n",
    "            eqTemp = eqTemp.replace(\"x{}\".format(i + 1), str(e))\n",
    "        try:\n",
    "            yHat = eval(eqTemp)\n",
    "        except:\n",
    "            # print(\"Exception has been occured! EQ: {}, OR: {}\".format(eqTemp, eq))\n",
    "            yHat = 100\n",
    "        try:\n",
    "            # handle overflow\n",
    "            err += relativeErr(y, yHat)  # (y-yHat)**2\n",
    "        except:\n",
    "            # print(\n",
    "            #    \"Exception has been occured! EQ: {}, OR: {}, y:{}-yHat:{}\".format(\n",
    "            #        eqTemp, eq, y, yHat\n",
    "            #    )\n",
    "            # )\n",
    "            err += 10\n",
    "\n",
    "    err /= len(Y)\n",
    "    return err\n",
    "\n",
    "\n",
    "def get_skeleton(tokens, train_dataset: CharDataset):\n",
    "    skeleton = \"\".join([train_dataset.itos[int(idx)] for idx in tokens])\n",
    "    skeleton = skeleton.strip(train_dataset.paddingToken).split(\">\")\n",
    "    skeleton = skeleton[0] if len(skeleton[0]) >= 1 else skeleton[1]\n",
    "    skeleton = skeleton.strip(\"<\").strip(\">\")\n",
    "    skeleton = skeleton.replace(\"Ce\", \"C*e\")\n",
    "\n",
    "    return skeleton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92f71320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:01:42.002683Z",
     "iopub.status.busy": "2025-04-12T23:01:42.002285Z",
     "iopub.status.idle": "2025-04-12T23:01:42.122928Z",
     "shell.execute_reply": "2025-04-12T23:01:42.122256Z"
    },
    "papermill": {
     "duration": 0.125989,
     "end_time": "2025-04-12T23:01:42.124159",
     "exception": false,
     "start_time": "2025-04-12T23:01:41.998170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "# from SymbolicGPT: https://github.com/mojivalipour/symbolicgpt/blob/master/models.py\n",
    "class PointNetConfig:\n",
    "    \"\"\"base PointNet config\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddingSize,\n",
    "        numberofPoints,\n",
    "        numberofVars,\n",
    "        numberofYs,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.embeddingSize = embeddingSize\n",
    "        self.numberofPoints = numberofPoints  # number of points\n",
    "        self.numberofVars = numberofVars  # input dimension (Xs)\n",
    "        self.numberofYs = numberofYs  # output dimension (Ys)\n",
    "\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "\n",
    "class tNet(nn.Module):\n",
    "    \"\"\"\n",
    "    The PointNet structure in the orginal PointNet paper:\n",
    "    PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation by Qi et. al. 2017\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(tNet, self).__init__()\n",
    "\n",
    "        self.activation_func = F.relu\n",
    "        self.num_units = config.embeddingSize\n",
    "\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            config.numberofVars + config.numberofYs, self.num_units, 1\n",
    "        )\n",
    "        self.conv2 = nn.Conv1d(self.num_units, 2 * self.num_units, 1)\n",
    "        self.conv3 = nn.Conv1d(2 * self.num_units, 4 * self.num_units, 1)\n",
    "        self.fc1 = nn.Linear(4 * self.num_units, 2 * self.num_units)\n",
    "        self.fc2 = nn.Linear(2 * self.num_units, self.num_units)\n",
    "\n",
    "        # self.relu = nn.ReLU()\n",
    "\n",
    "        self.input_batch_norm = nn.BatchNorm1d(config.numberofVars + config.numberofYs)\n",
    "        # self.input_layer_norm = nn.LayerNorm(config.numberofPoints)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(self.num_units)\n",
    "        self.bn2 = nn.BatchNorm1d(2 * self.num_units)\n",
    "        self.bn3 = nn.BatchNorm1d(4 * self.num_units)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * self.num_units)\n",
    "        self.bn5 = nn.BatchNorm1d(self.num_units)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch, #features, #points]\n",
    "        :return:\n",
    "            logit: [batch, embedding_size]\n",
    "        \"\"\"\n",
    "        x = self.input_batch_norm(x)\n",
    "        x = self.activation_func(self.bn1(self.conv1(x)))\n",
    "        x = self.activation_func(self.bn2(self.conv2(x)))\n",
    "        x = self.activation_func(self.bn3(self.conv3(x)))\n",
    "        x, _ = torch.max(x, dim=2)  # global max pooling\n",
    "        assert x.size(1) == 4 * self.num_units\n",
    "\n",
    "        x = self.activation_func(self.bn4(self.fc1(x)))\n",
    "        x = self.activation_func(self.bn5(self.fc2(x)))\n",
    "        # x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# from https://github.com/juho-lee/set_transformer/blob/master/modules.py\n",
    "class MAB(nn.Module):\n",
    "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False):\n",
    "        super(MAB, self).__init__()\n",
    "        self.dim_V = dim_V\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
    "        self.fc_k = nn.Linear(dim_K, dim_V)\n",
    "        self.fc_v = nn.Linear(dim_K, dim_V)\n",
    "        if ln:\n",
    "            self.ln0 = nn.LayerNorm(dim_V)\n",
    "            self.ln1 = nn.LayerNorm(dim_V)\n",
    "        self.fc_o = nn.Linear(dim_V, dim_V)\n",
    "\n",
    "    def forward(self, Q, K):\n",
    "        Q = self.fc_q(Q)\n",
    "        K, V = self.fc_k(K), self.fc_v(K)\n",
    "\n",
    "        dim_split = self.dim_V // self.num_heads\n",
    "        Q_ = torch.cat(Q.split(dim_split, 2), 0)\n",
    "        K_ = torch.cat(K.split(dim_split, 2), 0)\n",
    "        V_ = torch.cat(V.split(dim_split, 2), 0)\n",
    "\n",
    "        A = torch.softmax(Q_.bmm(K_.transpose(1, 2)) / math.sqrt(self.dim_V), 2)\n",
    "        O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), 0), 2)\n",
    "        O = O if getattr(self, \"ln0\", None) is None else self.ln0(O)\n",
    "        O = O + F.relu(self.fc_o(O))\n",
    "        O = O if getattr(self, \"ln1\", None) is None else self.ln1(O)\n",
    "        return O\n",
    "\n",
    "\n",
    "class SAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, ln=False):\n",
    "        super(SAB, self).__init__()\n",
    "        self.mab = MAB(dim_in, dim_in, dim_out, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.mab(X, X)\n",
    "\n",
    "\n",
    "class ISAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, num_inds, ln=False):\n",
    "        super(ISAB, self).__init__()\n",
    "        self.I = nn.Parameter(torch.Tensor(1, num_inds, dim_out))\n",
    "        nn.init.xavier_uniform_(self.I)\n",
    "        self.mab0 = MAB(dim_out, dim_in, dim_out, num_heads, ln=ln)\n",
    "        self.mab1 = MAB(dim_in, dim_out, dim_out, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        H = self.mab0(self.I.repeat(X.size(0), 1, 1), X)\n",
    "        return self.mab1(X, H)\n",
    "\n",
    "\n",
    "class PMA(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_seeds, ln=False):\n",
    "        super(PMA, self).__init__()\n",
    "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
    "        nn.init.xavier_uniform_(self.S)\n",
    "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.mab(self.S.repeat(X.size(0), 1, 1), X)\n",
    "\n",
    "\n",
    "# from https://github.com/juho-lee/set_transformer/blob/master/models.py\n",
    "class SetTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_input,\n",
    "        num_outputs,\n",
    "        dim_output,\n",
    "        num_inds=32,\n",
    "        dim_hidden=128,\n",
    "        num_heads=4,\n",
    "        ln=False,\n",
    "    ):\n",
    "        super(SetTransformer, self).__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            ISAB(dim_input, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "            ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            PMA(dim_hidden, num_heads, num_outputs, ln=ln),\n",
    "            SAB(dim_hidden, dim_hidden, num_heads, ln=ln),\n",
    "            SAB(dim_hidden, dim_hidden, num_heads, ln=ln),\n",
    "            nn.Linear(dim_hidden, dim_output),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dec(self.enc(X)).squeeze(1)\n",
    "\n",
    "\n",
    "class NoisePredictionTransformer(nn.Module):\n",
    "    def __init__(self, n_embd, max_seq_len, n_layer=6, n_head=8, max_timesteps=1000):\n",
    "        super().__init__()\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, max_seq_len, n_embd))\n",
    "        self.time_emb = nn.Embedding(max_timesteps, n_embd)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=n_embd,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=n_embd * 4,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layer)\n",
    "\n",
    "    def forward(self, x_t, t, condition):\n",
    "        _, L, _ = x_t.shape\n",
    "        pos_emb = self.pos_emb[:, :L, :]  # [1, L, n_embd]\n",
    "        time_emb = self.time_emb(t)\n",
    "        if time_emb.dim() == 1:  # Scalar t case, [n_embd]\n",
    "            time_emb = time_emb.unsqueeze(0)  # [1, n_embd]\n",
    "        time_emb = time_emb.unsqueeze(1)  # [1, 1, n_embd]\n",
    "        condition = condition.unsqueeze(1)  # [B, 1, n_embd]\n",
    "\n",
    "        x = x_t + pos_emb + time_emb + condition\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "# influenced by https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/simple_diffusion.py\n",
    "class SymbolicGaussianDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tnet_config: PointNetConfig,\n",
    "        vocab_size,\n",
    "        max_seq_len,\n",
    "        padding_idx: int = 0,\n",
    "        max_num_vars: int = 9,\n",
    "        n_layer=6,\n",
    "        n_head=8,\n",
    "        n_embd=512,\n",
    "        timesteps=1000,\n",
    "        beta_start=0.0001,\n",
    "        beta_end=0.02,\n",
    "        set_transformer=True,\n",
    "        ce_weight=1.0,  \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.padding_idx = padding_idx\n",
    "        self.n_embd = n_embd\n",
    "        self.timesteps = timesteps\n",
    "        self.set_transformer = set_transformer\n",
    "        self.ce_weight = ce_weight\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd, padding_idx=self.padding_idx)\n",
    "        self.vars_emb = nn.Embedding(max_num_vars, n_embd)\n",
    "\n",
    "        self.decoder = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        self.decoder.weight = self.tok_emb.weight\n",
    "\n",
    "        if set_transformer:\n",
    "            dim_input = tnet_config.numberofVars + tnet_config.numberofYs\n",
    "            self.tnet = SetTransformer(\n",
    "                dim_input=dim_input,\n",
    "                num_outputs=1,\n",
    "                dim_output=tnet_config.embeddingSize,\n",
    "                num_inds=tnet_config.numberofPoints,\n",
    "                num_heads=4,\n",
    "            )\n",
    "        else:\n",
    "            self.tnet = tNet(tnet_config)\n",
    "\n",
    "        self.model = NoisePredictionTransformer(\n",
    "            n_embd, max_seq_len, n_layer, n_head, timesteps\n",
    "        )\n",
    "\n",
    "        # Noise schedule\n",
    "        self.register_buffer(\"beta\", torch.linspace(beta_start, beta_end, timesteps))\n",
    "        self.register_buffer(\"alpha\", 1.0 - self.beta)\n",
    "        self.register_buffer(\"alpha_bar\", torch.cumprod(self.alpha, dim=0))\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        noise = torch.randn_like(x_start)\n",
    "        sqrt_alpha_bar = torch.sqrt(self.alpha_bar[t]).view(-1, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bar[t]).view(-1, 1, 1)\n",
    "\n",
    "        x_t = sqrt_alpha_bar * x_start + sqrt_one_minus_alpha_bar * noise\n",
    "        return x_t\n",
    "\n",
    "    def p_mean_variance(self, x, t, t_next, condition):\n",
    "        alpha_t = self.alpha[t]\n",
    "        alpha_bar_t = self.alpha_bar[t]\n",
    "        alpha_bar_t_next = self.alpha_bar[t_next]\n",
    "        beta_t = self.beta[t]\n",
    "\n",
    "        x_start_pred = self.model(x, t.long(), condition)\n",
    "\n",
    "        coeff1 = torch.sqrt(alpha_bar_t_next) * beta_t / (1 - alpha_bar_t)\n",
    "        coeff2 = torch.sqrt(alpha_t) * (1 - alpha_bar_t_next) / (1 - alpha_bar_t)\n",
    "        mean = coeff1 * x_start_pred + coeff2 * x\n",
    "        variance = (1 - alpha_bar_t_next) / (1 - alpha_bar_t) * beta_t\n",
    "        return mean, variance\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t, t_next, condition):\n",
    "        mean, variance = self.p_mean_variance(x, t, t_next, condition)\n",
    "        if torch.all(t_next == 0):\n",
    "            return mean\n",
    "        noise = torch.randn_like(x)\n",
    "        return mean + torch.sqrt(variance) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, points, variables, train_dataset, batch_size=16, ddim_step=1):\n",
    "        if self.set_transformer:\n",
    "            points = points.transpose(1, 2)\n",
    "\n",
    "        condition = self.tnet(points) + self.vars_emb(variables)\n",
    "        shape = (batch_size, self.max_seq_len, self.n_embd)\n",
    "        x = torch.randn(shape, device=self.device)\n",
    "        steps = torch.arange(self.timesteps - 1, -1, -1, device=self.device)\n",
    "\n",
    "        for i in range(0, self.timesteps, ddim_step):\n",
    "            t = steps[i]\n",
    "            t_next = (\n",
    "                steps[i + ddim_step]\n",
    "                if i + ddim_step < self.timesteps\n",
    "                else torch.tensor(0, device=self.device)\n",
    "            )\n",
    "            x = self.p_sample(x, t, t_next, condition)\n",
    "\n",
    "            # Print prediction every 250 steps\n",
    "            # if (i + 1) % 250 == 0:\n",
    "            #    logits = self.decoder(x)  # [B, L, vocab_size]\n",
    "            #    token_indices = torch.argmax(logits, dim=-1)  # [B, L]\n",
    "            #    for j in range(batch_size):\n",
    "            #       token_indices_j = token_indices[j]  # [L]\n",
    "            #        predicted_skeleton = get_predicted_skeleton(\n",
    "            #            token_indices_j, train_dataset\n",
    "            #        )\n",
    "            #        tqdm.write(f\" sample {j}: predicted_skeleton: {predicted_skeleton}\")\n",
    "\n",
    "        logits = self.decoder(x)  # [B, L, vocab_size]\n",
    "        token_indices = torch.argmax(logits, dim=-1)  # [B, L]\n",
    "        predicted_skeletons = []\n",
    "        for j in range(batch_size):\n",
    "            token_indices_j = token_indices[j]  # [L]\n",
    "            predicted_skeleton = get_predicted_skeleton(token_indices_j, train_dataset)\n",
    "            predicted_skeletons.append(predicted_skeleton)\n",
    "        return predicted_skeletons\n",
    "\n",
    "    def p_losses(\n",
    "        self,\n",
    "        x_start,\n",
    "        points,\n",
    "        tokens,\n",
    "        variables,\n",
    "        t,\n",
    "    ):\n",
    "        noise = torch.randn_like(x_start)\n",
    "        x_t = self.q_sample(x_start, t, noise)\n",
    "\n",
    "        if self.set_transformer:\n",
    "            points = points.transpose(1, 2)\n",
    "\n",
    "        condition = self.tnet(points) + self.vars_emb(variables)\n",
    "\n",
    "        x_start_pred = self.model(x_t, t.long(), condition)\n",
    "\n",
    "        # CE loss on tokens\n",
    "        logits = self.decoder(x_start_pred)  # [B, L, vocab_size]\n",
    "\n",
    "        ce_loss = F.cross_entropy(\n",
    "            logits.view(-1, self.vocab_size),  # [B*L, vocab_size]\n",
    "            tokens.view(-1),  # [B*L]\n",
    "            ignore_index=self.padding_idx,\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "\n",
    "        ce_loss = ce_loss * self.ce_weight\n",
    "\n",
    "        return ce_loss\n",
    "\n",
    "    def forward(self, points, tokens, variables, t):\n",
    "        token_emb = self.tok_emb(tokens)\n",
    "        ce_loss = self.p_losses(token_emb, points, tokens, variables, t)\n",
    "        return ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c823f27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:01:42.131417Z",
     "iopub.status.busy": "2025-04-12T23:01:42.131192Z",
     "iopub.status.idle": "2025-04-12T23:01:42.142424Z",
     "shell.execute_reply": "2025-04-12T23:01:42.141794Z"
    },
    "papermill": {
     "duration": 0.015857,
     "end_time": "2025-04-12T23:01:42.143512",
     "exception": false,
     "start_time": "2025-04-12T23:01:42.127655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import tqdm\n",
    "from typing import Tuple\n",
    "\n",
    "def train_epoch(\n",
    "    model: SymbolicGaussianDiffusion,\n",
    "    train_loader: DataLoader,\n",
    "    optimizer: Adam,\n",
    "    train_dataset: CharDataset,\n",
    "    timesteps: int,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    num_epochs: int,\n",
    ") -> Tuple[float, float, float]:\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for i, (_, tokens, points, variables) in tqdm.tqdm(\n",
    "        enumerate(train_loader),\n",
    "        total=len(train_loader),\n",
    "        desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
    "    ):\n",
    "        points, tokens, variables = (\n",
    "            points.to(device),\n",
    "            tokens.to(device),\n",
    "            variables.to(device),\n",
    "        )\n",
    "        t = torch.randint(0, timesteps, (tokens.shape[0],), device=device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss = model(points, tokens, variables, t)\n",
    "\n",
    "        if (i + 1) % 250 == 0:\n",
    "            print(f\"Batch {i + 1}/{len(train_loader)}:\")\n",
    "            print(f\"total_loss: {total_loss}\")\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += total_loss.item()\n",
    "\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    return avg_train_loss\n",
    "\n",
    "\n",
    "def val_epoch(\n",
    "    model: SymbolicGaussianDiffusion,\n",
    "    val_loader: DataLoader,\n",
    "    train_dataset: CharDataset,\n",
    "    timesteps: int,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    num_epochs: int,\n",
    ") -> Tuple[float, float, float]:\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, tokens, points, variables in tqdm.tqdm(\n",
    "            val_loader, total=len(val_loader), desc=\"Validating\"\n",
    "        ):\n",
    "            points, tokens, variables = (\n",
    "                points.to(device),\n",
    "                tokens.to(device),\n",
    "                variables.to(device),\n",
    "            )\n",
    "            t = torch.randint(0, timesteps, (tokens.shape[0],), device=device)\n",
    "            total_loss = model(points, tokens, variables, t)\n",
    "\n",
    "            total_val_loss += total_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    return avg_val_loss\n",
    "\n",
    "\n",
    "def train_single_gpu(\n",
    "    model: SymbolicGaussianDiffusion,\n",
    "    train_dataset: CharDataset,\n",
    "    val_dataset: CharDataset,\n",
    "    num_epochs=10,\n",
    "    save_every=2,\n",
    "    batch_size=32,\n",
    "    timesteps=1000,\n",
    "    learning_rate=1e-3,\n",
    "    path=None,\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_train_loss = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            train_dataset,\n",
    "            timesteps,\n",
    "            device,\n",
    "            epoch,\n",
    "            num_epochs,\n",
    "        )\n",
    "\n",
    "        avg_val_loss = val_epoch(\n",
    "            model, val_loader, train_dataset, timesteps, device, epoch, num_epochs\n",
    "        )\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        print(\"\\nEpoch Summary:\")\n",
    "        print(\n",
    "            f\"Train Total Loss: {avg_train_loss:.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Val Total Loss: {avg_val_loss:.4f}\"\n",
    "        )\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            state_dict = model.state_dict()\n",
    "            torch.save(state_dict, path)\n",
    "            print(f\"New best model saved with val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a66c8af8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:01:42.149940Z",
     "iopub.status.busy": "2025-04-12T23:01:42.149717Z",
     "iopub.status.idle": "2025-04-12T23:01:42.228443Z",
     "shell.execute_reply": "2025-04-12T23:01:42.227555Z"
    },
    "papermill": {
     "duration": 0.083492,
     "end_time": "2025-04-12T23:01:42.229801",
     "exception": false,
     "start_time": "2025-04-12T23:01:42.146309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_embd = 512\n",
    "timesteps = 1000\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 5\n",
    "blockSize = 32\n",
    "numVars = 2\n",
    "numYs = 1\n",
    "numPoints = 250\n",
    "target = 'Skeleton'\n",
    "const_range = [-2.1, 2.1]\n",
    "trainRange = [-3.0, 3.0]\n",
    "decimals = 8\n",
    "addVars = False\n",
    "maxNumFiles = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ebeb85a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:01:42.236689Z",
     "iopub.status.busy": "2025-04-12T23:01:42.236446Z",
     "iopub.status.idle": "2025-04-12T23:01:42.239350Z",
     "shell.execute_reply": "2025-04-12T23:01:42.238742Z"
    },
    "papermill": {
     "duration": 0.007517,
     "end_time": "2025-04-12T23:01:42.240476",
     "exception": false,
     "start_time": "2025-04-12T23:01:42.232959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataDir = \"/kaggle/input/2-var-dataset\"\n",
    "dataFolder = \"2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a4138a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:01:42.246906Z",
     "iopub.status.busy": "2025-04-12T23:01:42.246684Z",
     "iopub.status.idle": "2025-04-12T23:03:52.882416Z",
     "shell.execute_reply": "2025-04-12T23:03:52.881501Z"
    },
    "papermill": {
     "duration": 130.642852,
     "end_time": "2025-04-12T23:03:52.886301",
     "exception": false,
     "start_time": "2025-04-12T23:01:42.243449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 499035 examples, 30 unique.\n",
      "id:350555\n",
      "outputs:C*x1*x2+C>________________________\n",
      "variables:2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "\n",
    "path = '{}/{}/Train/*.json'.format(dataDir, dataFolder)\n",
    "files = glob.glob(path)[:maxNumFiles]\n",
    "text = processDataFiles(files)\n",
    "text = text.split('\\n') # convert the raw text to a set of examples\n",
    "# skeletons = []\n",
    "skeletons = [json.loads(item)['Skeleton'] for item in text if item.strip()]\n",
    "all_tokens = set()\n",
    "for eq in skeletons:\n",
    "    all_tokens.update(tokenize_equation(eq))\n",
    "integers = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9'}\n",
    "all_tokens.update(integers)  # add all integers to the token set\n",
    "tokens = sorted(list(all_tokens) + ['_', 'T', '<', '>', ':'])  # special tokens\n",
    "trainText = text[:-1] if len(text[-1]) == 0 else text\n",
    "random.shuffle(trainText) # shuffle the dataset, it's important specailly for the combined number of variables experiment\n",
    "train_dataset = CharDataset(trainText, blockSize, tokens=tokens, numVars=numVars,\n",
    "                        numYs=numYs, numPoints=numPoints, target=target, addVars=addVars,\n",
    "                        const_range=const_range, xRange=trainRange, decimals=decimals)\n",
    "\n",
    "idx = np.random.randint(train_dataset.__len__())\n",
    "inputs, outputs, points, variables = train_dataset.__getitem__(idx)\n",
    "inputs = ''.join([train_dataset.itos[int(i)] for i in inputs])\n",
    "outputs = ''.join([train_dataset.itos[int(i)] for i in outputs])\n",
    "print('id:{}\\noutputs:{}\\nvariables:{}'.format(idx,outputs,variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22361efc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:03:52.893405Z",
     "iopub.status.busy": "2025-04-12T23:03:52.893133Z",
     "iopub.status.idle": "2025-04-12T23:03:53.344523Z",
     "shell.execute_reply": "2025-04-12T23:03:53.343586Z"
    },
    "papermill": {
     "duration": 0.456379,
     "end_time": "2025-04-12T23:03:53.345892",
     "exception": false,
     "start_time": "2025-04-12T23:03:52.889513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 949 examples, 30 unique.\n",
      "tensor(-9.1742) tensor(7.4559)\n",
      "id:392\n",
      "outputs:C*log(C*x2**4)+C>___________________\n",
      "variables:2\n"
     ]
    }
   ],
   "source": [
    "path = '{}/{}/Val/*.json'.format(dataDir,dataFolder)\n",
    "files = glob.glob(path)\n",
    "textVal = processDataFiles([files[0]])\n",
    "textVal = textVal.split('\\n') # convert the raw text to a set of examples\n",
    "val_dataset = CharDataset(textVal, blockSize, tokens=tokens, numVars=numVars,\n",
    "                        numYs=numYs, numPoints=numPoints, target=target, addVars=addVars,\n",
    "                        const_range=const_range, xRange=trainRange, decimals=decimals)\n",
    "\n",
    "# print a random sample\n",
    "idx = np.random.randint(val_dataset.__len__())\n",
    "inputs, outputs, points, variables = val_dataset.__getitem__(idx)\n",
    "print(points.min(), points.max())\n",
    "inputs = ''.join([train_dataset.itos[int(i)] for i in inputs])\n",
    "outputs = ''.join([train_dataset.itos[int(i)] for i in outputs])\n",
    "print('id:{}\\noutputs:{}\\nvariables:{}'.format(idx,outputs,variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e97bb5ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:03:53.353467Z",
     "iopub.status.busy": "2025-04-12T23:03:53.353167Z",
     "iopub.status.idle": "2025-04-13T00:50:45.031167Z",
     "shell.execute_reply": "2025-04-13T00:50:45.030064Z"
    },
    "papermill": {
     "duration": 6411.68306,
     "end_time": "2025-04-13T00:50:45.032495",
     "exception": false,
     "start_time": "2025-04-12T23:03:53.349435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   3%|         | 250/7798 [00:41<17:52,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/7798:\n",
      "total_loss: 2.0335240364074707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   6%|         | 501/7798 [01:21<17:41,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/7798:\n",
      "total_loss: 1.547560214996338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  10%|         | 750/7798 [02:02<18:01,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 750/7798:\n",
      "total_loss: 1.1066755056381226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  13%|        | 1001/7798 [02:43<14:58,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/7798:\n",
      "total_loss: 0.9404404759407043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  16%|        | 1250/7798 [03:23<14:53,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1250/7798:\n",
      "total_loss: 0.7399563789367676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  19%|        | 1500/7798 [04:03<13:32,  7.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500/7798:\n",
      "total_loss: 0.837090253829956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  22%|       | 1750/7798 [04:44<13:27,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1750/7798:\n",
      "total_loss: 0.7731381058692932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  26%|       | 2001/7798 [05:25<17:14,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/7798:\n",
      "total_loss: 0.5326101779937744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  29%|       | 2250/7798 [06:05<11:50,  7.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2250/7798:\n",
      "total_loss: 0.4178681969642639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  32%|      | 2501/7798 [06:46<14:08,  6.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500/7798:\n",
      "total_loss: 0.5567715764045715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  35%|      | 2751/7798 [07:26<11:48,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2750/7798:\n",
      "total_loss: 0.7982273101806641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  38%|      | 3001/7798 [08:07<14:43,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000/7798:\n",
      "total_loss: 0.7644145488739014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  42%|     | 3251/7798 [08:48<10:27,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3250/7798:\n",
      "total_loss: 0.6474948525428772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  45%|     | 3501/7798 [09:29<11:52,  6.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500/7798:\n",
      "total_loss: 0.5260339975357056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  48%|     | 3751/7798 [10:09<09:38,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3750/7798:\n",
      "total_loss: 0.5430008172988892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  51%|    | 4000/7798 [10:49<09:53,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000/7798:\n",
      "total_loss: 0.8586531281471252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  55%|    | 4250/7798 [11:30<10:42,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4250/7798:\n",
      "total_loss: 0.7277719974517822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  58%|    | 4501/7798 [12:12<07:41,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4500/7798:\n",
      "total_loss: 0.4649205803871155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  61%|    | 4751/7798 [12:52<07:32,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4750/7798:\n",
      "total_loss: 0.46515029668807983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  64%|   | 5001/7798 [13:33<07:40,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000/7798:\n",
      "total_loss: 0.5895129442214966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  67%|   | 5250/7798 [14:14<05:59,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5250/7798:\n",
      "total_loss: 0.500469982624054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  71%|   | 5501/7798 [14:55<06:06,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/7798:\n",
      "total_loss: 0.30700618028640747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  74%|  | 5750/7798 [15:35<04:22,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5750/7798:\n",
      "total_loss: 0.5489462614059448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  77%|  | 6000/7798 [16:15<04:00,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000/7798:\n",
      "total_loss: 0.530622661113739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  80%|  | 6251/7798 [16:56<03:31,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6250/7798:\n",
      "total_loss: 0.38226714730262756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  83%| | 6501/7798 [17:37<03:29,  6.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6500/7798:\n",
      "total_loss: 0.47762253880500793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  87%| | 6750/7798 [18:17<02:21,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6750/7798:\n",
      "total_loss: 0.5803002715110779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  90%| | 7001/7798 [18:57<02:00,  6.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000/7798:\n",
      "total_loss: 0.42485731840133667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  93%|| 7250/7798 [19:38<01:20,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7250/7798:\n",
      "total_loss: 0.39745453000068665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  96%|| 7501/7798 [20:19<00:41,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500/7798:\n",
      "total_loss: 0.5022364854812622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  99%|| 7750/7798 [20:59<00:06,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7750/7798:\n",
      "total_loss: 0.560180127620697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|| 7798/7798 [21:06<00:00,  6.15it/s]\n",
      "Validating: 100%|| 15/15 [00:02<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Total Loss: 0.7400\n",
      "Val Total Loss: 0.4422\n",
      "Learning Rate: 0.000100\n",
      "New best model saved with val loss: 0.4422\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:   3%|         | 251/7798 [00:41<18:37,  6.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/7798:\n",
      "total_loss: 0.5449181795120239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:   6%|         | 500/7798 [01:22<16:53,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/7798:\n",
      "total_loss: 0.35035261511802673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  10%|         | 751/7798 [02:03<16:17,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 750/7798:\n",
      "total_loss: 0.591854453086853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  13%|        | 1001/7798 [02:44<17:42,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/7798:\n",
      "total_loss: 0.4698711931705475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  16%|        | 1250/7798 [03:25<16:20,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1250/7798:\n",
      "total_loss: 0.4402425289154053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  19%|        | 1501/7798 [04:06<17:28,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500/7798:\n",
      "total_loss: 0.5707692503929138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  22%|       | 1750/7798 [04:46<13:48,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1750/7798:\n",
      "total_loss: 0.3927963972091675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  26%|       | 2001/7798 [05:27<18:23,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/7798:\n",
      "total_loss: 0.6947987675666809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  29%|       | 2251/7798 [06:08<14:23,  6.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2250/7798:\n",
      "total_loss: 0.472433865070343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  32%|      | 2501/7798 [06:48<14:19,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500/7798:\n",
      "total_loss: 0.5535083413124084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  35%|      | 2750/7798 [07:29<12:05,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2750/7798:\n",
      "total_loss: 0.425148069858551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  38%|      | 3001/7798 [08:10<13:49,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000/7798:\n",
      "total_loss: 0.36599400639533997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  42%|     | 3251/7798 [08:50<12:57,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3250/7798:\n",
      "total_loss: 0.553316056728363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  45%|     | 3500/7798 [09:31<09:50,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500/7798:\n",
      "total_loss: 0.46246427297592163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  48%|     | 3751/7798 [10:12<11:05,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3750/7798:\n",
      "total_loss: 0.4450017213821411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  51%|    | 4001/7798 [10:54<08:49,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000/7798:\n",
      "total_loss: 0.6202139258384705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  55%|    | 4250/7798 [11:34<08:24,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4250/7798:\n",
      "total_loss: 0.5373077392578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  58%|    | 4500/7798 [12:14<07:38,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4500/7798:\n",
      "total_loss: 0.47083380818367004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  61%|    | 4750/7798 [12:55<07:01,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4750/7798:\n",
      "total_loss: 0.6197736859321594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  64%|   | 5001/7798 [13:36<07:44,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000/7798:\n",
      "total_loss: 0.4390840530395508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  67%|   | 5251/7798 [14:17<07:12,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5250/7798:\n",
      "total_loss: 0.36223018169403076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  71%|   | 5501/7798 [14:57<05:03,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/7798:\n",
      "total_loss: 0.4483366012573242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  74%|  | 5751/7798 [15:38<06:09,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5750/7798:\n",
      "total_loss: 0.44039055705070496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  77%|  | 6000/7798 [16:19<04:29,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000/7798:\n",
      "total_loss: 0.3426644504070282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  80%|  | 6250/7798 [16:59<03:37,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6250/7798:\n",
      "total_loss: 0.3423997163772583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  83%| | 6501/7798 [17:40<03:21,  6.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6500/7798:\n",
      "total_loss: 0.288652241230011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  87%| | 6751/7798 [18:21<02:54,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6750/7798:\n",
      "total_loss: 0.23257243633270264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  90%| | 7001/7798 [19:02<01:51,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000/7798:\n",
      "total_loss: 0.4396607279777527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  93%|| 7251/7798 [19:42<01:14,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7250/7798:\n",
      "total_loss: 0.34946736693382263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  96%|| 7501/7798 [20:23<00:45,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500/7798:\n",
      "total_loss: 0.36176273226737976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  99%|| 7751/7798 [21:04<00:07,  6.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7750/7798:\n",
      "total_loss: 0.3833358585834503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|| 7798/7798 [21:12<00:00,  6.13it/s]\n",
      "Validating: 100%|| 15/15 [00:02<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Total Loss: 0.4486\n",
      "Val Total Loss: 0.4310\n",
      "Learning Rate: 0.000100\n",
      "New best model saved with val loss: 0.4310\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:   3%|         | 251/7798 [00:41<17:59,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/7798:\n",
      "total_loss: 0.30551233887672424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:   6%|         | 501/7798 [01:23<19:38,  6.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/7798:\n",
      "total_loss: 0.4113222062587738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  10%|         | 751/7798 [02:03<22:03,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 750/7798:\n",
      "total_loss: 0.3833097219467163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  13%|        | 1000/7798 [02:43<17:27,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/7798:\n",
      "total_loss: 0.5040563941001892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  16%|        | 1251/7798 [03:24<14:18,  7.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1250/7798:\n",
      "total_loss: 0.3143034875392914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  19%|        | 1501/7798 [04:05<16:09,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500/7798:\n",
      "total_loss: 0.38047292828559875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  22%|       | 1751/7798 [04:46<13:36,  7.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1750/7798:\n",
      "total_loss: 0.4023347795009613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  26%|       | 2001/7798 [05:26<16:16,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/7798:\n",
      "total_loss: 0.4671076238155365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  29%|       | 2250/7798 [06:07<12:56,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2250/7798:\n",
      "total_loss: 0.41450920701026917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  32%|      | 2500/7798 [06:48<12:35,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500/7798:\n",
      "total_loss: 0.4089534878730774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  35%|      | 2751/7798 [07:30<12:08,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2750/7798:\n",
      "total_loss: 0.46418002247810364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  38%|      | 3001/7798 [08:10<11:43,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000/7798:\n",
      "total_loss: 0.4119807779788971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  42%|     | 3250/7798 [08:51<11:24,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3250/7798:\n",
      "total_loss: 0.3868420124053955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  45%|     | 3501/7798 [09:32<13:30,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500/7798:\n",
      "total_loss: 0.29295986890792847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  48%|     | 3750/7798 [10:13<12:38,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3750/7798:\n",
      "total_loss: 0.35891595482826233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  51%|    | 4001/7798 [10:54<08:59,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000/7798:\n",
      "total_loss: 0.3743860721588135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  55%|    | 4251/7798 [11:35<08:53,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4250/7798:\n",
      "total_loss: 0.4815927743911743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  58%|    | 4500/7798 [12:16<08:25,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4500/7798:\n",
      "total_loss: 0.42890194058418274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  61%|    | 4751/7798 [12:56<07:41,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4750/7798:\n",
      "total_loss: 0.38309282064437866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  64%|   | 5000/7798 [13:36<06:23,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000/7798:\n",
      "total_loss: 0.3833862841129303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  67%|   | 5251/7798 [14:18<06:30,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5250/7798:\n",
      "total_loss: 0.2626541554927826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  71%|   | 5501/7798 [14:58<07:44,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/7798:\n",
      "total_loss: 0.13497336208820343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  74%|  | 5751/7798 [15:39<04:48,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5750/7798:\n",
      "total_loss: 0.23532883822917938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  77%|  | 6000/7798 [16:20<05:05,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000/7798:\n",
      "total_loss: 0.39767158031463623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  80%|  | 6251/7798 [17:02<03:40,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6250/7798:\n",
      "total_loss: 0.2688133716583252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  83%| | 6501/7798 [17:43<04:03,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6500/7798:\n",
      "total_loss: 0.21982479095458984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  87%| | 6750/7798 [18:23<02:25,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6750/7798:\n",
      "total_loss: 0.3711056709289551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  90%| | 7001/7798 [19:05<01:52,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000/7798:\n",
      "total_loss: 0.31796929240226746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  93%|| 7251/7798 [19:46<01:17,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7250/7798:\n",
      "total_loss: 0.30972743034362793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  96%|| 7501/7798 [20:27<00:48,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500/7798:\n",
      "total_loss: 0.14879363775253296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  99%|| 7751/7798 [21:08<00:06,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7750/7798:\n",
      "total_loss: 0.21964773535728455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|| 7798/7798 [21:16<00:00,  6.11it/s]\n",
      "Validating: 100%|| 15/15 [00:02<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Total Loss: 0.3784\n",
      "Val Total Loss: 0.3585\n",
      "Learning Rate: 0.000100\n",
      "New best model saved with val loss: 0.3585\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:   3%|         | 251/7798 [00:41<18:33,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/7798:\n",
      "total_loss: 0.27363669872283936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:   6%|         | 501/7798 [01:23<18:16,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/7798:\n",
      "total_loss: 0.2557045817375183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  10%|         | 751/7798 [02:04<15:39,  7.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 750/7798:\n",
      "total_loss: 0.1902540773153305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  13%|        | 1001/7798 [02:45<17:51,  6.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/7798:\n",
      "total_loss: 0.34499499201774597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  16%|        | 1250/7798 [03:26<17:57,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1250/7798:\n",
      "total_loss: 0.26976242661476135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  19%|        | 1501/7798 [04:08<16:05,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500/7798:\n",
      "total_loss: 0.281416654586792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  22%|       | 1751/7798 [04:49<16:55,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1750/7798:\n",
      "total_loss: 0.42971521615982056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  26%|       | 2000/7798 [05:29<13:24,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/7798:\n",
      "total_loss: 0.420315682888031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  29%|       | 2251/7798 [06:10<13:38,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2250/7798:\n",
      "total_loss: 0.2922974228858948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  32%|      | 2500/7798 [06:51<12:52,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500/7798:\n",
      "total_loss: 0.21899950504302979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  35%|      | 2751/7798 [07:32<11:14,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2750/7798:\n",
      "total_loss: 0.43271365761756897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  38%|      | 3001/7798 [08:13<10:54,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000/7798:\n",
      "total_loss: 0.3787152171134949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  42%|     | 3251/7798 [08:53<13:01,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3250/7798:\n",
      "total_loss: 0.4244782626628876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  45%|     | 3500/7798 [09:34<10:09,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500/7798:\n",
      "total_loss: 0.33892130851745605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  48%|     | 3751/7798 [10:16<09:03,  7.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3750/7798:\n",
      "total_loss: 0.4946500062942505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  51%|    | 4001/7798 [10:57<10:24,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000/7798:\n",
      "total_loss: 0.18454013764858246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  55%|    | 4251/7798 [11:39<10:23,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4250/7798:\n",
      "total_loss: 0.20388442277908325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  58%|    | 4501/7798 [12:20<07:52,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4500/7798:\n",
      "total_loss: 0.3013049066066742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  61%|    | 4751/7798 [13:01<07:09,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4750/7798:\n",
      "total_loss: 0.2738250195980072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  64%|   | 5001/7798 [13:42<06:26,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000/7798:\n",
      "total_loss: 0.2525571286678314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  67%|   | 5251/7798 [14:23<06:00,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5250/7798:\n",
      "total_loss: 0.36576032638549805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  71%|   | 5501/7798 [15:04<05:24,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/7798:\n",
      "total_loss: 0.23252901434898376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  74%|  | 5751/7798 [15:45<05:48,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5750/7798:\n",
      "total_loss: 0.2801627218723297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  77%|  | 6000/7798 [16:26<03:57,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000/7798:\n",
      "total_loss: 0.3191813826560974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  80%|  | 6250/7798 [17:07<03:35,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6250/7798:\n",
      "total_loss: 0.4013369381427765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  83%| | 6501/7798 [17:49<03:04,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6500/7798:\n",
      "total_loss: 0.39873039722442627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  87%| | 6750/7798 [18:29<02:28,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6750/7798:\n",
      "total_loss: 0.21713222563266754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  90%| | 7001/7798 [19:11<01:54,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000/7798:\n",
      "total_loss: 0.34757477045059204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  93%|| 7251/7798 [19:53<01:11,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7250/7798:\n",
      "total_loss: 0.33339884877204895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  96%|| 7501/7798 [20:34<00:41,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500/7798:\n",
      "total_loss: 0.25124070048332214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  99%|| 7750/7798 [21:15<00:06,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7750/7798:\n",
      "total_loss: 0.3373490869998932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|| 7798/7798 [21:24<00:00,  6.07it/s]\n",
      "Validating: 100%|| 15/15 [00:02<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Total Loss: 0.3233\n",
      "Val Total Loss: 0.2580\n",
      "Learning Rate: 0.000100\n",
      "New best model saved with val loss: 0.2580\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:   3%|         | 251/7798 [00:41<20:10,  6.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/7798:\n",
      "total_loss: 0.2777555584907532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:   6%|         | 501/7798 [01:22<20:21,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/7798:\n",
      "total_loss: 0.4330442547798157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  10%|         | 751/7798 [02:03<15:47,  7.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 750/7798:\n",
      "total_loss: 0.22531835734844208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  13%|        | 1001/7798 [02:45<20:58,  5.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/7798:\n",
      "total_loss: 0.3949824571609497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  16%|        | 1251/7798 [03:26<15:52,  6.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1250/7798:\n",
      "total_loss: 0.4885062575340271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  19%|        | 1501/7798 [04:07<21:14,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500/7798:\n",
      "total_loss: 0.27576249837875366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  22%|       | 1751/7798 [04:49<17:56,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1750/7798:\n",
      "total_loss: 0.3283178210258484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  26%|       | 2000/7798 [05:29<13:46,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/7798:\n",
      "total_loss: 0.40140920877456665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  29%|       | 2250/7798 [06:10<13:33,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2250/7798:\n",
      "total_loss: 0.2004805952310562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  32%|      | 2501/7798 [06:52<13:45,  6.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500/7798:\n",
      "total_loss: 0.2907135486602783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  35%|      | 2750/7798 [07:33<10:55,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2750/7798:\n",
      "total_loss: 0.46517881751060486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  38%|      | 3001/7798 [08:14<11:57,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000/7798:\n",
      "total_loss: 0.3708907663822174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  42%|     | 3251/7798 [08:55<11:44,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3250/7798:\n",
      "total_loss: 0.3527458906173706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  45%|     | 3501/7798 [09:37<10:57,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500/7798:\n",
      "total_loss: 0.4738311469554901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  48%|     | 3751/7798 [10:18<12:34,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3750/7798:\n",
      "total_loss: 0.3565346896648407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  51%|    | 4000/7798 [10:59<11:20,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000/7798:\n",
      "total_loss: 0.26512423157691956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  55%|    | 4251/7798 [11:42<10:05,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4250/7798:\n",
      "total_loss: 0.33567798137664795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  58%|    | 4500/7798 [12:23<07:41,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4500/7798:\n",
      "total_loss: 0.3410412669181824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  61%|    | 4750/7798 [13:05<06:34,  7.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4750/7798:\n",
      "total_loss: 0.3228471577167511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  64%|   | 5001/7798 [13:47<09:47,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000/7798:\n",
      "total_loss: 0.23056963086128235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  67%|   | 5251/7798 [14:29<06:54,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5250/7798:\n",
      "total_loss: 0.219675213098526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  71%|   | 5501/7798 [15:10<05:39,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/7798:\n",
      "total_loss: 0.3023071587085724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  74%|  | 5750/7798 [15:51<05:55,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5750/7798:\n",
      "total_loss: 0.4805169701576233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  77%|  | 6001/7798 [16:34<04:00,  7.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000/7798:\n",
      "total_loss: 0.29013386368751526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  80%|  | 6251/7798 [17:15<03:58,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6250/7798:\n",
      "total_loss: 0.2820550799369812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  83%| | 6501/7798 [17:56<03:47,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6500/7798:\n",
      "total_loss: 0.2095610350370407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  87%| | 6751/7798 [18:38<02:23,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6750/7798:\n",
      "total_loss: 0.18718862533569336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  90%| | 7001/7798 [19:19<02:31,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000/7798:\n",
      "total_loss: 0.17820097506046295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  93%|| 7251/7798 [20:01<01:13,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7250/7798:\n",
      "total_loss: 0.2159208506345749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  96%|| 7501/7798 [20:42<00:46,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500/7798:\n",
      "total_loss: 0.2443581223487854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  99%|| 7750/7798 [21:24<00:06,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7750/7798:\n",
      "total_loss: 0.2566443383693695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|| 7798/7798 [21:32<00:00,  6.03it/s]\n",
      "Validating: 100%|| 15/15 [00:02<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Total Loss: 0.2878\n",
      "Val Total Loss: 0.2308\n",
      "Learning Rate: 0.000100\n",
      "New best model saved with val loss: 0.2308\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pconfig = PointNetConfig(\n",
    "    embeddingSize=n_embd,\n",
    "    numberofPoints=numPoints,\n",
    "    numberofVars=numVars,\n",
    "    numberofYs=numYs,\n",
    ")\n",
    "\n",
    "model = SymbolicGaussianDiffusion(\n",
    "    tnet_config=pconfig,  \n",
    "    vocab_size=train_dataset.vocab_size,\n",
    "    max_seq_len=blockSize,\n",
    "    padding_idx=train_dataset.paddingID,\n",
    "    max_num_vars=9,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    n_embd=n_embd,\n",
    "    timesteps=timesteps,\n",
    "    beta_start=0.0001,\n",
    "    beta_end=0.02\n",
    ")\n",
    "\n",
    "train_single_gpu(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    num_epochs=num_epochs,\n",
    "    save_every=2,\n",
    "    batch_size=batch_size,\n",
    "    timesteps=timesteps,\n",
    "    learning_rate=learning_rate,\n",
    "    path=\"2_var_set_transformer\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6977314,
     "sourceId": 11178716,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7089758,
     "sourceId": 11333846,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7119350,
     "sourceId": 11372292,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 282263,
     "modelInstanceId": 261112,
     "sourceId": 306062,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 290784,
     "modelInstanceId": 269794,
     "sourceId": 319741,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 290853,
     "modelInstanceId": 269860,
     "sourceId": 319828,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6557.524757,
   "end_time": "2025-04-13T00:50:50.532698",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-12T23:01:33.007941",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
