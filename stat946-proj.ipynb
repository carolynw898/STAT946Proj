{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "919cc7d8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-09T05:04:00.848503Z",
     "iopub.status.busy": "2025-04-09T05:04:00.848173Z",
     "iopub.status.idle": "2025-04-09T05:04:01.771527Z",
     "shell.execute_reply": "2025-04-09T05:04:01.770571Z"
    },
    "papermill": {
     "duration": 0.929079,
     "end_time": "2025-04-09T05:04:01.772979",
     "exception": false,
     "start_time": "2025-04-09T05:04:00.843900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/xye_9var/pytorch/default/1/XYE_9Var.pt\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/config.txt\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Val/0_2_0_13062021_184140.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Test/0_2_0_13062021_184319.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/4_2_0_15062021_123153.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/2_2_0_15062021_123153.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/7_2_0_15062021_123153.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/5_2_0_15062021_123153.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/0_2_0_15062021_123153.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/6_2_0_15062021_123153.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/1_2_0_15062021_123153.json\n",
      "/kaggle/input/2-var-dataset/2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points/Train/3_2_0_15062021_123153.json\n",
      "/kaggle/input/1-var-dataset/1_var_test.json\n",
      "/kaggle/input/1-var-dataset/1_var_val.json\n",
      "/kaggle/input/1-var-dataset/1_var_train.json\n",
      "/kaggle/input/xye_1var/pytorch/default/1/XYE_1Var.pt\n",
      "/kaggle/input/symbolic_diffusion_initial/pytorch/default/1/symbolic_diffusion_model.pth\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a51dc933",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T05:04:01.780502Z",
     "iopub.status.busy": "2025-04-09T05:04:01.780132Z",
     "iopub.status.idle": "2025-04-09T05:04:06.396120Z",
     "shell.execute_reply": "2025-04-09T05:04:06.395393Z"
    },
    "papermill": {
     "duration": 4.621473,
     "end_time": "2025-04-09T05:04:06.397809",
     "exception": false,
     "start_time": "2025-04-09T05:04:01.776336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.optimize import minimize\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def generateDataStrEq(\n",
    "    eq, n_points=2, n_vars=3, decimals=4, supportPoints=None, min_x=0, max_x=3\n",
    "):\n",
    "    X = []\n",
    "    Y = []\n",
    "    # TODO: Need to make this faster\n",
    "    for p in range(n_points):\n",
    "        if supportPoints is None:\n",
    "            if type(min_x) == list:\n",
    "                x = []\n",
    "                for _ in range(n_vars):\n",
    "                    idx = np.random.randint(len(min_x))\n",
    "                    x += list(\n",
    "                        np.round(np.random.uniform(min_x[idx], max_x[idx], 1), decimals)\n",
    "                    )\n",
    "            else:\n",
    "                x = list(np.round(np.random.uniform(min_x, max_x, n_vars), decimals))\n",
    "            assert (\n",
    "                len(x) != 0\n",
    "            ), \"For some reason, we didn't generate the points correctly!\"\n",
    "        else:\n",
    "            x = supportPoints[p]\n",
    "\n",
    "        tmpEq = eq + \"\"\n",
    "        for nVID in range(n_vars):\n",
    "            tmpEq = tmpEq.replace(\"x{}\".format(nVID + 1), str(x[nVID]))\n",
    "        y = float(np.round(eval(tmpEq), decimals))\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# def processDataFiles(files):\n",
    "#     text = \"\"\n",
    "#     for f in tqdm(files):\n",
    "#         with open(f, 'r') as h:\n",
    "#             lines = h.read() # don't worry we won't run out of file handles\n",
    "#             if lines[-1]==-1:\n",
    "#                 lines = lines[:-1]\n",
    "#             #text += lines #json.loads(line)\n",
    "#             text = ''.join([lines,text])\n",
    "#     return text\n",
    "\n",
    "\n",
    "def processDataFiles(files):\n",
    "    text = \"\"\n",
    "    for f in files:\n",
    "        with open(f, \"r\") as h:\n",
    "            lines = h.read()  # don't worry we won't run out of file handles\n",
    "            if lines[-1] == -1:\n",
    "                lines = lines[:-1]\n",
    "            # text += lines #json.loads(line)\n",
    "            text = \"\".join([lines, text])\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize_equation(eq):\n",
    "    token_spec = [\n",
    "        (r\"\\*\\*\"),  # exponentiation\n",
    "        (r\"exp\"),  # exp function\n",
    "        (r\"[+\\-*/=()]\"),  # operators and parentheses\n",
    "        (r\"sin\"),  # sin function\n",
    "        (r\"cos\"),  # cos function\n",
    "        (r\"log\"),  # log function\n",
    "        (r\"x\\d+\"),  # variables like x1, x23, etc.\n",
    "        (r\"C\"),  # constants placeholder\n",
    "        (r\"-?\\d+\\.\\d+\"),  # decimal numbers\n",
    "        (r\"-?\\d+\"),  # integers\n",
    "        (r\"_\"),  # padding token\n",
    "    ]\n",
    "    token_regex = \"|\".join(f\"({pattern})\" for pattern in token_spec)\n",
    "    matches = re.finditer(token_regex, eq)\n",
    "    return [match.group(0) for match in matches]\n",
    "\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        block_size,\n",
    "        tokens,\n",
    "        numVars,\n",
    "        numYs,\n",
    "        numPoints,\n",
    "        target=\"Skeleton\",\n",
    "        addVars=False,\n",
    "        const_range=[-0.4, 0.4],\n",
    "        xRange=[-3.0, 3.0],\n",
    "        decimals=4,\n",
    "        augment=False,\n",
    "    ):\n",
    "\n",
    "        data_size, vocab_size = len(data), len(tokens)\n",
    "        print(\"data has %d examples, %d unique.\" % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = {tok: i for i, tok in enumerate(tokens)}\n",
    "        self.itos = {i: tok for i, tok in enumerate(tokens)}\n",
    "\n",
    "        self.numVars = numVars\n",
    "        self.numYs = numYs\n",
    "        self.numPoints = numPoints\n",
    "\n",
    "        # padding token\n",
    "        self.paddingToken = \"_\"\n",
    "        self.paddingID = self.stoi[\"_\"]  # or another ID not already used\n",
    "        self.stoi[self.paddingToken] = self.paddingID\n",
    "        self.itos[self.paddingID] = self.paddingToken\n",
    "\n",
    "        self.threshold = [-1000, 1000]\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data  # it should be a list of examples\n",
    "        self.target = target\n",
    "        self.addVars = addVars\n",
    "\n",
    "        self.const_range = const_range\n",
    "        self.xRange = xRange\n",
    "        self.decimals = decimals\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab an example from the data\n",
    "        chunk = self.data[idx]  # sequence of tokens including x, y, eq, etc.\n",
    "\n",
    "        try:\n",
    "            chunk = json.loads(chunk)  # convert the sequence tokens to a dictionary\n",
    "        except Exception as e:\n",
    "            print(\"Couldn't convert to json: {} \\n error is: {}\".format(chunk, e))\n",
    "            # try the previous example\n",
    "            idx = idx - 1\n",
    "            idx = idx if idx >= 0 else 0\n",
    "            chunk = self.data[idx]\n",
    "            chunk = json.loads(chunk)  # convert the sequence tokens to a dictionary\n",
    "\n",
    "        # find the number of variables in the equation\n",
    "        printInfoCondition = random.random() < 0.0000001\n",
    "        eq = chunk[self.target]\n",
    "        if printInfoCondition:\n",
    "            print(f\"\\nEquation: {eq}\")\n",
    "        vars = re.finditer(\"x[\\d]+\", eq)\n",
    "        numVars = 0\n",
    "        for v in vars:\n",
    "            v = v.group(0).strip(\"x\")\n",
    "            v = eval(v)\n",
    "            v = int(v)\n",
    "            if v > numVars:\n",
    "                numVars = v\n",
    "\n",
    "        if self.target == \"Skeleton\" and self.augment:\n",
    "            threshold = 5000\n",
    "            # randomly generate the constants\n",
    "            cleanEqn = \"\"\n",
    "            for chr in eq:\n",
    "                if chr == \"C\":\n",
    "                    # genereate a new random number\n",
    "                    chr = \"{}\".format(\n",
    "                        np.random.uniform(self.const_range[0], self.const_range[1])\n",
    "                    )\n",
    "                cleanEqn += chr\n",
    "\n",
    "            # update the points\n",
    "            nPoints = np.random.randint(\n",
    "                *self.numPoints\n",
    "            )  # if supportPoints is None else len(supportPoints)\n",
    "            try:\n",
    "                if printInfoCondition:\n",
    "                    print(\"Org:\", chunk[\"X\"], chunk[\"Y\"])\n",
    "\n",
    "                X, y = generateDataStrEq(\n",
    "                    cleanEqn,\n",
    "                    n_points=nPoints,\n",
    "                    n_vars=self.numVars,\n",
    "                    decimals=self.decimals,\n",
    "                    min_x=self.xRange[0],\n",
    "                    max_x=self.xRange[1],\n",
    "                )\n",
    "\n",
    "                # replace out of threshold with maximum numbers\n",
    "                y = [e if abs(e) < threshold else np.sign(e) * threshold for e in y]\n",
    "\n",
    "                # check if there is nan/inf/very large numbers in the y\n",
    "                conditions = (\n",
    "                    (np.isnan(y).any() or np.isinf(y).any())\n",
    "                    or len(y) == 0\n",
    "                    or (abs(min(y)) > threshold or abs(max(y)) > threshold)\n",
    "                )\n",
    "                if not conditions:\n",
    "                    chunk[\"X\"], chunk[\"Y\"] = X, y\n",
    "\n",
    "                if printInfoCondition:\n",
    "                    print(\"Evd:\", chunk[\"X\"], chunk[\"Y\"])\n",
    "            except Exception as e:\n",
    "                # for different reason this might happend including but not limited to division by zero\n",
    "                print(\n",
    "                    \"\".join(\n",
    "                        [\n",
    "                            f\"We just used the original equation and support points because of {e}. \",\n",
    "                            f\"The equation is {eq}, and we update the equation to {cleanEqn}\",\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # encode every character in the equation to an integer\n",
    "        # < is SOS, > is EOS\n",
    "        if self.addVars:\n",
    "            dix = [self.stoi[s] for s in \"<\" + str(numVars) + \":\" + eq + \">\"]\n",
    "        else:\n",
    "            eq_tokens = tokenize_equation(eq)\n",
    "            if self.addVars:\n",
    "                token_seq = [\"<\", str(numVars), \":\", *eq_tokens, \">\"]\n",
    "            else:\n",
    "                token_seq = [\"<\", *eq_tokens, \">\"]\n",
    "            dix = [self.stoi[tok] for tok in token_seq]\n",
    "\n",
    "        inputs = dix[:-1]\n",
    "        outputs = dix[1:]\n",
    "\n",
    "        # add the padding to the equations\n",
    "        paddingSize = max(self.block_size - len(inputs), 0)\n",
    "        paddingList = [self.paddingID] * paddingSize\n",
    "        inputs += paddingList\n",
    "        outputs += paddingList\n",
    "\n",
    "        # make sure it is not more than what should be\n",
    "        inputs = inputs[: self.block_size]\n",
    "        outputs = outputs[: self.block_size]\n",
    "\n",
    "        points = torch.zeros(self.numVars + self.numYs, self.numPoints - 1)\n",
    "        for idx, xy in enumerate(zip(chunk[\"X\"], chunk[\"Y\"])):\n",
    "\n",
    "            if not isinstance(xy[0], list) or not isinstance(\n",
    "                xy[1], (list, float, np.float64)\n",
    "            ):\n",
    "                print(f\"Unexpected types: {type(xy[0])}, {type(xy[1])}\")\n",
    "                continue  # Skip if types are incorrect\n",
    "\n",
    "            # don't let to exceed the maximum number of points\n",
    "            if idx >= self.numPoints - 1:\n",
    "                break\n",
    "\n",
    "            x = xy[0]\n",
    "            x = x + [0] * (max(self.numVars - len(x), 0))  # padding\n",
    "\n",
    "            y = [xy[1]] if type(xy[1]) == float or type(xy[1]) == np.float64 else xy[1]\n",
    "\n",
    "            y = y + [0] * (max(self.numYs - len(y), 0))  # padding\n",
    "            p = x + y  # because it is only one point\n",
    "            p = torch.tensor(p)\n",
    "            # replace nan and inf\n",
    "            p = torch.nan_to_num(\n",
    "                p,\n",
    "                nan=self.threshold[1],\n",
    "                posinf=self.threshold[1],\n",
    "                neginf=self.threshold[0],\n",
    "            )\n",
    "\n",
    "            points[:, idx] = p\n",
    "\n",
    "        points = torch.nan_to_num(\n",
    "            points,\n",
    "            nan=self.threshold[1],\n",
    "            posinf=self.threshold[1],\n",
    "            neginf=self.threshold[0],\n",
    "        )\n",
    "\n",
    "        inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "        outputs = torch.tensor(outputs, dtype=torch.long)\n",
    "        numVars = torch.tensor(numVars, dtype=torch.long)\n",
    "        return inputs, outputs, points, numVars\n",
    "\n",
    "\n",
    "# Relative Mean Square Error\n",
    "def relativeErr(y, yHat, info=False, eps=1e-5):\n",
    "    yHat = np.reshape(yHat, [1, -1])[0]\n",
    "    y = np.reshape(y, [1, -1])[0]\n",
    "    if len(y) > 0 and len(y) == len(yHat):\n",
    "        err = ((yHat - y)) ** 2 / np.linalg.norm(y + eps)\n",
    "        if info:\n",
    "            for _ in range(5):\n",
    "                i = np.random.randint(len(y))\n",
    "                # print(\"yPR,yTrue:{},{}, Err:{}\".format(yHat[i], y[i], err[i]))\n",
    "    else:\n",
    "        err = 100\n",
    "\n",
    "    return np.mean(err)\n",
    "\n",
    "\n",
    "def lossFunc(constants, eq, X, Y, eps=1e-5):\n",
    "    err = 0\n",
    "    eq = eq.replace(\"C\", \"{}\").format(*constants)\n",
    "\n",
    "    for x, y in zip(X, Y):\n",
    "        eqTemp = eq + \"\"\n",
    "        if type(x) == np.float32:\n",
    "            x = [x]\n",
    "        for i, e in enumerate(x):\n",
    "            # make sure e is not a tensor\n",
    "            if type(e) == torch.Tensor:\n",
    "                e = e.item()\n",
    "            eqTemp = eqTemp.replace(\"x{}\".format(i + 1), str(e))\n",
    "        try:\n",
    "            yHat = eval(eqTemp)\n",
    "        except:\n",
    "            # print(\"Exception has been occured! EQ: {}, OR: {}\".format(eqTemp, eq))\n",
    "            yHat = 100\n",
    "        try:\n",
    "            # handle overflow\n",
    "            err += relativeErr(y, yHat)  # (y-yHat)**2\n",
    "        except:\n",
    "            # print(\n",
    "            #    \"Exception has been occured! EQ: {}, OR: {}, y:{}-yHat:{}\".format(\n",
    "            #        eqTemp, eq, y, yHat\n",
    "            #    )\n",
    "            # )\n",
    "            err += 10\n",
    "\n",
    "    err /= len(Y)\n",
    "    return err\n",
    "\n",
    "\n",
    "def get_predicted_skeleton(generated_tokens, train_dataset: CharDataset):\n",
    "    predicted_tokens = generated_tokens.cpu().numpy()\n",
    "    predicted = \"\".join([train_dataset.itos[int(idx)] for idx in predicted_tokens])\n",
    "    predicted = predicted.strip(train_dataset.paddingToken).split(\">\")\n",
    "    predicted = predicted[0] if len(predicted[0]) >= 1 else predicted[1]\n",
    "    predicted = predicted.strip(\"<\").strip(\">\")\n",
    "    predicted = predicted.replace(\"Ce\", \"C*e\")\n",
    "\n",
    "    return predicted\n",
    "\n",
    "\n",
    "def sample_skeleton(model, points, variables, train_dataset, batch_size, ddim_step=20):\n",
    "    \"\"\"Sample skeletons from the model using DDIM or DDPM.\"\"\"\n",
    "    return model.sample(\n",
    "        points, variables, train_dataset, batch_size=batch_size, ddim_step=ddim_step\n",
    "    )\n",
    "\n",
    "\n",
    "def fit_constants(predicted_skeleton, X, Y):\n",
    "    \"\"\"Fit constants in the predicted skeleton using optimization.\"\"\"\n",
    "    c = [1.0 for i, x in enumerate(predicted_skeleton) if x == \"C\"]\n",
    "    b = [(-2, 2) for _, x in enumerate(predicted_skeleton) if x == \"C\"]\n",
    "    predicted = predicted_skeleton\n",
    "    if len(c) != 0:\n",
    "        try:\n",
    "            cHat = minimize(lossFunc, c, args=(predicted_skeleton, X, Y), bounds=b)\n",
    "            if cHat.success and cHat.fun != float(\"inf\"):\n",
    "                predicted = predicted_skeleton.replace(\"C\", \"{}\").format(*cHat.x)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid predicted equation or optimization failed: {predicted_skeleton}\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            raise ValueError(\n",
    "                f\"Error fitting constants: {e}, Equation: {predicted_skeleton}\"\n",
    "            )\n",
    "    return predicted\n",
    "\n",
    "\n",
    "def evaluate_equation(eq, xs, target=True):\n",
    "    \"\"\"Evaluate an equation at given points xs.\"\"\"\n",
    "    SAFE_GLOBALS = {\n",
    "        \"sin\": math.sin,\n",
    "        \"cos\": math.cos,\n",
    "        \"tan\": math.tan,\n",
    "        \"log\": math.log,\n",
    "        \"exp\": math.exp,\n",
    "        \"sqrt\": math.sqrt,\n",
    "        \"abs\": abs,\n",
    "        \"pow\": pow,\n",
    "        \"__builtins__\": {},\n",
    "    }\n",
    "    try:\n",
    "        eq_tmp = eq.replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "        for i, x in enumerate(xs):\n",
    "            eq_tmp = eq_tmp.replace(f\"x{i+1}\", str(x))\n",
    "            if \",\" in eq_tmp:\n",
    "                raise ValueError(\"There is a , in the equation!\")\n",
    "        y = eval(eq_tmp, SAFE_GLOBALS)\n",
    "        y = 0 if np.isnan(y) else y\n",
    "        y = 100 if np.isinf(y) else y\n",
    "    except Exception as e:\n",
    "        if target:\n",
    "            # print(f\"TA: Evaluation failed for equation: {eq_tmp}, Reason: {e}\")\n",
    "            pass\n",
    "        else:\n",
    "            # print(f\"PR: Evaluation failed for equation: {eq_tmp}, Reason: {e}\")\n",
    "            pass\n",
    "        y = 100\n",
    "    return y\n",
    "\n",
    "\n",
    "def evaluate_sample(target_eq, predicted_eq, test_points):\n",
    "    \"\"\"Evaluate target and predicted equations over test points.\"\"\"\n",
    "    Ys, Yhats = [], []\n",
    "    for xs in test_points:\n",
    "        Ys.append(evaluate_equation(target_eq, xs, target=True))\n",
    "        Yhats.append(evaluate_equation(predicted_eq, xs, target=False))\n",
    "    return Ys, Yhats\n",
    "\n",
    "\n",
    "def sample_and_evaluate(\n",
    "    model, points, variables, train_dataset, target_eq, t, ddim_step=None\n",
    "):\n",
    "    \"\"\"Sample a skeleton, fit constants, evaluate, and compute error.\"\"\"\n",
    "    predicted_skeleton = sample_skeleton(\n",
    "        model, points, variables, train_dataset, batch_size=1, ddim_step=ddim_step\n",
    "    )[0]\n",
    "\n",
    "    predicted = fit_constants(predicted_skeleton, t[\"X\"], t[\"Y\"])\n",
    "\n",
    "    Ys, Yhats = evaluate_sample(target_eq, predicted, t[\"XT\"])\n",
    "\n",
    "    err = relativeErr(Ys, Yhats, info=True)\n",
    "    return predicted_skeleton, predicted, err\n",
    "\n",
    "\n",
    "def sample_and_select_best(\n",
    "    model,\n",
    "    points,\n",
    "    variables,\n",
    "    train_dataset,\n",
    "    target_eq,\n",
    "    t,\n",
    "    num_tests,\n",
    "    ddim_step=None,\n",
    "):\n",
    "    \"\"\"Sample num_tests times and select the best sample based on error.\"\"\"\n",
    "    best_err = 10000000\n",
    "    best_predicted_skeleton = \"C\"\n",
    "    best_predicted = \"C\"\n",
    "\n",
    "    for _ in range(num_tests):\n",
    "        predicted_skeleton, predicted, err = sample_and_evaluate(\n",
    "            model, points, variables, train_dataset, target_eq, t, ddim_step\n",
    "        )\n",
    "        if err < best_err:\n",
    "            best_err = err\n",
    "            best_predicted_skeleton = predicted_skeleton\n",
    "            best_predicted = predicted\n",
    "\n",
    "    return best_predicted_skeleton, best_predicted, best_err\n",
    "\n",
    "\n",
    "def plot_and_save_results(\n",
    "    resultDict, fName, pconf, titleTemplate, textTest, modelKey=\"SymbolicDiffusion\"\n",
    "):\n",
    "    \"\"\"Plot cumulative error distribution and save results to a file.\n",
    "    Args:\n",
    "        resultDict: dict with results (e.g., {'err': [], 'trg': [], 'prd': []})\n",
    "        fName: str, output file name\n",
    "        pconf: PointNetConfig object with numberofVars\n",
    "        titleTemplate: str, template for plot title\n",
    "        textTest: list of test data\n",
    "        modelKey: str, key for the model in resultDict\n",
    "    \"\"\"\n",
    "    if isinstance(resultDict, dict):\n",
    "        num_eqns = len(resultDict[fName][modelKey][\"err\"])\n",
    "        num_vars = pconf.numberofVars\n",
    "        title = titleTemplate.format(num_eqns, num_vars)\n",
    "\n",
    "        models = list(\n",
    "            key\n",
    "            for key in resultDict[fName].keys()\n",
    "            if len(resultDict[fName][key][\"err\"]) == num_eqns\n",
    "        )\n",
    "        lists_of_error_scores = [\n",
    "            resultDict[fName][key][\"err\"]\n",
    "            for key in models\n",
    "            if len(resultDict[fName][key][\"err\"]) == num_eqns\n",
    "        ]\n",
    "        linestyles = [\"-\", \"dashdot\", \"dotted\", \"--\"]\n",
    "\n",
    "        eps = 0.00001\n",
    "        y, x, _ = plt.hist(\n",
    "            [\n",
    "                np.log([max(min(x + eps, 1e5), 1e-5) for x in e])\n",
    "                for e in lists_of_error_scores\n",
    "            ],\n",
    "            label=models,\n",
    "            cumulative=True,\n",
    "            histtype=\"step\",\n",
    "            bins=2000,\n",
    "            density=True,\n",
    "            log=False,\n",
    "        )\n",
    "        y = np.expand_dims(y, 0)\n",
    "\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for idx, m in enumerate(models):\n",
    "            plt.plot(x[:-1], y[idx] * 100, linestyle=linestyles[idx], label=m)\n",
    "\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Log of Relative Mean Square Error\")\n",
    "        plt.ylabel(\"Normalized Cumulative Frequency\")\n",
    "\n",
    "        name = \"{}.png\".format(fName.split(\".txt\")[0])\n",
    "        plt.savefig(name)\n",
    "        plt.close()\n",
    "\n",
    "        with open(fName, \"w\", encoding=\"utf-8\") as o:\n",
    "            for i in range(num_eqns):\n",
    "                err = resultDict[fName][modelKey][\"err\"][i]\n",
    "                eq = resultDict[fName][modelKey][\"trg\"][i]\n",
    "                predicted = resultDict[fName][modelKey][\"prd\"][i]\n",
    "                print(f\"Test Case {i}.\")\n",
    "                print(f\"Target: {eq}\\nSkeleton: {predicted}\")\n",
    "                print(f\"Err: {err}\\n\")\n",
    "\n",
    "                o.write(f\"Test Case {i}/{len(textTest)-1}.\\n\")\n",
    "                o.write(f\"{eq}\\n\")\n",
    "                o.write(f\"{modelKey}:\\n\")\n",
    "                o.write(f\"{predicted}\\n{err}\\n\\n\")\n",
    "\n",
    "            avg_err = np.mean(resultDict[fName][modelKey][\"err\"])\n",
    "            o.write(f\"Avg Err: {avg_err}\\n\")\n",
    "            print(f\"Avg Err: {avg_err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a5b38ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T05:04:06.405253Z",
     "iopub.status.busy": "2025-04-09T05:04:06.404899Z",
     "iopub.status.idle": "2025-04-09T05:04:06.428331Z",
     "shell.execute_reply": "2025-04-09T05:04:06.427750Z"
    },
    "papermill": {
     "duration": 0.028484,
     "end_time": "2025-04-09T05:04:06.429675",
     "exception": false,
     "start_time": "2025-04-09T05:04:06.401191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# from SymbolicGPT: https://github.com/mojivalipour/symbolicgpt/blob/master/models.py\n",
    "class PointNetConfig:\n",
    "    \"\"\"base PointNet config\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddingSize,\n",
    "        numberofPoints,\n",
    "        numberofVars,\n",
    "        numberofYs,\n",
    "        method=\"GPT\",\n",
    "        varibleEmbedding=\"NOT_VAR\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.embeddingSize = embeddingSize\n",
    "        self.numberofPoints = numberofPoints  # number of points\n",
    "        self.numberofVars = numberofVars  # input dimension (Xs)\n",
    "        self.numberofYs = numberofYs  # output dimension (Ys)\n",
    "        self.method = method\n",
    "        self.varibleEmbedding = varibleEmbedding\n",
    "\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "\n",
    "class tNet(nn.Module):\n",
    "    \"\"\"\n",
    "    The PointNet structure in the orginal PointNet paper:\n",
    "    PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation by Qi et. al. 2017\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(tNet, self).__init__()\n",
    "\n",
    "        self.activation_func = F.relu\n",
    "        self.num_units = config.embeddingSize\n",
    "\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            config.numberofVars + config.numberofYs, self.num_units, 1\n",
    "        )\n",
    "        self.conv2 = nn.Conv1d(self.num_units, 2 * self.num_units, 1)\n",
    "        self.conv3 = nn.Conv1d(2 * self.num_units, 4 * self.num_units, 1)\n",
    "        self.fc1 = nn.Linear(4 * self.num_units, 2 * self.num_units)\n",
    "        self.fc2 = nn.Linear(2 * self.num_units, self.num_units)\n",
    "\n",
    "        # self.relu = nn.ReLU()\n",
    "\n",
    "        self.input_batch_norm = nn.BatchNorm1d(config.numberofVars + config.numberofYs)\n",
    "        # self.input_layer_norm = nn.LayerNorm(config.numberofPoints)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(self.num_units)\n",
    "        self.bn2 = nn.BatchNorm1d(2 * self.num_units)\n",
    "        self.bn3 = nn.BatchNorm1d(4 * self.num_units)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * self.num_units)\n",
    "        self.bn5 = nn.BatchNorm1d(self.num_units)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch, #features, #points]\n",
    "        :return:\n",
    "            logit: [batch, embedding_size]\n",
    "        \"\"\"\n",
    "        x = self.input_batch_norm(x)\n",
    "        x = self.activation_func(self.bn1(self.conv1(x)))\n",
    "        x = self.activation_func(self.bn2(self.conv2(x)))\n",
    "        x = self.activation_func(self.bn3(self.conv3(x)))\n",
    "        x, _ = torch.max(x, dim=2)  # global max pooling\n",
    "        assert x.size(1) == 4 * self.num_units\n",
    "\n",
    "        x = self.activation_func(self.bn4(self.fc1(x)))\n",
    "        x = self.activation_func(self.bn5(self.fc2(x)))\n",
    "        # x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class NoisePredictionTransformer(nn.Module):\n",
    "    def __init__(self, n_embd, max_seq_len, n_layer=6, n_head=8, max_timesteps=1000):\n",
    "        super().__init__()\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, max_seq_len, n_embd))\n",
    "        self.time_emb = nn.Embedding(max_timesteps, n_embd)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=n_embd,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=n_embd * 4,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layer)\n",
    "\n",
    "    def forward(self, x_t, t, condition):\n",
    "        _, L, _ = x_t.shape\n",
    "        pos_emb = self.pos_emb[:, :L, :]  # [1, L, n_embd]\n",
    "        time_emb = self.time_emb(t)\n",
    "        if time_emb.dim() == 1:  # Scalar t case, [n_embd]\n",
    "            time_emb = time_emb.unsqueeze(0)  # [1, n_embd]\n",
    "        time_emb = time_emb.unsqueeze(1)  # [1, 1, n_embd]\n",
    "        condition = condition.unsqueeze(1)  # [B, 1, n_embd]\n",
    "\n",
    "        x = x_t + pos_emb + time_emb + condition\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "# influenced by https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/simple_diffusion.py\n",
    "class SymbolicGaussianDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tnet_config,\n",
    "        vocab_size,\n",
    "        max_seq_len,\n",
    "        padding_idx: int = 0,\n",
    "        max_num_vars: int = 9,\n",
    "        n_layer=6,\n",
    "        n_head=8,\n",
    "        n_embd=512,\n",
    "        timesteps=1000,\n",
    "        beta_start=0.0001,\n",
    "        beta_end=0.02,\n",
    "        ce_weight=1.0,  # Weight for CE loss relative to MSE\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.padding_idx = padding_idx\n",
    "        self.n_embd = n_embd\n",
    "        self.timesteps = timesteps\n",
    "        self.ce_weight = ce_weight\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd, padding_idx=self.padding_idx)\n",
    "        self.vars_emb = nn.Embedding(max_num_vars, n_embd)\n",
    "\n",
    "        self.decoder = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        self.decoder.weight = self.tok_emb.weight\n",
    "\n",
    "        self.tnet = tNet(tnet_config)\n",
    "        self.model = NoisePredictionTransformer(\n",
    "            n_embd, max_seq_len, n_layer, n_head, timesteps\n",
    "        )\n",
    "\n",
    "        # Noise schedule\n",
    "        self.register_buffer(\"beta\", torch.linspace(beta_start, beta_end, timesteps))\n",
    "        self.register_buffer(\"alpha\", 1.0 - self.beta)\n",
    "        self.register_buffer(\"alpha_bar\", torch.cumprod(self.alpha, dim=0))\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        noise = torch.randn_like(x_start)\n",
    "        sqrt_alpha_bar = torch.sqrt(self.alpha_bar[t]).view(-1, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bar[t]).view(-1, 1, 1)\n",
    "\n",
    "        x_t = sqrt_alpha_bar * x_start + sqrt_one_minus_alpha_bar * noise\n",
    "        return x_t\n",
    "\n",
    "    def p_mean_variance(self, x, t, t_next, condition):\n",
    "        alpha_t = self.alpha[t]\n",
    "        alpha_bar_t = self.alpha_bar[t]\n",
    "        alpha_bar_t_next = self.alpha_bar[t_next]\n",
    "        beta_t = self.beta[t]\n",
    "\n",
    "        x_start_pred = self.model(x, t.long(), condition)\n",
    "\n",
    "        coeff1 = torch.sqrt(alpha_bar_t_next) * beta_t / (1 - alpha_bar_t)\n",
    "        coeff2 = torch.sqrt(alpha_t) * (1 - alpha_bar_t_next) / (1 - alpha_bar_t)\n",
    "        mean = coeff1 * x_start_pred + coeff2 * x\n",
    "        variance = (1 - alpha_bar_t_next) / (1 - alpha_bar_t) * beta_t\n",
    "        return mean, variance\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t, t_next, condition):\n",
    "        mean, variance = self.p_mean_variance(x, t, t_next, condition)\n",
    "        if torch.all(t_next == 0):\n",
    "            return mean\n",
    "        noise = torch.randn_like(x)\n",
    "        return mean + torch.sqrt(variance) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, points, variables, train_dataset, batch_size=16, ddim_step=0):\n",
    "        condition = self.tnet(points) + self.vars_emb(variables)\n",
    "        shape = (batch_size, self.max_seq_len, self.n_embd)\n",
    "        x = torch.randn(shape, device=self.device)\n",
    "        steps = torch.arange(self.timesteps - 1, -1, -1, device=self.device)\n",
    "\n",
    "        for i in range(0, self.timesteps, ddim_step):\n",
    "            t = steps[i]\n",
    "            t_next = (\n",
    "                steps[i + ddim_step]\n",
    "                if i + ddim_step < self.timesteps\n",
    "                else torch.tensor(0, device=self.device)\n",
    "            )\n",
    "            x = self.p_sample(x, t, t_next, condition)\n",
    "\n",
    "            # Print prediction every 250 steps\n",
    "            # if (i + 1) % 250 == 0:\n",
    "            #    logits = self.decoder(x)  # [B, L, vocab_size]\n",
    "            #    token_indices = torch.argmax(logits, dim=-1)  # [B, L]\n",
    "            #    for j in range(batch_size):\n",
    "            #       token_indices_j = token_indices[j]  # [L]\n",
    "            #        predicted_skeleton = get_predicted_skeleton(\n",
    "            #            token_indices_j, train_dataset\n",
    "            #        )\n",
    "            #        tqdm.write(f\" sample {j}: predicted_skeleton: {predicted_skeleton}\")\n",
    "\n",
    "        logits = self.decoder(x)  # [B, L, vocab_size]\n",
    "        token_indices = torch.argmax(logits, dim=-1)  # [B, L]\n",
    "        predicted_skeletons = []\n",
    "        for j in range(batch_size):\n",
    "            token_indices_j = token_indices[j]  # [L]\n",
    "            predicted_skeleton = get_predicted_skeleton(token_indices_j, train_dataset)\n",
    "            predicted_skeletons.append(predicted_skeleton)\n",
    "        return predicted_skeletons\n",
    "\n",
    "    def p_losses(\n",
    "        self, x_start, points, tokens, variables, t, noise=None, mse: bool = False\n",
    "    ):\n",
    "        \"\"\"Hybrid loss: MSE on embeddings + CE on tokens.\"\"\"\n",
    "        noise = torch.randn_like(x_start)\n",
    "        x_t = self.q_sample(x_start, t, noise)\n",
    "        condition = self.tnet(points) + self.vars_emb(variables)\n",
    "        x_start_pred = self.model(x_t, t.long(), condition)\n",
    "\n",
    "        # MSE loss on embeddings\n",
    "        if mse:\n",
    "            mse_loss = F.mse_loss(x_start_pred, x_start)\n",
    "        else:\n",
    "            mse_loss = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        # CE loss on tokens\n",
    "        logits = self.decoder(x_start_pred)  # [B, L, vocab_size]\n",
    "        ce_loss = F.cross_entropy(\n",
    "            logits.view(-1, self.vocab_size),  # [B*L, vocab_size]\n",
    "            tokens.view(-1),  # [B*L]\n",
    "            ignore_index=self.padding_idx,\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "\n",
    "        total_loss = mse_loss + self.ce_weight * ce_loss\n",
    "        return total_loss, mse_loss, ce_loss\n",
    "\n",
    "    def forward(self, points, tokens, variables, t, mse=False):\n",
    "        token_emb = self.tok_emb(tokens)\n",
    "        total_loss, mse_loss, ce_loss = self.p_losses(\n",
    "            token_emb, points, tokens, variables, t, mse=mse\n",
    "        )\n",
    "        return total_loss, mse_loss, ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87aef511",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T05:04:06.436288Z",
     "iopub.status.busy": "2025-04-09T05:04:06.436028Z",
     "iopub.status.idle": "2025-04-09T05:04:06.448872Z",
     "shell.execute_reply": "2025-04-09T05:04:06.447977Z"
    },
    "papermill": {
     "duration": 0.017539,
     "end_time": "2025-04-09T05:04:06.450172",
     "exception": false,
     "start_time": "2025-04-09T05:04:06.432633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import tqdm\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: SymbolicGaussianDiffusion,  \n",
    "    train_loader: DataLoader,\n",
    "    optimizer: Adam,\n",
    "    train_dataset: CharDataset,\n",
    "    timesteps: int,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    num_epochs: int,\n",
    ") -> Tuple[float, float, float]:  \n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_mse_loss = 0\n",
    "    total_ce_loss = 0\n",
    "\n",
    "    for i, (_, tokens, points, variables) in tqdm.tqdm(\n",
    "        enumerate(train_loader),\n",
    "        total=len(train_loader),\n",
    "        desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
    "    ):\n",
    "        points, tokens, variables = (\n",
    "            points.to(device),\n",
    "            tokens.to(device),\n",
    "            variables.to(device),\n",
    "        )\n",
    "        t = torch.randint(0, timesteps, (tokens.shape[0],), device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss, mse_loss, ce_loss = model(points, tokens, variables, t)\n",
    "\n",
    "        if (i + 1) % 250 == 0:\n",
    "            print(f\"Batch {i + 1}/{len(train_loader)}:\")\n",
    "            print(f\"total_loss: {total_loss}, mse: {mse_loss}, ce: {ce_loss}\")\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += total_loss.item()\n",
    "        total_mse_loss += mse_loss.item()\n",
    "        total_ce_loss += ce_loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    avg_mse_loss = total_mse_loss / len(train_loader)\n",
    "    avg_ce_loss = total_ce_loss / len(train_loader)\n",
    "    return avg_train_loss, avg_mse_loss, avg_ce_loss\n",
    "\n",
    "\n",
    "def val_epoch(\n",
    "    model: SymbolicGaussianDiffusion,  \n",
    "    val_loader: DataLoader,\n",
    "    train_dataset: CharDataset,\n",
    "    timesteps: int,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    num_epochs: int,\n",
    ") -> Tuple[float, float, float]:  \n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_mse_loss = 0\n",
    "    total_ce_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, tokens, points, variables in tqdm.tqdm(\n",
    "            val_loader, total=len(val_loader), desc=\"Validating\"\n",
    "        ):\n",
    "            points, tokens, variables = (\n",
    "                points.to(device),\n",
    "                tokens.to(device),\n",
    "                variables.to(device),\n",
    "            )\n",
    "            t = torch.randint(0, timesteps, (tokens.shape[0],), device=device)\n",
    "\n",
    "            total_loss, mse_loss, ce_loss = model(points, tokens, variables, t)\n",
    "\n",
    "            total_val_loss += total_loss.item()\n",
    "            total_mse_loss += mse_loss.item()\n",
    "            total_ce_loss += ce_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    avg_mse_loss = total_mse_loss / len(val_loader)\n",
    "    avg_ce_loss = total_ce_loss / len(val_loader)\n",
    "    return avg_val_loss, avg_mse_loss, avg_ce_loss\n",
    "\n",
    "\n",
    "def train_single_gpu(\n",
    "    model: SymbolicGaussianDiffusion,  \n",
    "    train_dataset: CharDataset,\n",
    "    val_dataset: CharDataset,\n",
    "    num_epochs=10,\n",
    "    save_every=2,\n",
    "    batch_size=32,\n",
    "    timesteps=1000,\n",
    "    learning_rate=1e-3,\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_train_loss, avg_mse_loss, avg_ce_loss = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            train_dataset,\n",
    "            timesteps,\n",
    "            device,\n",
    "            epoch,\n",
    "            num_epochs,\n",
    "        )\n",
    "\n",
    "        avg_val_loss, val_mse_loss, val_ce_loss = val_epoch(\n",
    "            model, val_loader, train_dataset, timesteps, device, epoch, num_epochs\n",
    "        )\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        print(\"\\nEpoch Summary:\")\n",
    "        print(\n",
    "            f\"Train Total Loss: {avg_train_loss:.4f} (MSE: {avg_mse_loss:.4f}, CE: {avg_ce_loss:.4f})\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Val Total Loss: {avg_val_loss:.4f} (MSE: {val_mse_loss:.4f}, CE: {val_ce_loss:.4f})\"\n",
    "        )\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            state_dict = model.state_dict()\n",
    "            torch.save(state_dict, \"best_model.pth\")\n",
    "            print(f\"New best model saved with val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e130f185",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T05:04:06.456899Z",
     "iopub.status.busy": "2025-04-09T05:04:06.456664Z",
     "iopub.status.idle": "2025-04-09T05:04:06.529600Z",
     "shell.execute_reply": "2025-04-09T05:04:06.528790Z"
    },
    "papermill": {
     "duration": 0.077665,
     "end_time": "2025-04-09T05:04:06.530990",
     "exception": false,
     "start_time": "2025-04-09T05:04:06.453325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_embd = 512\n",
    "timesteps = 1000\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 5\n",
    "blockSize = 32\n",
    "numVars = 2\n",
    "numYs = 1\n",
    "numPoints = 250\n",
    "target = 'Skeleton'\n",
    "const_range = [-2.1, 2.1]\n",
    "trainRange = [-3.0, 3.0]\n",
    "decimals = 8\n",
    "addVars = False\n",
    "maxNumFiles = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2acb8756",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T05:04:06.538303Z",
     "iopub.status.busy": "2025-04-09T05:04:06.538041Z",
     "iopub.status.idle": "2025-04-09T05:04:06.541172Z",
     "shell.execute_reply": "2025-04-09T05:04:06.540531Z"
    },
    "papermill": {
     "duration": 0.008066,
     "end_time": "2025-04-09T05:04:06.542451",
     "exception": false,
     "start_time": "2025-04-09T05:04:06.534385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataDir = \"/kaggle/input/2-var-dataset\"\n",
    "dataFolder = \"2Var_RandSupport_FixedLength_-3to3_-5.0to-3.0-3.0to5.0_200Points\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f245030",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T05:04:06.549118Z",
     "iopub.status.busy": "2025-04-09T05:04:06.548911Z",
     "iopub.status.idle": "2025-04-09T05:05:54.010025Z",
     "shell.execute_reply": "2025-04-09T05:05:54.009187Z"
    },
    "papermill": {
     "duration": 107.469008,
     "end_time": "2025-04-09T05:05:54.014513",
     "exception": false,
     "start_time": "2025-04-09T05:04:06.545505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 499035 examples, 30 unique.\n",
      "id:172649\n",
      "outputs:C*x1/x2+C>________________________\n",
      "variables:2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "\n",
    "path = '{}/{}/Train/*.json'.format(dataDir, dataFolder)\n",
    "files = glob.glob(path)[:maxNumFiles]\n",
    "text = processDataFiles(files)\n",
    "text = text.split('\\n') # convert the raw text to a set of examples\n",
    "# skeletons = []\n",
    "skeletons = [json.loads(item)['Skeleton'] for item in text if item.strip()]\n",
    "all_tokens = set()\n",
    "for eq in skeletons:\n",
    "    all_tokens.update(tokenize_equation(eq))\n",
    "integers = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9'}\n",
    "all_tokens.update(integers)  # add all integers to the token set\n",
    "tokens = sorted(list(all_tokens) + ['_', 'T', '<', '>', ':'])  # special tokens\n",
    "trainText = text[:-1] if len(text[-1]) == 0 else text\n",
    "random.shuffle(trainText) # shuffle the dataset, it's important specailly for the combined number of variables experiment\n",
    "train_dataset = CharDataset(trainText, blockSize, tokens=tokens, numVars=numVars,\n",
    "                        numYs=numYs, numPoints=numPoints, target=target, addVars=addVars,\n",
    "                        const_range=const_range, xRange=trainRange, decimals=decimals)\n",
    "\n",
    "idx = np.random.randint(train_dataset.__len__())\n",
    "inputs, outputs, points, variables = train_dataset.__getitem__(idx)\n",
    "inputs = ''.join([train_dataset.itos[int(i)] for i in inputs])\n",
    "outputs = ''.join([train_dataset.itos[int(i)] for i in outputs])\n",
    "print('id:{}\\noutputs:{}\\nvariables:{}'.format(idx,outputs,variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b164c8aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T05:05:54.021412Z",
     "iopub.status.busy": "2025-04-09T05:05:54.021148Z",
     "iopub.status.idle": "2025-04-09T05:05:54.364650Z",
     "shell.execute_reply": "2025-04-09T05:05:54.363557Z"
    },
    "papermill": {
     "duration": 0.348501,
     "end_time": "2025-04-09T05:05:54.366078",
     "exception": false,
     "start_time": "2025-04-09T05:05:54.017577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 949 examples, 30 unique.\n",
      "tensor(-36.2764) tensor(26.8219)\n",
      "id:739\n",
      "outputs:C*x1+C*x1/x2+C>____________________\n",
      "variables:2\n"
     ]
    }
   ],
   "source": [
    "path = '{}/{}/Val/*.json'.format(dataDir,dataFolder)\n",
    "files = glob.glob(path)\n",
    "textVal = processDataFiles([files[0]])\n",
    "textVal = textVal.split('\\n') # convert the raw text to a set of examples\n",
    "val_dataset = CharDataset(textVal, blockSize, tokens=tokens, numVars=numVars,\n",
    "                        numYs=numYs, numPoints=numPoints, target=target, addVars=addVars,\n",
    "                        const_range=const_range, xRange=trainRange, decimals=decimals)\n",
    "\n",
    "# print a random sample\n",
    "idx = np.random.randint(val_dataset.__len__())\n",
    "inputs, outputs, points, variables = val_dataset.__getitem__(idx)\n",
    "print(points.min(), points.max())\n",
    "inputs = ''.join([train_dataset.itos[int(i)] for i in inputs])\n",
    "outputs = ''.join([train_dataset.itos[int(i)] for i in outputs])\n",
    "print('id:{}\\noutputs:{}\\nvariables:{}'.format(idx,outputs,variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d600533",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T05:05:54.373508Z",
     "iopub.status.busy": "2025-04-09T05:05:54.373224Z",
     "iopub.status.idle": "2025-04-09T07:01:37.703493Z",
     "shell.execute_reply": "2025-04-09T07:01:37.702091Z"
    },
    "papermill": {
     "duration": 6943.33581,
     "end_time": "2025-04-09T07:01:37.705242",
     "exception": false,
     "start_time": "2025-04-09T05:05:54.369432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   3%|         | 250/7798 [00:44<22:01,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/7798:\n",
      "total_loss: 2.0131969451904297, mse: 0.0, ce: 2.0131969451904297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   6%|         | 500/7798 [01:25<22:07,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/7798:\n",
      "total_loss: 1.2893157005310059, mse: 0.0, ce: 1.2893157005310059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  10%|         | 750/7798 [02:09<19:25,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 750/7798:\n",
      "total_loss: 0.8388022780418396, mse: 0.0, ce: 0.8388022780418396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  13%|        | 1000/7798 [02:51<19:14,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/7798:\n",
      "total_loss: 0.7277601957321167, mse: 0.0, ce: 0.7277601957321167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  16%|        | 1250/7798 [03:34<20:17,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1250/7798:\n",
      "total_loss: 0.5860534310340881, mse: 0.0, ce: 0.5860534310340881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  19%|        | 1500/7798 [04:18<17:06,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500/7798:\n",
      "total_loss: 0.6162816882133484, mse: 0.0, ce: 0.6162816882133484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  22%|       | 1750/7798 [05:02<19:57,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1750/7798:\n",
      "total_loss: 0.6820328831672668, mse: 0.0, ce: 0.6820328831672668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  26%|       | 2000/7798 [05:47<18:26,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/7798:\n",
      "total_loss: 0.5167811512947083, mse: 0.0, ce: 0.5167811512947083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  29%|       | 2250/7798 [06:32<14:39,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2250/7798:\n",
      "total_loss: 0.7305555939674377, mse: 0.0, ce: 0.7305555939674377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  32%|      | 2500/7798 [07:14<14:31,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500/7798:\n",
      "total_loss: 0.9005461931228638, mse: 0.0, ce: 0.9005461931228638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  35%|      | 2750/7798 [07:57<13:30,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2750/7798:\n",
      "total_loss: 0.5307647585868835, mse: 0.0, ce: 0.5307647585868835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  38%|      | 3000/7798 [08:40<13:22,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000/7798:\n",
      "total_loss: 0.5825098156929016, mse: 0.0, ce: 0.5825098156929016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  42%|     | 3250/7798 [09:22<12:16,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3250/7798:\n",
      "total_loss: 0.46163079142570496, mse: 0.0, ce: 0.46163079142570496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  45%|     | 3500/7798 [10:07<11:35,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500/7798:\n",
      "total_loss: 0.5859965682029724, mse: 0.0, ce: 0.5859965682029724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  48%|     | 3750/7798 [10:51<11:16,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3750/7798:\n",
      "total_loss: 0.556455135345459, mse: 0.0, ce: 0.556455135345459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  51%|    | 4000/7798 [11:35<10:34,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000/7798:\n",
      "total_loss: 0.5271299481391907, mse: 0.0, ce: 0.5271299481391907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  55%|    | 4250/7798 [12:19<09:59,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4250/7798:\n",
      "total_loss: 0.599577009677887, mse: 0.0, ce: 0.599577009677887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  58%|    | 4500/7798 [13:02<09:37,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4500/7798:\n",
      "total_loss: 0.5736927390098572, mse: 0.0, ce: 0.5736927390098572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  61%|    | 4750/7798 [13:45<08:07,  6.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4750/7798:\n",
      "total_loss: 0.48065385222435, mse: 0.0, ce: 0.48065385222435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  64%|   | 5000/7798 [14:29<07:27,  6.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000/7798:\n",
      "total_loss: 0.5997774600982666, mse: 0.0, ce: 0.5997774600982666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  67%|   | 5250/7798 [15:13<07:03,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5250/7798:\n",
      "total_loss: 0.375911682844162, mse: 0.0, ce: 0.375911682844162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  71%|   | 5500/7798 [15:58<06:29,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/7798:\n",
      "total_loss: 0.568957507610321, mse: 0.0, ce: 0.568957507610321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  74%|  | 5750/7798 [16:41<05:29,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5750/7798:\n",
      "total_loss: 0.5445278882980347, mse: 0.0, ce: 0.5445278882980347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  77%|  | 6000/7798 [17:24<05:50,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000/7798:\n",
      "total_loss: 0.368813157081604, mse: 0.0, ce: 0.368813157081604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  80%|  | 6250/7798 [18:08<04:13,  6.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6250/7798:\n",
      "total_loss: 0.4334956705570221, mse: 0.0, ce: 0.4334956705570221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  83%| | 6500/7798 [18:52<03:35,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6500/7798:\n",
      "total_loss: 0.32339245080947876, mse: 0.0, ce: 0.32339245080947876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  87%| | 6750/7798 [19:35<03:17,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6750/7798:\n",
      "total_loss: 0.6152048707008362, mse: 0.0, ce: 0.6152048707008362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  90%| | 7000/7798 [20:19<02:15,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000/7798:\n",
      "total_loss: 0.3303667902946472, mse: 0.0, ce: 0.3303667902946472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  93%|| 7250/7798 [21:02<01:41,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7250/7798:\n",
      "total_loss: 0.5540169477462769, mse: 0.0, ce: 0.5540169477462769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  96%|| 7500/7798 [21:47<00:54,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500/7798:\n",
      "total_loss: 0.4497641324996948, mse: 0.0, ce: 0.4497641324996948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  99%|| 7750/7798 [22:31<00:07,  6.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7750/7798:\n",
      "total_loss: 0.2653367817401886, mse: 0.0, ce: 0.2653367817401886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|| 7798/7798 [22:39<00:00,  5.73it/s]\n",
      "Validating: 100%|| 15/15 [00:03<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Total Loss: 0.7116 (MSE: 0.0000, CE: 0.7116)\n",
      "Val Total Loss: 0.4654 (MSE: 0.0000, CE: 0.4654)\n",
      "Learning Rate: 0.000100\n",
      "New best model saved with val loss: 0.4654\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:   3%|         | 250/7798 [00:44<20:45,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/7798:\n",
      "total_loss: 0.4009297490119934, mse: 0.0, ce: 0.4009297490119934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:   6%|         | 500/7798 [01:28<20:29,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/7798:\n",
      "total_loss: 0.6734201908111572, mse: 0.0, ce: 0.6734201908111572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  10%|         | 750/7798 [02:12<19:42,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 750/7798:\n",
      "total_loss: 0.43581223487854004, mse: 0.0, ce: 0.43581223487854004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  13%|        | 1000/7798 [02:54<18:38,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/7798:\n",
      "total_loss: 0.559002697467804, mse: 0.0, ce: 0.559002697467804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  16%|        | 1250/7798 [03:37<18:38,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1250/7798:\n",
      "total_loss: 0.4218398630619049, mse: 0.0, ce: 0.4218398630619049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  19%|        | 1500/7798 [04:21<18:55,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500/7798:\n",
      "total_loss: 0.4114779829978943, mse: 0.0, ce: 0.4114779829978943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  22%|       | 1750/7798 [05:02<16:01,  6.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1750/7798:\n",
      "total_loss: 0.5679795742034912, mse: 0.0, ce: 0.5679795742034912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  26%|       | 2000/7798 [05:45<15:20,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/7798:\n",
      "total_loss: 0.45944321155548096, mse: 0.0, ce: 0.45944321155548096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  29%|       | 2250/7798 [06:27<14:44,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2250/7798:\n",
      "total_loss: 0.4956612288951874, mse: 0.0, ce: 0.4956612288951874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  32%|      | 2500/7798 [07:10<14:45,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500/7798:\n",
      "total_loss: 0.4931405782699585, mse: 0.0, ce: 0.4931405782699585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  35%|      | 2750/7798 [07:52<15:50,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2750/7798:\n",
      "total_loss: 0.4038023054599762, mse: 0.0, ce: 0.4038023054599762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  38%|      | 3000/7798 [08:35<13:25,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000/7798:\n",
      "total_loss: 0.5033740401268005, mse: 0.0, ce: 0.5033740401268005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  42%|     | 3250/7798 [09:18<13:08,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3250/7798:\n",
      "total_loss: 0.35310178995132446, mse: 0.0, ce: 0.35310178995132446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  45%|     | 3500/7798 [10:01<11:48,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500/7798:\n",
      "total_loss: 0.4364631175994873, mse: 0.0, ce: 0.4364631175994873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  48%|     | 3750/7798 [10:44<11:05,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3750/7798:\n",
      "total_loss: 0.5113399624824524, mse: 0.0, ce: 0.5113399624824524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  51%|    | 4000/7798 [11:29<10:47,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000/7798:\n",
      "total_loss: 0.5430559515953064, mse: 0.0, ce: 0.5430559515953064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  55%|    | 4250/7798 [12:13<09:32,  6.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4250/7798:\n",
      "total_loss: 0.6018736362457275, mse: 0.0, ce: 0.6018736362457275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  58%|    | 4500/7798 [12:58<11:31,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4500/7798:\n",
      "total_loss: 0.49431246519088745, mse: 0.0, ce: 0.49431246519088745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  61%|    | 4750/7798 [13:44<09:13,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4750/7798:\n",
      "total_loss: 0.2737366259098053, mse: 0.0, ce: 0.2737366259098053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  64%|   | 5000/7798 [14:29<09:01,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000/7798:\n",
      "total_loss: 0.46989893913269043, mse: 0.0, ce: 0.46989893913269043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  67%|   | 5250/7798 [15:13<08:20,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5250/7798:\n",
      "total_loss: 0.2741564214229584, mse: 0.0, ce: 0.2741564214229584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  71%|   | 5500/7798 [15:57<06:55,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/7798:\n",
      "total_loss: 0.4239252209663391, mse: 0.0, ce: 0.4239252209663391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  74%|  | 5750/7798 [16:41<06:04,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5750/7798:\n",
      "total_loss: 0.3688611090183258, mse: 0.0, ce: 0.3688611090183258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  77%|  | 6000/7798 [17:25<05:12,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000/7798:\n",
      "total_loss: 0.3723880648612976, mse: 0.0, ce: 0.3723880648612976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  80%|  | 6250/7798 [18:09<04:16,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6250/7798:\n",
      "total_loss: 0.4068833589553833, mse: 0.0, ce: 0.4068833589553833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  83%| | 6500/7798 [18:53<03:34,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6500/7798:\n",
      "total_loss: 0.5215950608253479, mse: 0.0, ce: 0.5215950608253479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  87%| | 6750/7798 [19:36<03:50,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6750/7798:\n",
      "total_loss: 0.458268940448761, mse: 0.0, ce: 0.458268940448761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  90%| | 7000/7798 [20:19<02:08,  6.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000/7798:\n",
      "total_loss: 0.41440585255622864, mse: 0.0, ce: 0.41440585255622864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  93%|| 7250/7798 [21:03<01:52,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7250/7798:\n",
      "total_loss: 0.4527263939380646, mse: 0.0, ce: 0.4527263939380646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  96%|| 7500/7798 [21:47<00:49,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500/7798:\n",
      "total_loss: 0.2922958433628082, mse: 0.0, ce: 0.2922958433628082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  99%|| 7750/7798 [22:30<00:07,  6.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7750/7798:\n",
      "total_loss: 0.4108909070491791, mse: 0.0, ce: 0.4108909070491791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|| 7798/7798 [22:38<00:00,  5.74it/s]\n",
      "Validating: 100%|| 15/15 [00:02<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Total Loss: 0.4321 (MSE: 0.0000, CE: 0.4321)\n",
      "Val Total Loss: 0.3419 (MSE: 0.0000, CE: 0.3419)\n",
      "Learning Rate: 0.000100\n",
      "New best model saved with val loss: 0.3419\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:   3%|         | 250/7798 [00:44<20:06,  6.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/7798:\n",
      "total_loss: 0.2847294807434082, mse: 0.0, ce: 0.2847294807434082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:   6%|         | 500/7798 [01:28<20:21,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/7798:\n",
      "total_loss: 0.2280711680650711, mse: 0.0, ce: 0.2280711680650711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  10%|         | 750/7798 [02:13<18:59,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 750/7798:\n",
      "total_loss: 0.322794646024704, mse: 0.0, ce: 0.322794646024704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  13%|        | 1000/7798 [02:58<19:06,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/7798:\n",
      "total_loss: 0.3712262809276581, mse: 0.0, ce: 0.3712262809276581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  16%|        | 1250/7798 [03:43<19:55,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1250/7798:\n",
      "total_loss: 0.4058595299720764, mse: 0.0, ce: 0.4058595299720764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  19%|        | 1500/7798 [04:28<21:37,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500/7798:\n",
      "total_loss: 0.3055996298789978, mse: 0.0, ce: 0.3055996298789978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  22%|       | 1750/7798 [05:12<17:07,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1750/7798:\n",
      "total_loss: 0.49198147654533386, mse: 0.0, ce: 0.49198147654533386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  26%|       | 2000/7798 [05:57<16:47,  5.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/7798:\n",
      "total_loss: 0.3800065517425537, mse: 0.0, ce: 0.3800065517425537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  29%|       | 2250/7798 [06:41<17:46,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2250/7798:\n",
      "total_loss: 0.2855990529060364, mse: 0.0, ce: 0.2855990529060364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  32%|      | 2500/7798 [07:25<15:14,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500/7798:\n",
      "total_loss: 0.3205718994140625, mse: 0.0, ce: 0.3205718994140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  35%|      | 2750/7798 [08:07<13:41,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2750/7798:\n",
      "total_loss: 0.5065377950668335, mse: 0.0, ce: 0.5065377950668335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  38%|      | 3000/7798 [08:52<13:17,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000/7798:\n",
      "total_loss: 0.4129800796508789, mse: 0.0, ce: 0.4129800796508789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  42%|     | 3250/7798 [09:36<12:21,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3250/7798:\n",
      "total_loss: 0.31523755192756653, mse: 0.0, ce: 0.31523755192756653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  45%|     | 3500/7798 [10:19<12:59,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500/7798:\n",
      "total_loss: 0.48681512475013733, mse: 0.0, ce: 0.48681512475013733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  48%|     | 3750/7798 [11:05<11:25,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3750/7798:\n",
      "total_loss: 0.27029097080230713, mse: 0.0, ce: 0.27029097080230713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  51%|    | 4000/7798 [11:49<10:56,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000/7798:\n",
      "total_loss: 0.26145991683006287, mse: 0.0, ce: 0.26145991683006287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  55%|    | 4250/7798 [12:33<11:36,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4250/7798:\n",
      "total_loss: 0.4909767210483551, mse: 0.0, ce: 0.4909767210483551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  58%|    | 4500/7798 [13:18<08:59,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4500/7798:\n",
      "total_loss: 0.3109782040119171, mse: 0.0, ce: 0.3109782040119171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  61%|    | 4750/7798 [14:01<08:37,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4750/7798:\n",
      "total_loss: 0.32263755798339844, mse: 0.0, ce: 0.32263755798339844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  64%|   | 5000/7798 [14:45<09:15,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000/7798:\n",
      "total_loss: 0.370359867811203, mse: 0.0, ce: 0.370359867811203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  67%|   | 5250/7798 [15:31<07:12,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5250/7798:\n",
      "total_loss: 0.2807105481624603, mse: 0.0, ce: 0.2807105481624603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  71%|   | 5500/7798 [16:16<06:34,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/7798:\n",
      "total_loss: 0.3828918933868408, mse: 0.0, ce: 0.3828918933868408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  74%|  | 5750/7798 [17:02<06:20,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5750/7798:\n",
      "total_loss: 0.23193341493606567, mse: 0.0, ce: 0.23193341493606567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  77%|  | 6000/7798 [17:47<05:00,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000/7798:\n",
      "total_loss: 0.4012472331523895, mse: 0.0, ce: 0.4012472331523895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  80%|  | 6250/7798 [18:32<04:16,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6250/7798:\n",
      "total_loss: 0.2710103690624237, mse: 0.0, ce: 0.2710103690624237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  83%| | 6500/7798 [19:18<03:37,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6500/7798:\n",
      "total_loss: 0.24057339131832123, mse: 0.0, ce: 0.24057339131832123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  87%| | 6750/7798 [20:02<03:01,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6750/7798:\n",
      "total_loss: 0.2914310097694397, mse: 0.0, ce: 0.2914310097694397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  90%| | 7000/7798 [20:48<02:27,  5.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000/7798:\n",
      "total_loss: 0.34384334087371826, mse: 0.0, ce: 0.34384334087371826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  93%|| 7250/7798 [21:33<01:28,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7250/7798:\n",
      "total_loss: 0.27623146772384644, mse: 0.0, ce: 0.27623146772384644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  96%|| 7500/7798 [22:17<00:52,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500/7798:\n",
      "total_loss: 0.2994987964630127, mse: 0.0, ce: 0.2994987964630127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  99%|| 7750/7798 [23:03<00:07,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7750/7798:\n",
      "total_loss: 0.38797104358673096, mse: 0.0, ce: 0.38797104358673096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|| 7798/7798 [23:12<00:00,  5.60it/s]\n",
      "Validating: 100%|| 15/15 [00:02<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Total Loss: 0.3617 (MSE: 0.0000, CE: 0.3617)\n",
      "Val Total Loss: 0.3247 (MSE: 0.0000, CE: 0.3247)\n",
      "Learning Rate: 0.000100\n",
      "New best model saved with val loss: 0.3247\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:   3%|         | 250/7798 [00:46<24:04,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/7798:\n",
      "total_loss: 0.32645419239997864, mse: 0.0, ce: 0.32645419239997864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:   6%|         | 500/7798 [01:31<19:50,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/7798:\n",
      "total_loss: 0.4052009582519531, mse: 0.0, ce: 0.4052009582519531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  10%|         | 750/7798 [02:14<19:55,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 750/7798:\n",
      "total_loss: 0.15783493220806122, mse: 0.0, ce: 0.15783493220806122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  13%|        | 1000/7798 [03:00<20:09,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/7798:\n",
      "total_loss: 0.3120156526565552, mse: 0.0, ce: 0.3120156526565552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  16%|        | 1250/7798 [03:47<17:30,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1250/7798:\n",
      "total_loss: 0.25142714381217957, mse: 0.0, ce: 0.25142714381217957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  19%|        | 1500/7798 [04:31<21:09,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500/7798:\n",
      "total_loss: 0.3928866684436798, mse: 0.0, ce: 0.3928866684436798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  22%|       | 1750/7798 [05:16<19:23,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1750/7798:\n",
      "total_loss: 0.31125083565711975, mse: 0.0, ce: 0.31125083565711975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  26%|       | 2000/7798 [06:01<15:45,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/7798:\n",
      "total_loss: 0.37428662180900574, mse: 0.0, ce: 0.37428662180900574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  29%|       | 2250/7798 [06:46<16:33,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2250/7798:\n",
      "total_loss: 0.2209145575761795, mse: 0.0, ce: 0.2209145575761795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  32%|      | 2500/7798 [07:30<15:55,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500/7798:\n",
      "total_loss: 0.6632392406463623, mse: 0.0, ce: 0.6632392406463623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  35%|      | 2750/7798 [08:14<14:24,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2750/7798:\n",
      "total_loss: 0.35231053829193115, mse: 0.0, ce: 0.35231053829193115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  38%|      | 3000/7798 [08:59<17:05,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000/7798:\n",
      "total_loss: 0.26392439007759094, mse: 0.0, ce: 0.26392439007759094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  42%|     | 3250/7798 [09:45<16:33,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3250/7798:\n",
      "total_loss: 0.254856139421463, mse: 0.0, ce: 0.254856139421463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  45%|     | 3500/7798 [10:29<12:25,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500/7798:\n",
      "total_loss: 0.2904185652732849, mse: 0.0, ce: 0.2904185652732849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  48%|     | 3750/7798 [11:13<11:34,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3750/7798:\n",
      "total_loss: 0.22679559886455536, mse: 0.0, ce: 0.22679559886455536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  51%|    | 4000/7798 [11:57<13:15,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000/7798:\n",
      "total_loss: 0.4437624216079712, mse: 0.0, ce: 0.4437624216079712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  55%|    | 4250/7798 [12:40<10:02,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4250/7798:\n",
      "total_loss: 0.3000640571117401, mse: 0.0, ce: 0.3000640571117401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  58%|    | 4500/7798 [13:24<10:07,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4500/7798:\n",
      "total_loss: 0.20054619014263153, mse: 0.0, ce: 0.20054619014263153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  61%|    | 4750/7798 [14:09<08:32,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4750/7798:\n",
      "total_loss: 0.3382430672645569, mse: 0.0, ce: 0.3382430672645569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  64%|   | 5000/7798 [14:53<07:29,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000/7798:\n",
      "total_loss: 0.32594117522239685, mse: 0.0, ce: 0.32594117522239685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  67%|   | 5250/7798 [15:38<07:18,  5.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5250/7798:\n",
      "total_loss: 0.41945937275886536, mse: 0.0, ce: 0.41945937275886536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  71%|   | 5500/7798 [16:23<07:28,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/7798:\n",
      "total_loss: 0.35854852199554443, mse: 0.0, ce: 0.35854852199554443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  74%|  | 5750/7798 [17:10<06:35,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5750/7798:\n",
      "total_loss: 0.32405316829681396, mse: 0.0, ce: 0.32405316829681396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  77%|  | 6000/7798 [17:56<05:38,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000/7798:\n",
      "total_loss: 0.19787469506263733, mse: 0.0, ce: 0.19787469506263733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  80%|  | 6250/7798 [18:40<04:23,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6250/7798:\n",
      "total_loss: 0.18279840052127838, mse: 0.0, ce: 0.18279840052127838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  83%| | 6500/7798 [19:26<04:17,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6500/7798:\n",
      "total_loss: 0.3685469925403595, mse: 0.0, ce: 0.3685469925403595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  87%| | 6750/7798 [20:10<03:25,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6750/7798:\n",
      "total_loss: 0.3932362198829651, mse: 0.0, ce: 0.3932362198829651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  90%| | 7000/7798 [20:55<02:19,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000/7798:\n",
      "total_loss: 0.14846624433994293, mse: 0.0, ce: 0.14846624433994293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  93%|| 7250/7798 [21:40<01:40,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7250/7798:\n",
      "total_loss: 0.1690981686115265, mse: 0.0, ce: 0.1690981686115265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  96%|| 7500/7798 [22:24<01:00,  4.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500/7798:\n",
      "total_loss: 0.31736433506011963, mse: 0.0, ce: 0.31736433506011963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  99%|| 7750/7798 [23:09<00:07,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7750/7798:\n",
      "total_loss: 0.2604575753211975, mse: 0.0, ce: 0.2604575753211975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|| 7798/7798 [23:18<00:00,  5.58it/s]\n",
      "Validating: 100%|| 15/15 [00:02<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Total Loss: 0.3142 (MSE: 0.0000, CE: 0.3142)\n",
      "Val Total Loss: 0.2687 (MSE: 0.0000, CE: 0.2687)\n",
      "Learning Rate: 0.000100\n",
      "New best model saved with val loss: 0.2687\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:   3%|         | 250/7798 [00:46<28:05,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/7798:\n",
      "total_loss: 0.17554756999015808, mse: 0.0, ce: 0.17554756999015808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:   6%|         | 500/7798 [01:29<19:34,  6.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/7798:\n",
      "total_loss: 0.3526351749897003, mse: 0.0, ce: 0.3526351749897003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  10%|         | 750/7798 [02:15<20:12,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 750/7798:\n",
      "total_loss: 0.20444472134113312, mse: 0.0, ce: 0.20444472134113312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  13%|        | 1000/7798 [03:01<18:41,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/7798:\n",
      "total_loss: 0.43756887316703796, mse: 0.0, ce: 0.43756887316703796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  16%|        | 1250/7798 [03:47<18:46,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1250/7798:\n",
      "total_loss: 0.3038882911205292, mse: 0.0, ce: 0.3038882911205292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  19%|        | 1500/7798 [04:33<20:19,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500/7798:\n",
      "total_loss: 0.21626442670822144, mse: 0.0, ce: 0.21626442670822144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  22%|       | 1750/7798 [05:19<20:28,  4.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1750/7798:\n",
      "total_loss: 0.349139928817749, mse: 0.0, ce: 0.349139928817749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  26%|       | 2000/7798 [06:04<16:35,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/7798:\n",
      "total_loss: 0.28643113374710083, mse: 0.0, ce: 0.28643113374710083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  29%|       | 2250/7798 [06:52<16:04,  5.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2250/7798:\n",
      "total_loss: 0.35912826657295227, mse: 0.0, ce: 0.35912826657295227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  32%|      | 2500/7798 [07:37<15:32,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500/7798:\n",
      "total_loss: 0.20731748640537262, mse: 0.0, ce: 0.20731748640537262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  35%|      | 2750/7798 [08:23<26:02,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2750/7798:\n",
      "total_loss: 0.31402814388275146, mse: 0.0, ce: 0.31402814388275146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  38%|      | 3000/7798 [09:10<15:36,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000/7798:\n",
      "total_loss: 0.23646417260169983, mse: 0.0, ce: 0.23646417260169983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  42%|     | 3250/7798 [09:55<14:30,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3250/7798:\n",
      "total_loss: 0.20999392867088318, mse: 0.0, ce: 0.20999392867088318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  45%|     | 3500/7798 [10:41<12:09,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500/7798:\n",
      "total_loss: 0.32435184717178345, mse: 0.0, ce: 0.32435184717178345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  48%|     | 3750/7798 [11:27<12:27,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3750/7798:\n",
      "total_loss: 0.391988605260849, mse: 0.0, ce: 0.391988605260849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  51%|    | 4000/7798 [12:13<15:24,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000/7798:\n",
      "total_loss: 0.34856924414634705, mse: 0.0, ce: 0.34856924414634705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  55%|    | 4250/7798 [12:56<09:38,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4250/7798:\n",
      "total_loss: 0.12544086575508118, mse: 0.0, ce: 0.12544086575508118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  58%|    | 4500/7798 [13:41<13:07,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4500/7798:\n",
      "total_loss: 0.3953969180583954, mse: 0.0, ce: 0.3953969180583954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  61%|    | 4750/7798 [14:27<08:53,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4750/7798:\n",
      "total_loss: 0.3639010488986969, mse: 0.0, ce: 0.3639010488986969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  64%|   | 5000/7798 [15:13<08:14,  5.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000/7798:\n",
      "total_loss: 0.3315276801586151, mse: 0.0, ce: 0.3315276801586151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  67%|   | 5250/7798 [15:59<07:30,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5250/7798:\n",
      "total_loss: 0.1944158673286438, mse: 0.0, ce: 0.1944158673286438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  71%|   | 5500/7798 [16:46<06:04,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500/7798:\n",
      "total_loss: 0.14669297635555267, mse: 0.0, ce: 0.14669297635555267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  74%|  | 5750/7798 [17:29<05:28,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5750/7798:\n",
      "total_loss: 0.16195350885391235, mse: 0.0, ce: 0.16195350885391235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  77%|  | 6000/7798 [18:13<04:53,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000/7798:\n",
      "total_loss: 0.25909385085105896, mse: 0.0, ce: 0.25909385085105896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  80%|  | 6250/7798 [18:57<04:31,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6250/7798:\n",
      "total_loss: 0.19592003524303436, mse: 0.0, ce: 0.19592003524303436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  83%| | 6500/7798 [19:41<04:00,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6500/7798:\n",
      "total_loss: 0.31638625264167786, mse: 0.0, ce: 0.31638625264167786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  87%| | 6750/7798 [20:25<02:55,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6750/7798:\n",
      "total_loss: 0.2363111972808838, mse: 0.0, ce: 0.2363111972808838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  90%| | 7000/7798 [21:09<02:06,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000/7798:\n",
      "total_loss: 0.2362971156835556, mse: 0.0, ce: 0.2362971156835556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  93%|| 7250/7798 [21:53<01:26,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7250/7798:\n",
      "total_loss: 0.24905864894390106, mse: 0.0, ce: 0.24905864894390106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  96%|| 7500/7798 [22:38<01:00,  4.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500/7798:\n",
      "total_loss: 0.19266961514949799, mse: 0.0, ce: 0.19266961514949799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  99%|| 7750/7798 [23:24<00:08,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7750/7798:\n",
      "total_loss: 0.13843496143817902, mse: 0.0, ce: 0.13843496143817902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|| 7798/7798 [23:33<00:00,  5.52it/s]\n",
      "Validating: 100%|| 15/15 [00:03<00:00,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Total Loss: 0.2749 (MSE: 0.0000, CE: 0.2749)\n",
      "Val Total Loss: 0.2779 (MSE: 0.0000, CE: 0.2779)\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pconfig = PointNetConfig(\n",
    "    embeddingSize=n_embd,\n",
    "    numberofPoints=numPoints,\n",
    "    numberofVars=numVars,\n",
    "    numberofYs=numYs,\n",
    ")\n",
    "\n",
    "model = SymbolicGaussianDiffusion(\n",
    "    tnet_config=pconfig,  \n",
    "    vocab_size=train_dataset.vocab_size,\n",
    "    max_seq_len=blockSize,\n",
    "    padding_idx=train_dataset.paddingID,\n",
    "    max_num_vars=9,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    n_embd=n_embd,\n",
    "    timesteps=timesteps,\n",
    "    beta_start=0.0001,\n",
    "    beta_end=0.02,\n",
    "    ce_weight=1.0  \n",
    ")\n",
    "\n",
    "train_single_gpu(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    num_epochs=num_epochs,\n",
    "    save_every=2,\n",
    "    batch_size=batch_size,\n",
    "    timesteps=timesteps,\n",
    "    learning_rate=learning_rate\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6977314,
     "sourceId": 11178716,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7089758,
     "sourceId": 11333846,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 282263,
     "modelInstanceId": 261112,
     "sourceId": 306062,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 290784,
     "modelInstanceId": 269794,
     "sourceId": 319741,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 290853,
     "modelInstanceId": 269860,
     "sourceId": 319828,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7065.620845,
   "end_time": "2025-04-09T07:01:43.654691",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-09T05:03:58.033846",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
